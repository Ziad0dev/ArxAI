[{"id": "2004.00993", "filepath": "papers/2004.00993.pdf", "title": "Augmented Q Imitation Learning (AQIL)", "abstract": "The study of unsupervised learning can be generally divided into two\ncategories: imitation learning and reinforcement learning. In imitation\nlearning the machine learns by mimicking the behavior of an expert system\nwhereas in reinforcement learning the machine learns via direct environment\nfeedback. Traditional deep reinforcement learning takes a significant time\nbefore the machine starts to converge to an optimal policy. This paper proposes\nAugmented Q-Imitation-Learning, a method by which deep reinforcement learning\nconvergence can be accelerated by applying Q-imitation-learning as the initial\ntraining process in traditional Deep Q-learning.", "authors": ["Xiao Lei Zhang", "Anish Agarwal"], "published": "2020-03-31T18:08:23+00:00", "updated": "2020-04-05T17:16:23+00:00", "categories": ["cs.LG", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2004.00993v2", "entry_id": "http://arxiv.org/abs/2004.00993v2"}, {"id": "1706.08001", "filepath": "papers/1706.08001.pdf", "title": "Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of learning relational order via reinforcement learning procedure?", "abstract": "In this article, we extend the conventional framework of\nconvolutional-Restricted-Boltzmann-Machine to learn highly abstract features\namong abitrary number of time related input maps by constructing a layer of\nmultiplicative units, which capture the relations among inputs. In many cases,\nmore than two maps are strongly related, so it is wise to make multiplicative\nunit learn relations among more input maps, in other words, to find the optimal\nrelational-order of each unit. In order to enable our machine to learn\nrelational order, we developed a reinforcement-learning method whose optimality\nis proven to train the network.", "authors": ["Zizhuang Wang"], "published": "2017-06-24T20:56:27+00:00", "updated": "2017-06-24T20:56:27+00:00", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1706.08001v1", "entry_id": "http://arxiv.org/abs/1706.08001v1"}, {"id": "1907.08908", "filepath": "papers/1907.08908.pdf", "title": "Techniques for Automated Machine Learning", "abstract": "Automated machine learning (AutoML) aims to find optimal machine learning\nsolutions automatically given a machine learning problem. It could release the\nburden of data scientists from the multifarious manual tuning process and\nenable the access of domain experts to the off-the-shelf machine learning\nsolutions without extensive experience. In this paper, we review the current\ndevelopments of AutoML in terms of three categories, automated feature\nengineering (AutoFE), automated model and hyperparameter learning (AutoMHL),\nand automated deep learning (AutoDL). State-of-the-art techniques adopted in\nthe three categories are presented, including Bayesian optimization,\nreinforcement learning, evolutionary algorithm, and gradient-based approaches.\nWe summarize popular AutoML frameworks and conclude with current open\nchallenges of AutoML.", "authors": ["Yi-Wei Chen", "Qingquan Song", "Xia Hu"], "published": "2019-07-21T04:03:36+00:00", "updated": "2019-07-21T04:03:36+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1907.08908v1", "entry_id": "http://arxiv.org/abs/1907.08908v1"}, {"id": "2312.03120", "filepath": "papers/2312.03120.pdf", "title": "The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning", "abstract": "With the advance of the powerful heterogeneous, parallel and distributed\ncomputing systems and ever increasing immense amount of data, machine learning\nhas become an indispensable part of cutting-edge technology, scientific\nresearch and consumer products. In this study, we present a review of modern\nmachine and deep learning. We provide a high-level overview for the latest\nadvanced machine learning algorithms, applications, and frameworks. Our\ndiscussion encompasses parallel distributed learning, deep learning as well as\nfederated learning. As a result, our work serves as an introductory text to the\nvast field of modern machine learning.", "authors": ["Omer Subasi", "Oceane Bel", "Joseph Manzano", "Kevin Barker"], "published": "2023-12-05T20:40:05+00:00", "updated": "2023-12-05T20:40:05+00:00", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf_url": "http://arxiv.org/pdf/2312.03120v1", "entry_id": "http://arxiv.org/abs/2312.03120v1"}, {"id": "2401.11351", "filepath": "papers/2401.11351.pdf", "title": "A comprehensive review of Quantum Machine Learning: from NISQ to Fault Tolerance", "abstract": "Quantum machine learning, which involves running machine learning algorithms\non quantum devices, has garnered significant attention in both academic and\nbusiness circles. In this paper, we offer a comprehensive and unbiased review\nof the various concepts that have emerged in the field of quantum machine\nlearning. This includes techniques used in Noisy Intermediate-Scale Quantum\n(NISQ) technologies and approaches for algorithms compatible with\nfault-tolerant quantum computing hardware. Our review covers fundamental\nconcepts, algorithms, and the statistical learning theory pertinent to quantum\nmachine learning.", "authors": ["Yunfei Wang", "Junyu Liu"], "published": "2024-01-21T00:19:16+00:00", "updated": "2024-03-31T00:32:13+00:00", "categories": ["quant-ph", "cs.AI", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2401.11351v2", "entry_id": "http://arxiv.org/abs/2401.11351v2"}, {"id": "2011.11819", "filepath": "papers/2011.11819.pdf", "title": "When Machine Learning Meets Privacy: A Survey and Outlook", "abstract": "The newly emerged machine learning (e.g. deep learning) methods have become a\nstrong driving force to revolutionize a wide range of industries, such as smart\nhealthcare, financial technology, and surveillance systems. Meanwhile, privacy\nhas emerged as a big concern in this machine learning-based artificial\nintelligence era. It is important to note that the problem of privacy\npreservation in the context of machine learning is quite different from that in\ntraditional data privacy protection, as machine learning can act as both friend\nand foe. Currently, the work on the preservation of privacy and machine\nlearning (ML) is still in an infancy stage, as most existing solutions only\nfocus on privacy problems during the machine learning process. Therefore, a\ncomprehensive study on the privacy preservation problems and machine learning\nis required. This paper surveys the state of the art in privacy issues and\nsolutions for machine learning. The survey covers three categories of\ninteractions between privacy and machine learning: (i) private machine\nlearning, (ii) machine learning aided privacy protection, and (iii) machine\nlearning-based privacy attack and corresponding protection schemes. The current\nresearch progress in each category is reviewed and the key challenges are\nidentified. Finally, based on our in-depth analysis of the area of privacy and\nmachine learning, we point out future research directions in this field.", "authors": ["Bo Liu", "Ming Ding", "Sina Shaham", "Wenny Rahayu", "Farhad Farokhi", "Zihuai Lin"], "published": "2020-11-24T00:52:49+00:00", "updated": "2020-11-24T00:52:49+00:00", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf_url": "http://arxiv.org/pdf/2011.11819v1", "entry_id": "http://arxiv.org/abs/2011.11819v1"}, {"id": "1212.2686", "filepath": "papers/1212.2686.pdf", "title": "Joint Training of Deep Boltzmann Machines", "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior\nmethods require an initial learning pass that trains the deep Boltzmann machine\ngreedily, one layer at a time, or do not perform well on classifi- cation\ntasks.", "authors": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "published": "2012-12-12T01:59:27+00:00", "updated": "2012-12-12T01:59:27+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1212.2686v1", "entry_id": "http://arxiv.org/abs/1212.2686v1"}, {"id": "2206.07090", "filepath": "papers/2206.07090.pdf", "title": "Parallelization of Machine Learning Algorithms Respectively on Single Machine and Spark", "abstract": "With the rapid development of big data technologies, how to dig out useful\ninformation from massive data becomes an essential problem. However, using\nmachine learning algorithms to analyze large data may be time-consuming and\ninefficient on the traditional single machine. To solve these problems, this\npaper has made some research on the parallelization of several classic machine\nlearning algorithms respectively on the single machine and the big data\nplatform Spark. We compare the runtime and efficiency of traditional machine\nlearning algorithms with parallelized machine learning algorithms respectively\non the single machine and Spark platform. The research results have shown\nsignificant improvement in runtime and efficiency of parallelized machine\nlearning algorithms.", "authors": ["Jiajun Shen"], "published": "2022-05-08T03:47:30+00:00", "updated": "2023-04-13T03:30:03+00:00", "categories": ["cs.DC"], "pdf_url": "http://arxiv.org/pdf/2206.07090v2", "entry_id": "http://arxiv.org/abs/2206.07090v2"}, {"id": "1607.02450", "filepath": "papers/1607.02450.pdf", "title": "Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in Social Good Applications", "abstract": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning\nin Social Good Applications, which was held on June 24, 2016 in New York.", "authors": ["Kush R. Varshney"], "published": "2016-07-08T16:55:31+00:00", "updated": "2016-08-28T15:23:47+00:00", "categories": ["stat.ML", "cs.CY", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1607.02450v2", "entry_id": "http://arxiv.org/abs/1607.02450v2"}, {"id": "1811.04422", "filepath": "papers/1811.04422.pdf", "title": "An Optimal Control View of Adversarial Machine Learning", "abstract": "I describe an optimal control view of adversarial machine learning, where the\ndynamical system is the machine learner, the input are adversarial actions, and\nthe control costs are defined by the adversary's goals to do harm and be hard\nto detect. This view encompasses many types of adversarial machine learning,\nincluding test-item attacks, training-data poisoning, and adversarial reward\nshaping. The view encourages adversarial machine learning researcher to utilize\nadvances in control theory and reinforcement learning.", "authors": ["Xiaojin Zhu"], "published": "2018-11-11T14:28:34+00:00", "updated": "2018-11-11T14:28:34+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1811.04422v1", "entry_id": "http://arxiv.org/abs/1811.04422v1"}, {"id": "1903.08801", "filepath": "papers/1903.08801.pdf", "title": "A Unified Analytical Framework for Trustable Machine Learning and Automation Running with Blockchain", "abstract": "Traditional machine learning algorithms use data from databases that are\nmutable, and therefore the data cannot be fully trusted. Also, the machine\nlearning process is difficult to automate. This paper proposes building a\ntrustable machine learning system by using blockchain technology, which can\nstore data in a permanent and immutable way. In addition, smart contracts are\nused to automate the machine learning process. This paper makes three\ncontributions. First, it establishes a link between machine learning technology\nand blockchain technology. Previously, machine learning and blockchain have\nbeen considered two independent technologies without an obvious link. Second,\nit proposes a unified analytical framework for trustable machine learning by\nusing blockchain technology. This unified framework solves both the\ntrustability and automation issues in machine learning. Third, it enables a\ncomputer to translate core machine learning implementation from a single thread\non a single machine to multiple threads on multiple machines running with\nblockchain by using a unified approach. The paper uses association rule mining\nas an example to demonstrate how trustable machine learning can be implemented\nwith blockchain, and it shows how this approach can be used to analyze opioid\nprescriptions to help combat the opioid crisis.", "authors": ["Tao Wang"], "published": "2019-03-21T02:17:08+00:00", "updated": "2019-03-21T02:17:08+00:00", "categories": ["cs.LG", "cs.CR"], "pdf_url": "http://arxiv.org/pdf/1903.08801v1", "entry_id": "http://arxiv.org/abs/1903.08801v1"}, {"id": "2007.05479", "filepath": "papers/2007.05479.pdf", "title": "Impact of Legal Requirements on Explainability in Machine Learning", "abstract": "The requirements on explainability imposed by European laws and their\nimplications for machine learning (ML) models are not always clear. In that\nperspective, our research analyzes explanation obligations imposed for private\nand public decision-making, and how they can be implemented by machine learning\ntechniques.", "authors": ["Adrien Bibal", "Michael Lognoul", "Alexandre de Streel", "Beno\u00eet Fr\u00e9nay"], "published": "2020-07-10T16:57:18+00:00", "updated": "2020-07-10T16:57:18+00:00", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2007.05479v1", "entry_id": "http://arxiv.org/abs/2007.05479v1"}, {"id": "1207.4676", "filepath": "papers/1207.4676.pdf", "title": "Proceedings of the 29th International Conference on Machine Learning (ICML-12)", "abstract": "This is an index to the papers that appear in the Proceedings of the 29th\nInternational Conference on Machine Learning (ICML-12). The conference was held\nin Edinburgh, Scotland, June 27th - July 3rd, 2012.", "authors": ["John Langford", "Joelle Pineau"], "published": "2012-07-19T14:08:22+00:00", "updated": "2012-09-16T11:24:54+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1207.4676v2", "entry_id": "http://arxiv.org/abs/1207.4676v2"}, {"id": "2303.18087", "filepath": "papers/2303.18087.pdf", "title": "Evaluation Challenges for Geospatial ML", "abstract": "As geospatial machine learning models and maps derived from their predictions\nare increasingly used for downstream analyses in science and policy, it is\nimperative to evaluate their accuracy and applicability. Geospatial machine\nlearning has key distinctions from other learning paradigms, and as such, the\ncorrect way to measure performance of spatial machine learning outputs has been\na topic of debate. In this paper, I delineate unique challenges of model\nevaluation for geospatial machine learning with global or remotely sensed\ndatasets, culminating in concrete takeaways to improve evaluations of\ngeospatial model performance.", "authors": ["Esther Rolf"], "published": "2023-03-31T14:24:06+00:00", "updated": "2023-03-31T14:24:06+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2303.18087v1", "entry_id": "http://arxiv.org/abs/2303.18087v1"}, {"id": "1707.04849", "filepath": "papers/1707.04849.pdf", "title": "Minimax deviation strategies for machine learning and recognition with short learning samples", "abstract": "The article is devoted to the problem of small learning samples in machine\nlearning. The flaws of maximum likelihood learning and minimax learning are\nlooked into and the concept of minimax deviation learning is introduced that is\nfree of those flaws.", "authors": ["Michail Schlesinger", "Evgeniy Vodolazskiy"], "published": "2017-07-16T09:15:08+00:00", "updated": "2017-07-16T09:15:08+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/1707.04849v1", "entry_id": "http://arxiv.org/abs/1707.04849v1"}, {"id": "0904.3664", "filepath": "papers/0904.3664.pdf", "title": "Introduction to Machine Learning: Class Notes 67577", "abstract": "Introduction to Machine learning covering Statistical Inference (Bayes, EM,\nML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),\nand PAC learning (the Formal model, VC dimension, Double Sampling theorem).", "authors": ["Amnon Shashua"], "published": "2009-04-23T11:40:57+00:00", "updated": "2009-04-23T11:40:57+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/0904.3664v1", "entry_id": "http://arxiv.org/abs/0904.3664v1"}, {"id": "2108.07915", "filepath": "papers/2108.07915.pdf", "title": "Data Pricing in Machine Learning Pipelines", "abstract": "Machine learning is disruptive. At the same time, machine learning can only\nsucceed by collaboration among many parties in multiple steps naturally as\npipelines in an eco-system, such as collecting data for possible machine\nlearning applications, collaboratively training models by multiple parties and\ndelivering machine learning services to end users. Data is critical and\npenetrating in the whole machine learning pipelines. As machine learning\npipelines involve many parties and, in order to be successful, have to form a\nconstructive and dynamic eco-system, marketplaces and data pricing are\nfundamental in connecting and facilitating those many parties. In this article,\nwe survey the principles and the latest research development of data pricing in\nmachine learning pipelines. We start with a brief review of data marketplaces\nand pricing desiderata. Then, we focus on pricing in three important steps in\nmachine learning pipelines. To understand pricing in the step of training data\ncollection, we review pricing raw data sets and data labels. We also\ninvestigate pricing in the step of collaborative training of machine learning\nmodels, and overview pricing machine learning models for end users in the step\nof machine learning deployment. We also discuss a series of possible future\ndirections.", "authors": ["Zicun Cong", "Xuan Luo", "Pei Jian", "Feida Zhu", "Yong Zhang"], "published": "2021-08-18T00:57:06+00:00", "updated": "2021-08-18T00:57:06+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/2108.07915v1", "entry_id": "http://arxiv.org/abs/2108.07915v1"}, {"id": "1909.09246", "filepath": "papers/1909.09246.pdf", "title": "Machine Learning for Clinical Predictive Analytics", "abstract": "In this chapter, we provide a brief overview of applying machine learning\ntechniques for clinical prediction tasks. We begin with a quick introduction to\nthe concepts of machine learning and outline some of the most common machine\nlearning algorithms. Next, we demonstrate how to apply the algorithms with\nappropriate toolkits to conduct machine learning experiments for clinical\nprediction tasks. The objectives of this chapter are to (1) understand the\nbasics of machine learning techniques and the reasons behind why they are\nuseful for solving clinical prediction problems, (2) understand the intuition\nbehind some machine learning models, including regression, decision trees, and\nsupport vector machines, and (3) understand how to apply these models to\nclinical prediction problems using publicly available datasets via case\nstudies.", "authors": ["Wei-Hung Weng"], "published": "2019-09-19T22:02:00+00:00", "updated": "2019-09-19T22:02:00+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1909.09246v1", "entry_id": "http://arxiv.org/abs/1909.09246v1"}, {"id": "2202.10564", "filepath": "papers/2202.10564.pdf", "title": "Human-in-the-loop Machine Learning: A Macro-Micro Perspective", "abstract": "Though technical advance of artificial intelligence and machine learning has\nenabled many promising intelligent systems, many computing tasks are still not\nable to be fully accomplished by machine intelligence. Motivated by the\ncomplementary nature of human and machine intelligence, an emerging trend is to\ninvolve humans in the loop of machine learning and decision-making. In this\npaper, we provide a macro-micro review of human-in-the-loop machine learning.\nWe first describe major machine learning challenges which can be addressed by\nhuman intervention in the loop. Then we examine closely the latest research and\nfindings of introducing humans into each step of the lifecycle of machine\nlearning. Finally, we analyze current research gaps and point out future\nresearch directions.", "authors": ["Jiangtao Wang", "Bin Guo", "Liming Chen"], "published": "2022-02-21T22:45:59+00:00", "updated": "2022-02-21T22:45:59+00:00", "categories": ["cs.HC"], "pdf_url": "http://arxiv.org/pdf/2202.10564v1", "entry_id": "http://arxiv.org/abs/2202.10564v1"}, {"id": "2407.05526", "filepath": "papers/2407.05526.pdf", "title": "Can Machines Learn the True Probabilities?", "abstract": "When there exists uncertainty, AI machines are designed to make decisions so\nas to reach the best expected outcomes. Expectations are based on true facts\nabout the objective environment the machines interact with, and those facts can\nbe encoded into AI models in the form of true objective probability functions.\nAccordingly, AI models involve probabilistic machine learning in which the\nprobabilities should be objectively interpreted. We prove under some basic\nassumptions when machines can learn the true objective probabilities, if any,\nand when machines cannot learn them.", "authors": ["Jinsook Kim"], "published": "2024-07-08T00:19:43+00:00", "updated": "2024-07-08T00:19:43+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2407.05526v1", "entry_id": "http://arxiv.org/abs/2407.05526v1"}, {"id": "2007.01503", "filepath": "papers/2007.01503.pdf", "title": "Mathematical Perspective of Machine Learning", "abstract": "We take a closer look at some theoretical challenges of Machine Learning as a\nfunction approximation, gradient descent as the default optimization algorithm,\nlimitations of fixed length and width networks and a different approach to RNNs\nfrom a mathematical perspective.", "authors": ["Yarema Boryshchak"], "published": "2020-07-03T05:26:02+00:00", "updated": "2020-07-03T05:26:02+00:00", "categories": ["cs.LG", "stat.ML", "68T07"], "pdf_url": "http://arxiv.org/pdf/2007.01503v1", "entry_id": "http://arxiv.org/abs/2007.01503v1"}, {"id": "2001.09608", "filepath": "papers/2001.09608.pdf", "title": "Some Insights into Lifelong Reinforcement Learning Systems", "abstract": "A lifelong reinforcement learning system is a learning system that has the\nability to learn through trail-and-error interaction with the environment over\nits lifetime. In this paper, I give some arguments to show that the traditional\nreinforcement learning paradigm fails to model this type of learning system.\nSome insights into lifelong reinforcement learning are provided, along with a\nsimplistic prototype lifelong reinforcement learning system.", "authors": ["Changjian Li"], "published": "2020-01-27T07:26:12+00:00", "updated": "2020-01-27T07:26:12+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2001.09608v1", "entry_id": "http://arxiv.org/abs/2001.09608v1"}, {"id": "1906.06821", "filepath": "papers/1906.06821.pdf", "title": "A Survey of Optimization Methods from a Machine Learning Perspective", "abstract": "Machine learning develops rapidly, which has made many theoretical\nbreakthroughs and is widely applied in various fields. Optimization, as an\nimportant part of machine learning, has attracted much attention of\nresearchers. With the exponential growth of data amount and the increase of\nmodel complexity, optimization methods in machine learning face more and more\nchallenges. A lot of work on solving optimization problems or improving\noptimization methods in machine learning has been proposed successively. The\nsystematic retrospect and summary of the optimization methods from the\nperspective of machine learning are of great significance, which can offer\nguidance for both developments of optimization and machine learning research.\nIn this paper, we first describe the optimization problems in machine learning.\nThen, we introduce the principles and progresses of commonly used optimization\nmethods. Next, we summarize the applications and developments of optimization\nmethods in some popular machine learning fields. Finally, we explore and give\nsome challenges and open problems for the optimization in machine learning.", "authors": ["Shiliang Sun", "Zehui Cao", "Han Zhu", "Jing Zhao"], "published": "2019-06-17T02:54:51+00:00", "updated": "2019-10-23T08:26:31+00:00", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1906.06821v2", "entry_id": "http://arxiv.org/abs/1906.06821v2"}, {"id": "1702.08608", "filepath": "papers/1702.08608.pdf", "title": "Towards A Rigorous Science of Interpretable Machine Learning", "abstract": "As machine learning systems become ubiquitous, there has been a surge of\ninterest in interpretable machine learning: systems that provide explanation\nfor their outputs. These explanations are often used to qualitatively assess\nother criteria such as safety or non-discrimination. However, despite the\ninterest in interpretability, there is very little consensus on what\ninterpretable machine learning is and how it should be measured. In this\nposition paper, we first define interpretability and describe when\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\nfor rigorous evaluation and expose open questions towards a more rigorous\nscience of interpretable machine learning.", "authors": ["Finale Doshi-Velez", "Been Kim"], "published": "2017-02-28T02:19:20+00:00", "updated": "2017-03-02T19:32:10+00:00", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1702.08608v2", "entry_id": "http://arxiv.org/abs/1702.08608v2"}, {"id": "2009.11087", "filepath": "papers/2009.11087.pdf", "title": "Probabilistic Machine Learning for Healthcare", "abstract": "Machine learning can be used to make sense of healthcare data. Probabilistic\nmachine learning models help provide a complete picture of observed data in\nhealthcare. In this review, we examine how probabilistic machine learning can\nadvance healthcare. We consider challenges in the predictive model building\npipeline where probabilistic models can be beneficial including calibration and\nmissing data. Beyond predictive models, we also investigate the utility of\nprobabilistic machine learning models in phenotyping, in generative models for\nclinical use cases, and in reinforcement learning.", "authors": ["Irene Y. Chen", "Shalmali Joshi", "Marzyeh Ghassemi", "Rajesh Ranganath"], "published": "2020-09-23T12:14:05+00:00", "updated": "2020-09-23T12:14:05+00:00", "categories": ["stat.ML", "cs.CY", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2009.11087v1", "entry_id": "http://arxiv.org/abs/2009.11087v1"}, {"id": "1808.00033", "filepath": "papers/1808.00033.pdf", "title": "Techniques for Interpretable Machine Learning", "abstract": "Interpretable machine learning tackles the important problem that humans\ncannot understand the behaviors of complex machine learning models and how\nthese models arrive at a particular decision. Although many approaches have\nbeen proposed, a comprehensive understanding of the achievements and challenges\nis still lacking. We provide a survey covering existing techniques to increase\nthe interpretability of machine learning models. We also discuss crucial issues\nthat the community should consider in future work such as designing\nuser-friendly explanations and developing comprehensive evaluation metrics to\nfurther push forward the area of interpretable machine learning.", "authors": ["Mengnan Du", "Ninghao Liu", "Xia Hu"], "published": "2018-07-31T19:14:39+00:00", "updated": "2019-05-19T20:44:37+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1808.00033v3", "entry_id": "http://arxiv.org/abs/1808.00033v3"}, {"id": "1911.06612", "filepath": "papers/1911.06612.pdf", "title": "Position Paper: Towards Transparent Machine Learning", "abstract": "Transparent machine learning is introduced as an alternative form of machine\nlearning, where both the model and the learning system are represented in\nsource code form. The goal of this project is to enable direct human\nunderstanding of machine learning models, giving us the ability to learn,\nverify, and refine them as programs. If solved, this technology could represent\na best-case scenario for the safety and security of AI systems going forward.", "authors": ["Dustin Juliano"], "published": "2019-11-12T10:49:55+00:00", "updated": "2019-11-12T10:49:55+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/1911.06612v1", "entry_id": "http://arxiv.org/abs/1911.06612v1"}, {"id": "2007.01977", "filepath": "papers/2007.01977.pdf", "title": "Lale: Consistent Automated Machine Learning", "abstract": "Automated machine learning makes it easier for data scientists to develop\npipelines by searching over possible choices for hyperparameters, algorithms,\nand even pipeline topologies. Unfortunately, the syntax for automated machine\nlearning tools is inconsistent with manual machine learning, with each other,\nand with error checks. Furthermore, few tools support advanced features such as\ntopology search or higher-order operators. This paper introduces Lale, a\nlibrary of high-level Python interfaces that simplifies and unifies automated\nmachine learning in a consistent way.", "authors": ["Guillaume Baudart", "Martin Hirzel", "Kiran Kate", "Parikshit Ram", "Avraham Shinnar"], "published": "2020-07-04T00:55:41+00:00", "updated": "2020-07-04T00:55:41+00:00", "categories": ["cs.LG", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2007.01977v1", "entry_id": "http://arxiv.org/abs/2007.01977v1"}, {"id": "2001.04942", "filepath": "papers/2001.04942.pdf", "title": "Private Machine Learning via Randomised Response", "abstract": "We introduce a general learning framework for private machine learning based\non randomised response. Our assumption is that all actors are potentially\nadversarial and as such we trust only to release a single noisy version of an\nindividual's datapoint. We discuss a general approach that forms a consistent\nway to estimate the true underlying machine learning model and demonstrate this\nin the case of logistic regression.", "authors": ["David Barber"], "published": "2020-01-14T17:56:16+00:00", "updated": "2020-02-24T19:13:28+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2001.04942v2", "entry_id": "http://arxiv.org/abs/2001.04942v2"}, {"id": "1507.02188", "filepath": "papers/1507.02188.pdf", "title": "AutoCompete: A Framework for Machine Learning Competition", "abstract": "In this paper, we propose AutoCompete, a highly automated machine learning\nframework for tackling machine learning competitions. This framework has been\nlearned by us, validated and improved over a period of more than two years by\nparticipating in online machine learning competitions. It aims at minimizing\nhuman interference required to build a first useful predictive model and to\nassess the practical difficulty of a given machine learning challenge. The\nproposed system helps in identifying data types, choosing a machine learn- ing\nmodel, tuning hyper-parameters, avoiding over-fitting and optimization for a\nprovided evaluation metric. We also observe that the proposed system produces\nbetter (or comparable) results with less runtime as compared to other\napproaches.", "authors": ["Abhishek Thakur", "Artus Krohn-Grimberghe"], "published": "2015-07-08T15:07:39+00:00", "updated": "2015-07-08T15:07:39+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1507.02188v1", "entry_id": "http://arxiv.org/abs/1507.02188v1"}, {"id": "2405.03720", "filepath": "papers/2405.03720.pdf", "title": "Spatial Transfer Learning with Simple MLP", "abstract": "First step to investigate the potential of transfer learning applied to the\nfield of spatial statistics", "authors": ["Hongjian Yang"], "published": "2024-05-05T20:39:15+00:00", "updated": "2024-05-05T20:39:15+00:00", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2405.03720v1", "entry_id": "http://arxiv.org/abs/2405.03720v1"}, {"id": "2212.12303", "filepath": "papers/2212.12303.pdf", "title": "Introduction to Machine Learning for Physicians: A Survival Guide for Data Deluge", "abstract": "Many modern research fields increasingly rely on collecting and analysing\nmassive, often unstructured, and unwieldy datasets. Consequently, there is\ngrowing interest in machine learning and artificial intelligence applications\nthat can harness this `data deluge'. This broad nontechnical overview provides\na gentle introduction to machine learning with a specific focus on medical and\nbiological applications. We explain the common types of machine learning\nalgorithms and typical tasks that can be solved, illustrating the basics with\nconcrete examples from healthcare. Lastly, we provide an outlook on open\nchallenges, limitations, and potential impacts of machine-learning-powered\nmedicine.", "authors": ["Ri\u010dards Marcinkevi\u010ds", "Ece Ozkan", "Julia E. Vogt"], "published": "2022-12-23T13:08:59+00:00", "updated": "2022-12-23T13:08:59+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2212.12303v1", "entry_id": "http://arxiv.org/abs/2212.12303v1"}, {"id": "2305.15410", "filepath": "papers/2305.15410.pdf", "title": "Machine learning-assisted close-set X-ray diffraction phase identification of transition metals", "abstract": "Machine learning has been applied to the problem of X-ray diffraction phase\nprediction with promising results. In this paper, we describe a method for\nusing machine learning to predict crystal structure phases from X-ray\ndiffraction data of transition metals and their oxides. We evaluate the\nperformance of our method and compare the variety of its settings. Our results\ndemonstrate that the proposed machine learning framework achieves competitive\nperformance. This demonstrates the potential for machine learning to\nsignificantly impact the field of X-ray diffraction and crystal structure\ndetermination. Open-source implementation:\nhttps://github.com/maxnygma/NeuralXRD.", "authors": ["Maksim Zhdanov", "Andrey Zhdanov"], "published": "2023-04-28T09:29:10+00:00", "updated": "2023-04-28T09:29:10+00:00", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2305.15410v1", "entry_id": "http://arxiv.org/abs/2305.15410v1"}, {"id": "2003.05155", "filepath": "papers/2003.05155.pdf", "title": "Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology", "abstract": "Machine learning is an established and frequently used technique in industry\nand academia but a standard process model to improve success and efficiency of\nmachine learning applications is still missing. Project organizations and\nmachine learning practitioners have a need for guidance throughout the life\ncycle of a machine learning application to meet business expectations. We\ntherefore propose a process model for the development of machine learning\napplications, that covers six phases from defining the scope to maintaining the\ndeployed machine learning application. The first phase combines business and\ndata understanding as data availability oftentimes affects the feasibility of\nthe project. The sixth phase covers state-of-the-art approaches for monitoring\nand maintenance of a machine learning applications, as the risk of model\ndegradation in a changing environment is eminent. With each task of the\nprocess, we propose quality assurance methodology that is suitable to adress\nchallenges in machine learning development that we identify in form of risks.\nThe methodology is drawn from practical experience and scientific literature\nand has proven to be general and stable. The process model expands on CRISP-DM,\na data mining process model that enjoys strong industry support but lacks to\naddress machine learning specific tasks. Our work proposes an industry and\napplication neutral process model tailored for machine learning applications\nwith focus on technical tasks for quality assurance.", "authors": ["Stefan Studer", "Thanh Binh Bui", "Christian Drescher", "Alexander Hanuschkin", "Ludwig Winkler", "Steven Peters", "Klaus-Robert Mueller"], "published": "2020-03-11T08:25:49+00:00", "updated": "2021-02-24T14:33:24+00:00", "categories": ["cs.LG", "cs.SE", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2003.05155v2", "entry_id": "http://arxiv.org/abs/2003.05155v2"}, {"id": "1603.02185", "filepath": "papers/1603.02185.pdf", "title": "Distributed Multi-Task Learning with Shared Representation", "abstract": "We study the problem of distributed multi-task learning with shared\nrepresentation, where each machine aims to learn a separate, but related, task\nin an unknown shared low-dimensional subspaces, i.e. when the predictor matrix\nhas low rank. We consider a setting where each task is handled by a different\nmachine, with samples for the task available locally on the machine, and study\ncommunication-efficient methods for exploiting the shared structure.", "authors": ["Jialei Wang", "Mladen Kolar", "Nathan Srebro"], "published": "2016-03-07T18:11:54+00:00", "updated": "2016-03-07T18:11:54+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1603.02185v1", "entry_id": "http://arxiv.org/abs/1603.02185v1"}, {"id": "2412.00464", "filepath": "papers/2412.00464.pdf", "title": "On the Conditions for Domain Stability for Machine Learning: a Mathematical Approach", "abstract": "This work proposes a mathematical approach that (re)defines a property of\nMachine Learning models named stability and determines sufficient conditions to\nvalidate it. Machine Learning models are represented as functions, and the\ncharacteristics in scope depend upon the domain of the function, what allows us\nto adopt topological and metric spaces theory as a basis. Finally, this work\nprovides some equivalences useful to prove and test stability in Machine\nLearning models. The results suggest that whenever stability is aligned with\nthe notion of function smoothness, then the stability of Machine Learning\nmodels primarily depends upon certain topological, measurable properties of the\nclassification sets within the ML model domain.", "authors": ["Gabriel Pedroza"], "published": "2024-11-30T12:57:07+00:00", "updated": "2024-11-30T12:57:07+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML", "F.4.1; I.2.0"], "pdf_url": "http://arxiv.org/pdf/2412.00464v1", "entry_id": "http://arxiv.org/abs/2412.00464v1"}, {"id": "1707.09562", "filepath": "papers/1707.09562.pdf", "title": "MLBench: How Good Are Machine Learning Clouds for Binary Classification Tasks on Structured Data?", "abstract": "We conduct an empirical study of machine learning functionalities provided by\nmajor cloud service providers, which we call machine learning clouds. Machine\nlearning clouds hold the promise of hiding all the sophistication of running\nlarge-scale machine learning: Instead of specifying how to run a machine\nlearning task, users only specify what machine learning task to run and the\ncloud figures out the rest. Raising the level of abstraction, however, rarely\ncomes free - a performance penalty is possible. How good, then, are current\nmachine learning clouds on real-world machine learning workloads?\n  We study this question with a focus on binary classication problems. We\npresent mlbench, a novel benchmark constructed by harvesting datasets from\nKaggle competitions. We then compare the performance of the top winning code\navailable from Kaggle with that of running machine learning clouds from both\nAzure and Amazon on mlbench. Our comparative study reveals the strength and\nweakness of existing machine learning clouds and points out potential future\ndirections for improvement.", "authors": ["Yu Liu", "Hantian Zhang", "Luyuan Zeng", "Wentao Wu", "Ce Zhang"], "published": "2017-07-29T21:59:18+00:00", "updated": "2017-10-16T11:13:32+00:00", "categories": ["cs.DC", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1707.09562v3", "entry_id": "http://arxiv.org/abs/1707.09562v3"}, {"id": "2412.18979", "filepath": "papers/2412.18979.pdf", "title": "Quantum memristors for neuromorphic quantum machine learning", "abstract": "Quantum machine learning may permit to realize more efficient machine\nlearning calculations with near-term quantum devices. Among the diverse quantum\nmachine learning paradigms which are currently being considered, quantum\nmemristors are promising as a way of combining, in the same quantum hardware, a\nunitary evolution with the nonlinearity provided by the measurement and\nfeedforward. Thus, an efficient way of deploying neuromorphic quantum computing\nfor quantum machine learning may be enabled.", "authors": ["Lucas Lamata"], "published": "2024-12-25T20:21:24+00:00", "updated": "2024-12-25T20:21:24+00:00", "categories": ["quant-ph", "cs.NE"], "pdf_url": "http://arxiv.org/pdf/2412.18979v1", "entry_id": "http://arxiv.org/abs/2412.18979v1"}, {"id": "1911.00776", "filepath": "papers/1911.00776.pdf", "title": "Ten-year Survival Prediction for Breast Cancer Patients", "abstract": "This report assesses different machine learning approaches to 10-year\nsurvival prediction of breast cancer patients.", "authors": ["Changmao Li", "Han He", "Yunze Hao", "Caleb Ziems"], "published": "2019-11-02T19:53:32+00:00", "updated": "2019-11-02T19:53:32+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1911.00776v1", "entry_id": "http://arxiv.org/abs/1911.00776v1"}, {"id": "2012.04105", "filepath": "papers/2012.04105.pdf", "title": "The Tribes of Machine Learning and the Realm of Computer Architecture", "abstract": "Machine learning techniques have influenced the field of computer\narchitecture like many other fields. This paper studies how the fundamental\nmachine learning techniques can be applied towards computer architecture\nproblems. We also provide a detailed survey of computer architecture research\nthat employs different machine learning methods. Finally, we present some\nfuture opportunities and the outstanding challenges that need to be overcome to\nexploit full potential of machine learning for computer architecture.", "authors": ["Ayaz Akram", "Jason Lowe-Power"], "published": "2020-12-07T23:10:51+00:00", "updated": "2020-12-07T23:10:51+00:00", "categories": ["cs.LG", "cs.AR"], "pdf_url": "http://arxiv.org/pdf/2012.04105v1", "entry_id": "http://arxiv.org/abs/2012.04105v1"}, {"id": "2002.12364", "filepath": "papers/2002.12364.pdf", "title": "Theoretical Models of Learning to Learn", "abstract": "A Machine can only learn if it is biased in some way. Typically the bias is\nsupplied by hand, for example through the choice of an appropriate set of\nfeatures. However, if the learning machine is embedded within an {\\em\nenvironment} of related tasks, then it can {\\em learn} its own bias by learning\nsufficiently many tasks from the environment. In this paper two models of bias\nlearning (or equivalently, learning to learn) are introduced and the main\ntheoretical results presented. The first model is a PAC-type model based on\nempirical process theory, while the second is a hierarchical Bayes model.", "authors": ["Jonathan Baxter"], "published": "2020-02-27T13:35:26+00:00", "updated": "2020-02-27T13:35:26+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2002.12364v1", "entry_id": "http://arxiv.org/abs/2002.12364v1"}, {"id": "2007.14206", "filepath": "papers/2007.14206.pdf", "title": "Machine Learning Potential Repository", "abstract": "This paper introduces a machine learning potential repository that includes\nPareto optimal machine learning potentials. It also shows the systematic\ndevelopment of accurate and fast machine learning potentials for a wide range\nof elemental systems. As a result, many Pareto optimal machine learning\npotentials are available in the repository from a website. Therefore, the\nrepository will help many scientists to perform accurate and fast atomistic\nsimulations.", "authors": ["Atsuto Seko"], "published": "2020-07-27T14:30:23+00:00", "updated": "2020-07-27T14:30:23+00:00", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci", "physics.chem-ph", "physics.data-an"], "pdf_url": "http://arxiv.org/pdf/2007.14206v1", "entry_id": "http://arxiv.org/abs/2007.14206v1"}, {"id": "1711.06552", "filepath": "papers/1711.06552.pdf", "title": "Introduction to intelligent computing unit 1", "abstract": "This brief note highlights some basic concepts required toward understanding\nthe evolution of machine learning and deep learning models. The note starts\nwith an overview of artificial intelligence and its relationship to biological\nneuron that ultimately led to the evolution of todays intelligent models.", "authors": ["Isa Inuwa-Dutse"], "published": "2017-11-15T16:52:48+00:00", "updated": "2017-11-15T16:52:48+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1711.06552v1", "entry_id": "http://arxiv.org/abs/1711.06552v1"}, {"id": "2004.05366", "filepath": "papers/2004.05366.pdf", "title": "In-Machine-Learning Database: Reimagining Deep Learning with Old-School SQL", "abstract": "In-database machine learning has been very popular, almost being a cliche.\nHowever, can we do it the other way around? In this work, we say \"yes\" by\napplying plain old SQL to deep learning, in a sense implementing deep learning\nalgorithms with SQL. Most deep learning frameworks, as well as generic machine\nlearning ones, share a de facto standard of multidimensional array operations,\nunderneath fancier infrastructure such as automatic differentiation. As SQL\ntables can be regarded as generalisations of (multi-dimensional) arrays, we\nhave found a way to express common deep learning operations in SQL, encouraging\na different way of thinking and thus potentially novel models. In particular,\none of the latest trend in deep learning was the introduction of sparsity in\nthe name of graph convolutional networks, whereas we take sparsity almost for\ngranted in the database world. As both databases and machine learning involve\ntransformation of datasets, we hope this work can inspire further works\nutilizing the large body of existing wisdom, algorithms and technologies in the\ndatabase field to advance the state of the art in machine learning, rather than\nmerely integerating machine learning into databases.", "authors": ["Len Du"], "published": "2020-04-11T11:00:26+00:00", "updated": "2020-04-14T18:08:28+00:00", "categories": ["cs.LG", "cs.DB", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2004.05366v2", "entry_id": "http://arxiv.org/abs/2004.05366v2"}, {"id": "1908.04710", "filepath": "papers/1908.04710.pdf", "title": "metric-learn: Metric Learning Algorithms in Python", "abstract": "metric-learn is an open source Python package implementing supervised and\nweakly-supervised distance metric learning algorithms. As part of\nscikit-learn-contrib, it provides a unified interface compatible with\nscikit-learn which allows to easily perform cross-validation, model selection,\nand pipelining with other machine learning estimators. metric-learn is\nthoroughly tested and available on PyPi under the MIT licence.", "authors": ["William de Vazelhes", "CJ Carey", "Yuan Tang", "Nathalie Vauquier", "Aur\u00e9lien Bellet"], "published": "2019-08-13T15:52:31+00:00", "updated": "2020-07-27T14:47:52+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1908.04710v3", "entry_id": "http://arxiv.org/abs/1908.04710v3"}, {"id": "2008.08080", "filepath": "papers/2008.08080.pdf", "title": "mlr3proba: An R Package for Machine Learning in Survival Analysis", "abstract": "As machine learning has become increasingly popular over the last few\ndecades, so too has the number of machine learning interfaces for implementing\nthese models. Whilst many R libraries exist for machine learning, very few\noffer extended support for survival analysis. This is problematic considering\nits importance in fields like medicine, bioinformatics, economics, engineering,\nand more. mlr3proba provides a comprehensive machine learning interface for\nsurvival analysis and connects with mlr3's general model tuning and\nbenchmarking facilities to provide a systematic infrastructure for survival\nmodeling and evaluation.", "authors": ["Raphael Sonabend", "Franz J. Kir\u00e1ly", "Andreas Bender", "Bernd Bischl", "Michel Lang"], "published": "2020-08-18T11:21:24+00:00", "updated": "2020-12-14T11:41:25+00:00", "categories": ["stat.CO", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2008.08080v2", "entry_id": "http://arxiv.org/abs/2008.08080v2"}, {"id": "1910.12387", "filepath": "papers/1910.12387.pdf", "title": "Components of Machine Learning: Binding Bits and FLOPS", "abstract": "Many machine learning problems and methods are combinations of three\ncomponents: data, hypothesis space and loss function. Different machine\nlearning methods are obtained as combinations of different choices for the\nrepresentation of data, hypothesis space and loss function. After reviewing the\nmathematical structure of these three components, we discuss intrinsic\ntrade-offs between statistical and computational properties of machine learning\nmethods.", "authors": ["Alexander Jung"], "published": "2019-10-25T17:33:33+00:00", "updated": "2019-10-30T07:59:02+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/1910.12387v2", "entry_id": "http://arxiv.org/abs/1910.12387v2"}, {"id": "2007.07981", "filepath": "papers/2007.07981.pdf", "title": "Differential Replication in Machine Learning", "abstract": "When deployed in the wild, machine learning models are usually confronted\nwith data and requirements that constantly vary, either because of changes in\nthe generating distribution or because external constraints change the\nenvironment where the model operates. To survive in such an ecosystem, machine\nlearning models need to adapt to new conditions by evolving over time. The idea\nof model adaptability has been studied from different perspectives. In this\npaper, we propose a solution based on reusing the knowledge acquired by the\nalready deployed machine learning models and leveraging it to train future\ngenerations. This is the idea behind differential replication of machine\nlearning models.", "authors": ["Irene Unceta", "Jordi Nin", "Oriol Pujol"], "published": "2020-07-15T20:26:49+00:00", "updated": "2020-07-15T20:26:49+00:00", "categories": ["cs.LG", "stat.ML", "cs.LG, stat.ML"], "pdf_url": "http://arxiv.org/pdf/2007.07981v1", "entry_id": "http://arxiv.org/abs/2007.07981v1"}, {"id": "2108.08712", "filepath": "papers/2108.08712.pdf", "title": "Teaching Uncertainty Quantification in Machine Learning through Use Cases", "abstract": "Uncertainty in machine learning is not generally taught as general knowledge\nin Machine Learning course curricula. In this paper we propose a short\ncurriculum for a course about uncertainty in machine learning, and complement\nthe course with a selection of use cases, aimed to trigger discussion and let\nstudents play with the concepts of uncertainty in a programming setting. Our\nuse cases cover the concept of output uncertainty, Bayesian neural networks and\nweight distributions, sources of uncertainty, and out of distribution\ndetection. We expect that this curriculum and set of use cases motivates the\ncommunity to adopt these important concepts into courses for safety in AI.", "authors": ["Matias Valdenegro-Toro"], "published": "2021-08-19T14:22:17+00:00", "updated": "2021-08-19T14:22:17+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2108.08712v1", "entry_id": "http://arxiv.org/abs/2108.08712v1"}, {"id": "1509.00913", "filepath": "papers/1509.00913.pdf", "title": "On-the-Fly Learning in a Perpetual Learning Machine", "abstract": "Despite the promise of brain-inspired machine learning, deep neural networks\n(DNN) have frustratingly failed to bridge the deceptively large gap between\nlearning and memory. Here, we introduce a Perpetual Learning Machine; a new\ntype of DNN that is capable of brain-like dynamic 'on the fly' learning because\nit exists in a self-supervised state of Perpetual Stochastic Gradient Descent.\nThus, we provide the means to unify learning and memory within a machine\nlearning framework. We also explore the elegant duality of abstraction and\nsynthesis: the Yin and Yang of deep learning.", "authors": ["Andrew J. R. Simpson"], "published": "2015-09-03T01:30:29+00:00", "updated": "2015-09-29T16:57:42+00:00", "categories": ["cs.LG", "68Txx"], "pdf_url": "http://arxiv.org/pdf/1509.00913v3", "entry_id": "http://arxiv.org/abs/1509.00913v3"}, {"id": "2110.12773", "filepath": "papers/2110.12773.pdf", "title": "Scientific Machine Learning Benchmarks", "abstract": "The breakthrough in Deep Learning neural networks has transformed the use of\nAI and machine learning technologies for the analysis of very large\nexperimental datasets. These datasets are typically generated by large-scale\nexperimental facilities at national laboratories. In the context of science,\nscientific machine learning focuses on training machines to identify patterns,\ntrends, and anomalies to extract meaningful scientific insights from such\ndatasets. With a new generation of experimental facilities, the rate of data\ngeneration and the scale of data volumes will increasingly require the use of\nmore automated data analysis. At present, identifying the most appropriate\nmachine learning algorithm for the analysis of any given scientific dataset is\nstill a challenge for scientists. This is due to many different machine\nlearning frameworks, computer architectures, and machine learning models.\nHistorically, for modelling and simulation on HPC systems such problems have\nbeen addressed through benchmarking computer applications, algorithms, and\narchitectures. Extending such a benchmarking approach and identifying metrics\nfor the application of machine learning methods to scientific datasets is a new\nchallenge for both scientists and computer scientists. In this paper, we\ndescribe our approach to the development of scientific machine learning\nbenchmarks and review other approaches to benchmarking scientific machine\nlearning.", "authors": ["Jeyan Thiyagalingam", "Mallikarjun Shankar", "Geoffrey Fox", "Tony Hey"], "published": "2021-10-25T10:05:11+00:00", "updated": "2021-10-25T10:05:11+00:00", "categories": ["cs.LG", "physics.comp-ph", "I.2"], "pdf_url": "http://arxiv.org/pdf/2110.12773v1", "entry_id": "http://arxiv.org/abs/2110.12773v1"}, {"id": "2104.05314", "filepath": "papers/2104.05314.pdf", "title": "Machine learning and deep learning", "abstract": "Today, intelligent systems that offer artificial intelligence capabilities\noften rely on machine learning. Machine learning describes the capacity of\nsystems to learn from problem-specific training data to automate the process of\nanalytical model building and solve associated tasks. Deep learning is a\nmachine learning concept based on artificial neural networks. For many\napplications, deep learning models outperform shallow machine learning models\nand traditional data analysis approaches. In this article, we summarize the\nfundamentals of machine learning and deep learning to generate a broader\nunderstanding of the methodical underpinning of current intelligent systems. In\nparticular, we provide a conceptual distinction between relevant terms and\nconcepts, explain the process of automated analytical model building through\nmachine learning and deep learning, and discuss the challenges that arise when\nimplementing such intelligent systems in the field of electronic markets and\nnetworked business. These naturally go beyond technological aspects and\nhighlight issues in human-machine interaction and artificial intelligence\nservitization.", "authors": ["Christian Janiesch", "Patrick Zschech", "Kai Heinrich"], "published": "2021-04-12T09:54:12+00:00", "updated": "2021-04-14T10:31:01+00:00", "categories": ["cs.AI"], "pdf_url": "http://arxiv.org/pdf/2104.05314v2", "entry_id": "http://arxiv.org/abs/2104.05314v2"}, {"id": "1705.07538", "filepath": "papers/1705.07538.pdf", "title": "Infrastructure for Usable Machine Learning: The Stanford DAWN Project", "abstract": "Despite incredible recent advances in machine learning, building machine\nlearning applications remains prohibitively time-consuming and expensive for\nall but the best-trained, best-funded engineering organizations. This expense\ncomes not from a need for new and improved statistical models but instead from\na lack of systems and tools for supporting end-to-end machine learning\napplication development, from data preparation and labeling to\nproductionization and monitoring. In this document, we outline opportunities\nfor infrastructure supporting usable, end-to-end machine learning applications\nin the context of the nascent DAWN (Data Analytics for What's Next) project at\nStanford.", "authors": ["Peter Bailis", "Kunle Olukotun", "Christopher Re", "Matei Zaharia"], "published": "2017-05-22T02:28:19+00:00", "updated": "2017-06-09T02:13:09+00:00", "categories": ["cs.LG", "cs.DB", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1705.07538v2", "entry_id": "http://arxiv.org/abs/1705.07538v2"}, {"id": "1909.03550", "filepath": "papers/1909.03550.pdf", "title": "Lecture Notes: Optimization for Machine Learning", "abstract": "Lecture notes on optimization for machine learning, derived from a course at\nPrinceton University and tutorials given in MLSS, Buenos Aires, as well as\nSimons Foundation, Berkeley.", "authors": ["Elad Hazan"], "published": "2019-09-08T21:49:42+00:00", "updated": "2019-09-08T21:49:42+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1909.03550v1", "entry_id": "http://arxiv.org/abs/1909.03550v1"}, {"id": "1612.04858", "filepath": "papers/1612.04858.pdf", "title": "Bayesian Optimization for Machine Learning : A Practical Guidebook", "abstract": "The engineering of machine learning systems is still a nascent field; relying\non a seemingly daunting collection of quickly evolving tools and best\npractices. It is our hope that this guidebook will serve as a useful resource\nfor machine learning practitioners looking to take advantage of Bayesian\noptimization techniques. We outline four example machine learning problems that\ncan be solved using open source machine learning libraries, and highlight the\nbenefits of using Bayesian optimization in the context of these common machine\nlearning applications.", "authors": ["Ian Dewancker", "Michael McCourt", "Scott Clark"], "published": "2016-12-14T22:04:33+00:00", "updated": "2016-12-14T22:04:33+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/1612.04858v1", "entry_id": "http://arxiv.org/abs/1612.04858v1"}, {"id": "2306.14624", "filepath": "papers/2306.14624.pdf", "title": "Insights From Insurance for Fair Machine Learning", "abstract": "We argue that insurance can act as an analogon for the social situatedness of\nmachine learning systems, hence allowing machine learning scholars to take\ninsights from the rich and interdisciplinary insurance literature. Tracing the\ninteraction of uncertainty, fairness and responsibility in insurance provides a\nfresh perspective on fairness in machine learning. We link insurance fairness\nconceptions to their machine learning relatives, and use this bridge to\nproblematize fairness as calibration. In this process, we bring to the\nforefront two themes that have been largely overlooked in the machine learning\nliterature: responsibility and aggregate-individual tensions.", "authors": ["Christian Fr\u00f6hlich", "Robert C. Williamson"], "published": "2023-06-26T11:56:00+00:00", "updated": "2024-01-23T09:43:23+00:00", "categories": ["cs.LG", "cs.CY"], "pdf_url": "http://arxiv.org/pdf/2306.14624v2", "entry_id": "http://arxiv.org/abs/2306.14624v2"}, {"id": "2001.11489", "filepath": "papers/2001.11489.pdf", "title": "Machine Learning in Network Security Using KNIME Analytics", "abstract": "Machine learning has more and more effect on our every day's life. This field\nkeeps growing and expanding into new areas. Machine learning is based on the\nimplementation of artificial intelligence that gives systems the capability to\nautomatically learn and enhance from experiments without being explicitly\nprogrammed. Machine Learning algorithms apply mathematical equations to analyze\ndatasets and predict values based on the dataset. In the field of\ncybersecurity, machine learning algorithms can be utilized to train and analyze\nthe Intrusion Detection Systems (IDSs) on security-related datasets. In this\npaper, we tested different machine learning algorithms to analyze NSL-KDD\ndataset using KNIME analytics.", "authors": ["Munther Abualkibash"], "published": "2019-11-18T14:10:17+00:00", "updated": "2019-11-18T14:10:17+00:00", "categories": ["cs.CR"], "pdf_url": "http://arxiv.org/pdf/2001.11489v1", "entry_id": "http://arxiv.org/abs/2001.11489v1"}, {"id": "2103.11249", "filepath": "papers/2103.11249.pdf", "title": "SELM: Software Engineering of Machine Learning Models", "abstract": "One of the pillars of any machine learning model is its concepts. Using\nsoftware engineering, we can engineer these concepts and then develop and\nexpand them. In this article, we present a SELM framework for Software\nEngineering of machine Learning Models. We then evaluate this framework through\na case study. Using the SELM framework, we can improve a machine learning\nprocess efficiency and provide more accuracy in learning with less processing\nhardware resources and a smaller training dataset. This issue highlights the\nimportance of an interdisciplinary approach to machine learning. Therefore, in\nthis article, we have provided interdisciplinary teams' proposals for machine\nlearning.", "authors": ["Nafiseh Jafari", "Mohammad Reza Besharati", "Mohammad Izadi", "Maryam Hourali"], "published": "2021-03-20T21:43:24+00:00", "updated": "2021-03-20T21:43:24+00:00", "categories": ["cs.SE", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2103.11249v1", "entry_id": "http://arxiv.org/abs/2103.11249v1"}, {"id": "2102.05639", "filepath": "papers/2102.05639.pdf", "title": "Energy-Harvesting Distributed Machine Learning", "abstract": "This paper provides a first study of utilizing energy harvesting for\nsustainable machine learning in distributed networks. We consider a distributed\nlearning setup in which a machine learning model is trained over a large number\nof devices that can harvest energy from the ambient environment, and develop a\npractical learning framework with theoretical convergence guarantees. We\ndemonstrate through numerical experiments that the proposed framework can\nsignificantly outperform energy-agnostic benchmarks. Our framework is scalable,\nrequires only local estimation of the energy statistics, and can be applied to\na wide range of distributed training settings, including machine learning in\nwireless networks, edge computing, and mobile internet of things.", "authors": ["Basak Guler", "Aylin Yener"], "published": "2021-02-10T18:53:51+00:00", "updated": "2021-02-10T18:53:51+00:00", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2102.05639v1", "entry_id": "http://arxiv.org/abs/2102.05639v1"}, {"id": "1510.00633", "filepath": "papers/1510.00633.pdf", "title": "Distributed Multitask Learning", "abstract": "We consider the problem of distributed multi-task learning, where each\nmachine learns a separate, but related, task. Specifically, each machine learns\na linear predictor in high-dimensional space,where all tasks share the same\nsmall support. We present a communication-efficient estimator based on the\ndebiased lasso and show that it is comparable with the optimal centralized\nmethod.", "authors": ["Jialei Wang", "Mladen Kolar", "Nathan Srebro"], "published": "2015-10-02T16:15:30+00:00", "updated": "2015-10-02T16:15:30+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1510.00633v1", "entry_id": "http://arxiv.org/abs/1510.00633v1"}, {"id": "1607.01400", "filepath": "papers/1607.01400.pdf", "title": "An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality in Machine Learning", "abstract": "We propose a clustering-based iterative algorithm to solve certain\noptimization problems in machine learning, where we start the algorithm by\naggregating the original data, solving the problem on aggregated data, and then\nin subsequent steps gradually disaggregate the aggregated data. We apply the\nalgorithm to common machine learning problems such as the least absolute\ndeviation regression problem, support vector machines, and semi-supervised\nsupport vector machines. We derive model-specific data aggregation and\ndisaggregation procedures. We also show optimality, convergence, and the\noptimality gap of the approximated solution in each iteration. A computational\nstudy is provided.", "authors": ["Young Woong Park", "Diego Klabjan"], "published": "2016-07-05T20:04:57+00:00", "updated": "2016-07-05T20:04:57+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1607.01400v1", "entry_id": "http://arxiv.org/abs/1607.01400v1"}, {"id": "1810.03548", "filepath": "papers/1810.03548.pdf", "title": "Meta-Learning: A Survey", "abstract": "Meta-learning, or learning to learn, is the science of systematically\nobserving how different machine learning approaches perform on a wide range of\nlearning tasks, and then learning from this experience, or meta-data, to learn\nnew tasks much faster than otherwise possible. Not only does this dramatically\nspeed up and improve the design of machine learning pipelines or neural\narchitectures, it also allows us to replace hand-engineered algorithms with\nnovel approaches learned in a data-driven way. In this chapter, we provide an\noverview of the state of the art in this fascinating and continuously evolving\nfield.", "authors": ["Joaquin Vanschoren"], "published": "2018-10-08T16:07:11+00:00", "updated": "2018-10-08T16:07:11+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1810.03548v1", "entry_id": "http://arxiv.org/abs/1810.03548v1"}, {"id": "1909.09248", "filepath": "papers/1909.09248.pdf", "title": "Representation Learning for Electronic Health Records", "abstract": "Information in electronic health records (EHR), such as clinical narratives,\nexamination reports, lab measurements, demographics, and other patient\nencounter entries, can be transformed into appropriate data representations\nthat can be used for downstream clinical machine learning tasks using\nrepresentation learning. Learning better representations is critical to improve\nthe performance of downstream tasks. Due to the advances in machine learning,\nwe now can learn better and meaningful representations from EHR through\ndisentangling the underlying factors inside data and distilling large amounts\nof information and knowledge from heterogeneous EHR sources. In this chapter,\nwe first introduce the background of learning representations and reasons why\nwe need good EHR representations in machine learning for medicine and\nhealthcare in Section 1. Next, we explain the commonly-used machine learning\nand evaluation methods for representation learning using a deep learning\napproach in Section 2. Following that, we review recent related studies of\nlearning patient state representation from EHR for clinical machine learning\ntasks in Section 3. Finally, in Section 4 we discuss more techniques, studies,\nand challenges for learning natural language representations when free texts,\nsuch as clinical notes, examination reports, or biomedical literature are used.\nWe also discuss challenges and opportunities in these rapidly growing research\nfields.", "authors": ["Wei-Hung Weng", "Peter Szolovits"], "published": "2019-09-19T22:12:30+00:00", "updated": "2019-09-19T22:12:30+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1909.09248v1", "entry_id": "http://arxiv.org/abs/1909.09248v1"}, {"id": "2106.07032", "filepath": "papers/2106.07032.pdf", "title": "Category Theory in Machine Learning", "abstract": "Over the past two decades machine learning has permeated almost every realm\nof technology. At the same time, many researchers have begun using category\ntheory as a unifying language, facilitating communication between different\nscientific disciplines. It is therefore unsurprising that there is a burgeoning\ninterest in applying category theory to machine learning. We aim to document\nthe motivations, goals and common themes across these applications. We touch on\ngradient-based learning, probability, and equivariant learning.", "authors": ["Dan Shiebler", "Bruno Gavranovi\u0107", "Paul Wilson"], "published": "2021-06-13T15:58:13+00:00", "updated": "2021-06-13T15:58:13+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/2106.07032v1", "entry_id": "http://arxiv.org/abs/2106.07032v1"}, {"id": "2201.06921", "filepath": "papers/2201.06921.pdf", "title": "Can Machine Learning be Moral?", "abstract": "The ethics of Machine Learning has become an unavoidable topic in the AI\nCommunity. The deployment of machine learning systems in multiple social\ncontexts has resulted in a closer ethical scrutiny of the design, development,\nand application of these systems. The AI/ML community has come to terms with\nthe imperative to think about the ethical implications of machine learning, not\nonly as a product but also as a practice (Birhane, 2021; Shen et al. 2021). The\ncritical question that is troubling many debates is what can constitute an\nethically accountable machine learning system. In this paper we explore\npossibilities for ethical evaluation of machine learning methodologies. We\nscrutinize techniques, methods and technical practices in machine learning from\na relational ethics perspective, taking into consideration how machine learning\nsystems are part of the world and how they relate to different forms of agency.\nTaking a page from Phil Agre (1997) we use the notion of a critical technical\npractice as a means of analysis of machine learning approaches. Our radical\nproposal is that supervised learning appears to be the only machine learning\nmethod that is ethically defensible.", "authors": ["Miguel Sicart", "Irina Shklovski", "Mirabelle Jones"], "published": "2021-12-13T07:20:50+00:00", "updated": "2021-12-13T07:20:50+00:00", "categories": ["cs.CY", "cs.HC"], "pdf_url": "http://arxiv.org/pdf/2201.06921v1", "entry_id": "http://arxiv.org/abs/2201.06921v1"}, {"id": "1405.1304", "filepath": "papers/1405.1304.pdf", "title": "Application of Machine Learning Techniques in Aquaculture", "abstract": "In this paper we present applications of different machine learning\nalgorithms in aquaculture. Machine learning algorithms learn models from\nhistorical data. In aquaculture historical data are obtained from farm\npractices, yields, and environmental data sources. Associations between these\ndifferent variables can be obtained by applying machine learning algorithms to\nhistorical data. In this paper we present applications of different machine\nlearning algorithms in aquaculture applications.", "authors": ["Akhlaqur Rahman", "Sumaira Tasnim"], "published": "2014-05-03T14:26:42+00:00", "updated": "2014-05-03T14:26:42+00:00", "categories": ["cs.CE", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1405.1304v1", "entry_id": "http://arxiv.org/abs/1405.1304v1"}, {"id": "1802.03830", "filepath": "papers/1802.03830.pdf", "title": "Distributed Stochastic Multi-Task Learning with Graph Regularization", "abstract": "We propose methods for distributed graph-based multi-task learning that are\nbased on weighted averaging of messages from other machines. Uniform averaging\nor diminishing stepsize in these methods would yield consensus (single task)\nlearning. We show how simply skewing the averaging weights or controlling the\nstepsize allows learning different, but related, tasks on the different\nmachines.", "authors": ["Weiran Wang", "Jialei Wang", "Mladen Kolar", "Nathan Srebro"], "published": "2018-02-11T22:23:34+00:00", "updated": "2018-02-11T22:23:34+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1802.03830v1", "entry_id": "http://arxiv.org/abs/1802.03830v1"}, {"id": "1810.11383", "filepath": "papers/1810.11383.pdf", "title": "Some Requests for Machine Learning Research from the East African Tech Scene", "abstract": "Based on 46 in-depth interviews with scientists, engineers, and CEOs, this\ndocument presents a list of concrete machine research problems, progress on\nwhich would directly benefit tech ventures in East Africa.", "authors": ["Milan Cvitkovic"], "published": "2018-10-25T02:53:14+00:00", "updated": "2018-11-08T01:03:50+00:00", "categories": ["cs.LG", "cs.CY", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1810.11383v2", "entry_id": "http://arxiv.org/abs/1810.11383v2"}, {"id": "2103.00742", "filepath": "papers/2103.00742.pdf", "title": "Automated Machine Learning on Graphs: A Survey", "abstract": "Machine learning on graphs has been extensively studied in both academic and\nindustry. However, as the literature on graph learning booms with a vast number\nof emerging methods and techniques, it becomes increasingly difficult to\nmanually design the optimal machine learning algorithm for different\ngraph-related tasks. To solve this critical challenge, automated machine\nlearning (AutoML) on graphs which combines the strength of graph machine\nlearning and AutoML together, is gaining attention from the research community.\nTherefore, we comprehensively survey AutoML on graphs in this paper, primarily\nfocusing on hyper-parameter optimization (HPO) and neural architecture search\n(NAS) for graph machine learning. We further overview libraries related to\nautomated graph machine learning and in-depth discuss AutoGL, the first\ndedicated open-source library for AutoML on graphs. In the end, we share our\ninsights on future research directions for automated graph machine learning.\nThis paper is the first systematic and comprehensive review of automated\nmachine learning on graphs to the best of our knowledge.", "authors": ["Ziwei Zhang", "Xin Wang", "Wenwu Zhu"], "published": "2021-03-01T04:20:33+00:00", "updated": "2021-12-20T05:03:30+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/2103.00742v4", "entry_id": "http://arxiv.org/abs/2103.00742v4"}, {"id": "2502.01708", "filepath": "papers/2502.01708.pdf", "title": "Aspects of Artificial Intelligence: Transforming Machine Learning Systems Naturally", "abstract": "In this paper, we study the machine learning elements which we are interested\nin together as a machine learning system, consisting of a collection of machine\nlearning elements and a collection of relations between the elements. The\nrelations we concern are algebraic operations, binary relations, and binary\nrelations with composition that can be reasoned categorically. A machine\nlearning system transformation between two systems is a map between the\nsystems, which preserves the relations we concern. The system transformations\ngiven by quotient or clustering, representable functor, and Yoneda embedding\nare highlighted and discussed by machine learning examples. An adjunction\nbetween machine learning systems, a special machine learning system\ntransformation loop, provides the optimal way of solving problems. Machine\nlearning system transformations are linked and compared by their maps at\n2-cell, natural transformations. New insights and structures can be obtained\nfrom universal properties and algebraic structures given by monads, which are\ngenerated from adjunctions.", "authors": ["Xiuzhan Guo"], "published": "2025-02-03T14:45:02+00:00", "updated": "2025-02-03T14:45:02+00:00", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.DM"], "pdf_url": "http://arxiv.org/pdf/2502.01708v1", "entry_id": "http://arxiv.org/abs/2502.01708v1"}, {"id": "1707.03184", "filepath": "papers/1707.03184.pdf", "title": "A Survey on Resilient Machine Learning", "abstract": "Machine learning based system are increasingly being used for sensitive tasks\nsuch as security surveillance, guiding autonomous vehicle, taking investment\ndecisions, detecting and blocking network intrusion and malware etc. However,\nrecent research has shown that machine learning models are venerable to attacks\nby adversaries at all phases of machine learning (eg, training data collection,\ntraining, operation). All model classes of machine learning systems can be\nmisled by providing carefully crafted inputs making them wrongly classify\ninputs. Maliciously created input samples can affect the learning process of a\nML system by either slowing down the learning process, or affecting the\nperformance of the learned mode, or causing the system make error(s) only in\nattacker's planned scenario. Because of these developments, understanding\nsecurity of machine learning algorithms and systems is emerging as an important\nresearch area among computer security and machine learning researchers and\npractitioners. We present a survey of this emerging area in machine learning.", "authors": ["Atul Kumar", "Sameep Mehta"], "published": "2017-07-11T09:15:46+00:00", "updated": "2017-07-11T09:15:46+00:00", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1707.03184v1", "entry_id": "http://arxiv.org/abs/1707.03184v1"}, {"id": "1612.04251", "filepath": "papers/1612.04251.pdf", "title": "TF.Learn: TensorFlow's High-level Module for Distributed Machine Learning", "abstract": "TF.Learn is a high-level Python module for distributed machine learning\ninside TensorFlow. It provides an easy-to-use Scikit-learn style interface to\nsimplify the process of creating, configuring, training, evaluating, and\nexperimenting a machine learning model. TF.Learn integrates a wide range of\nstate-of-art machine learning algorithms built on top of TensorFlow's low level\nAPIs for small to large-scale supervised and unsupervised problems. This module\nfocuses on bringing machine learning to non-specialists using a general-purpose\nhigh-level language as well as researchers who want to implement, benchmark,\nand compare their new methods in a structured environment. Emphasis is put on\nease of use, performance, documentation, and API consistency.", "authors": ["Yuan Tang"], "published": "2016-12-13T16:00:51+00:00", "updated": "2016-12-13T16:00:51+00:00", "categories": ["cs.DC", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1612.04251v1", "entry_id": "http://arxiv.org/abs/1612.04251v1"}, {"id": "1911.08587", "filepath": "papers/1911.08587.pdf", "title": "Solving machine learning optimization problems using quantum computers", "abstract": "Classical optimization algorithms in machine learning often take a long time\nto compute when applied to a multi-dimensional problem and require a huge\namount of CPU and GPU resource. Quantum parallelism has a potential to speed up\nmachine learning algorithms. We describe a generic mathematical model to\nleverage quantum parallelism to speed-up machine learning algorithms. We also\napply quantum machine learning and quantum parallelism applied to a\n$3$-dimensional image that vary with time.", "authors": ["Venkat R. Dasari", "Mee Seong Im", "Lubjana Beshaj"], "published": "2019-11-17T17:36:41+00:00", "updated": "2019-11-17T17:36:41+00:00", "categories": ["quant-ph", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1911.08587v1", "entry_id": "http://arxiv.org/abs/1911.08587v1"}, {"id": "1611.03969", "filepath": "papers/1611.03969.pdf", "title": "An Introduction to MM Algorithms for Machine Learning and Statistical", "abstract": "MM (majorization--minimization) algorithms are an increasingly popular tool\nfor solving optimization problems in machine learning and statistical\nestimation. This article introduces the MM algorithm framework in general and\nvia three popular example applications: Gaussian mixture regressions,\nmultinomial logistic regressions, and support vector machines. Specific\nalgorithms for the three examples are derived and numerical demonstrations are\npresented. Theoretical and practical aspects of MM algorithm design are\ndiscussed.", "authors": ["Hien D. Nguyen"], "published": "2016-11-12T08:18:38+00:00", "updated": "2016-11-12T08:18:38+00:00", "categories": ["stat.CO", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1611.03969v1", "entry_id": "http://arxiv.org/abs/1611.03969v1"}, {"id": "2310.11470", "filepath": "papers/2310.11470.pdf", "title": "Classic machine learning methods", "abstract": "In this chapter, we present the main classic machine learning methods. A\nlarge part of the chapter is devoted to supervised learning techniques for\nclassification and regression, including nearest-neighbor methods, linear and\nlogistic regressions, support vector machines and tree-based algorithms. We\nalso describe the problem of overfitting as well as strategies to overcome it.\nWe finally provide a brief overview of unsupervised learning methods, namely\nfor clustering and dimensionality reduction.", "authors": ["Johann Faouzi", "Olivier Colliot"], "published": "2023-05-24T13:38:38+00:00", "updated": "2023-05-24T13:38:38+00:00", "categories": ["cs.LG", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2310.11470v1", "entry_id": "http://arxiv.org/abs/2310.11470v1"}, {"id": "2407.05520", "filepath": "papers/2407.05520.pdf", "title": "A Theory of Machine Learning", "abstract": "We critically review three major theories of machine learning and provide a\nnew theory according to which machines learn a function when the machines\nsuccessfully compute it. We show that this theory challenges common assumptions\nin the statistical and the computational learning theories, for it implies that\nlearning true probabilities is equivalent neither to obtaining a correct\ncalculation of the true probabilities nor to obtaining an almost-sure\nconvergence to them. We also briefly discuss some case studies from natural\nlanguage processing and macroeconomics from the perspective of the new theory.", "authors": ["Jinsook Kim", "Jinho Kang"], "published": "2024-07-07T23:57:10+00:00", "updated": "2024-07-07T23:57:10+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2407.05520v1", "entry_id": "http://arxiv.org/abs/2407.05520v1"}, {"id": "1602.00198", "filepath": "papers/1602.00198.pdf", "title": "Discussion on Mechanical Learning and Learning Machine", "abstract": "Mechanical learning is a computing system that is based on a set of simple\nand fixed rules, and can learn from incoming data. A learning machine is a\nsystem that realizes mechanical learning. Importantly, we emphasis that it is\nbased on a set of simple and fixed rules, contrasting to often called machine\nlearning that is sophisticated software based on very complicated mathematical\ntheory, and often needs human intervene for software fine tune and manual\nadjustments. Here, we discuss some basic facts and principles of such system,\nand try to lay down a framework for further study. We propose 2 directions to\napproach mechanical learning, just like Church-Turing pair: one is trying to\nrealize a learning machine, another is trying to well describe the mechanical\nlearning.", "authors": ["Chuyu Xiong"], "published": "2016-01-31T04:05:50+00:00", "updated": "2016-01-31T04:05:50+00:00", "categories": ["cs.AI"], "pdf_url": "http://arxiv.org/pdf/1602.00198v1", "entry_id": "http://arxiv.org/abs/1602.00198v1"}, {"id": "1910.02544", "filepath": "papers/1910.02544.pdf", "title": "Using Deep Learning and Machine Learning to Detect Epileptic Seizure with Electroencephalography (EEG) Data", "abstract": "The prediction of epileptic seizure has always been extremely challenging in\nmedical domain. However, as the development of computer technology, the\napplication of machine learning introduced new ideas for seizure forecasting.\nApplying machine learning model onto the predication of epileptic seizure could\nhelp us obtain a better result and there have been plenty of scientists who\nhave been doing such works so that there are sufficient medical data provided\nfor researchers to do training of machine learning models.", "authors": ["Haotian Liu", "Lin Xi", "Ying Zhao", "Zhixiang Li"], "published": "2019-10-06T22:53:28+00:00", "updated": "2019-10-06T22:53:28+00:00", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1910.02544v1", "entry_id": "http://arxiv.org/abs/1910.02544v1"}, {"id": "1605.07805", "filepath": "papers/1605.07805.pdf", "title": "Learning Moore Machines from Input-Output Traces", "abstract": "The problem of learning automata from example traces (but no equivalence or\nmembership queries) is fundamental in automata learning theory and practice. In\nthis paper we study this problem for finite state machines with inputs and\noutputs, and in particular for Moore machines. We develop three algorithms for\nsolving this problem: (1) the PTAP algorithm, which transforms a set of\ninput-output traces into an incomplete Moore machine and then completes the\nmachine with self-loops; (2) the PRPNI algorithm, which uses the well-known\nRPNI algorithm for automata learning to learn a product of automata encoding a\nMoore machine; and (3) the MooreMI algorithm, which directly learns a Moore\nmachine using PTAP extended with state merging. We prove that MooreMI has the\nfundamental identification in the limit property. We also compare the\nalgorithms experimentally in terms of the size of the learned machine and\nseveral notions of accuracy, introduced in this paper. Finally, we compare with\nOSTIA, an algorithm that learns a more general class of transducers, and find\nthat OSTIA generally does not learn a Moore machine, even when fed with a\ncharacteristic sample.", "authors": ["Georgios Giantamidis", "Stavros Tripakis"], "published": "2016-05-25T10:11:03+00:00", "updated": "2016-09-02T09:27:40+00:00", "categories": ["cs.FL", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1605.07805v2", "entry_id": "http://arxiv.org/abs/1605.07805v2"}, {"id": "1807.06722", "filepath": "papers/1807.06722.pdf", "title": "Machine Learning Interpretability: A Science rather than a tool", "abstract": "The term \"interpretability\" is oftenly used by machine learning researchers\neach with their own intuitive understanding of it. There is no universal well\nagreed upon definition of interpretability in machine learning. As any type of\nscience discipline is mainly driven by the set of formulated questions rather\nthan by different tools in that discipline, e.g. astrophysics is the discipline\nthat learns the composition of stars, not as the discipline that use the\nspectroscopes. Similarly, we propose that machine learning interpretability\nshould be a discipline that answers specific questions related to\ninterpretability. These questions can be of statistical, causal and\ncounterfactual nature. Therefore, there is a need to look into the\ninterpretability problem of machine learning in the context of questions that\nneed to be addressed rather than different tools. We discuss about a\nhypothetical interpretability framework driven by a question based scientific\napproach rather than some specific machine learning model. Using a question\nbased notion of interpretability, we can step towards understanding the science\nof machine learning rather than its engineering. This notion will also help us\nunderstanding any specific problem more in depth rather than relying solely on\nmachine learning methods.", "authors": ["Abdul Karim", "Avinash Mishra", "MA Hakim Newton", "Abdul Sattar"], "published": "2018-07-18T00:50:18+00:00", "updated": "2018-07-25T07:23:45+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1807.06722v2", "entry_id": "http://arxiv.org/abs/1807.06722v2"}, {"id": "1908.00868", "filepath": "papers/1908.00868.pdf", "title": "Machine Learning as Ecology", "abstract": "Machine learning methods have had spectacular success on numerous problems.\nHere we show that a prominent class of learning algorithms - including Support\nVector Machines (SVMs) -- have a natural interpretation in terms of ecological\ndynamics. We use these ideas to design new online SVM algorithms that exploit\necological invasions, and benchmark performance using the MNIST dataset. Our\nwork provides a new ecological lens through which we can view statistical\nlearning and opens the possibility of designing ecosystems for machine\nlearning.\n  Supplemental code is found at https://github.com/owenhowell20/EcoSVM.", "authors": ["Owen Howell", "Cui Wenping", "Robert Marsland III", "Pankaj Mehta"], "published": "2019-08-02T14:08:17+00:00", "updated": "2019-08-23T13:52:08+00:00", "categories": ["cs.LG", "cond-mat.stat-mech", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1908.00868v2", "entry_id": "http://arxiv.org/abs/1908.00868v2"}, {"id": "1803.10311", "filepath": "papers/1803.10311.pdf", "title": "How Developers Iterate on Machine Learning Workflows -- A Survey of the Applied Machine Learning Literature", "abstract": "Machine learning workflow development is anecdotally regarded to be an\niterative process of trial-and-error with humans-in-the-loop. However, we are\nnot aware of quantitative evidence corroborating this popular belief. A\nquantitative characterization of iteration can serve as a benchmark for machine\nlearning workflow development in practice, and can aid the development of\nhuman-in-the-loop machine learning systems. To this end, we conduct a\nsmall-scale survey of the applied machine learning literature from five\ndistinct application domains. We collect and distill statistics on the role of\niteration within machine learning workflow development, and report preliminary\ntrends and insights from our investigation, as a starting point towards this\nbenchmark. Based on our findings, we finally describe desiderata for effective\nand versatile human-in-the-loop machine learning systems that can cater to\nusers in diverse domains.", "authors": ["Doris Xin", "Litian Ma", "Shuchen Song", "Aditya Parameswaran"], "published": "2018-03-27T20:38:05+00:00", "updated": "2018-05-17T22:16:31+00:00", "categories": ["cs.LG", "cs.DB", "cs.HC", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1803.10311v2", "entry_id": "http://arxiv.org/abs/1803.10311v2"}, {"id": "1912.09630", "filepath": "papers/1912.09630.pdf", "title": "Practical Solutions for Machine Learning Safety in Autonomous Vehicles", "abstract": "Autonomous vehicles rely on machine learning to solve challenging tasks in\nperception and motion planning. However, automotive software safety standards\nhave not fully evolved to address the challenges of machine learning safety\nsuch as interpretability, verification, and performance limitations. In this\npaper, we review and organize practical machine learning safety techniques that\ncan complement engineering safety for machine learning based software in\nautonomous vehicles. Our organization maps safety strategies to\nstate-of-the-art machine learning techniques in order to enhance dependability\nand safety of machine learning algorithms. We also discuss security limitations\nand user experience aspects of machine learning components in autonomous\nvehicles.", "authors": ["Sina Mohseni", "Mandar Pitale", "Vasu Singh", "Zhangyang Wang"], "published": "2019-12-20T03:47:28+00:00", "updated": "2019-12-20T03:47:28+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1912.09630v1", "entry_id": "http://arxiv.org/abs/1912.09630v1"}, {"id": "2409.03632", "filepath": "papers/2409.03632.pdf", "title": "Beyond Model Interpretability: Socio-Structural Explanations in Machine Learning", "abstract": "What is it to interpret the outputs of an opaque machine learning model. One\napproach is to develop interpretable machine learning techniques. These\ntechniques aim to show how machine learning models function by providing either\nmodel centric local or global explanations, which can be based on mechanistic\ninterpretations revealing the inner working mechanisms of models or\nnonmechanistic approximations showing input feature output data relationships.\nIn this paper, we draw on social philosophy to argue that interpreting machine\nlearning outputs in certain normatively salient domains could require appealing\nto a third type of explanation that we call sociostructural explanation. The\nrelevance of this explanation type is motivated by the fact that machine\nlearning models are not isolated entities but are embedded within and shaped by\nsocial structures. Sociostructural explanations aim to illustrate how social\nstructures contribute to and partially explain the outputs of machine learning\nmodels. We demonstrate the importance of sociostructural explanations by\nexamining a racially biased healthcare allocation algorithm. Our proposal\nhighlights the need for transparency beyond model interpretability,\nunderstanding the outputs of machine learning systems could require a broader\nanalysis that extends beyond the understanding of the machine learning model\nitself.", "authors": ["Andrew Smart", "Atoosa Kasirzadeh"], "published": "2024-09-05T15:47:04+00:00", "updated": "2024-09-05T15:47:04+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/2409.03632v1", "entry_id": "http://arxiv.org/abs/2409.03632v1"}, {"id": "2003.10146", "filepath": "papers/2003.10146.pdf", "title": "Julia Language in Machine Learning: Algorithms, Applications, and Open Issues", "abstract": "Machine learning is driving development across many fields in science and\nengineering. A simple and efficient programming language could accelerate\napplications of machine learning in various fields. Currently, the programming\nlanguages most commonly used to develop machine learning algorithms include\nPython, MATLAB, and C/C ++. However, none of these languages well balance both\nefficiency and simplicity. The Julia language is a fast, easy-to-use, and\nopen-source programming language that was originally designed for\nhigh-performance computing, which can well balance the efficiency and\nsimplicity. This paper summarizes the related research work and developments in\nthe application of the Julia language in machine learning. It first surveys the\npopular machine learning algorithms that are developed in the Julia language.\nThen, it investigates applications of the machine learning algorithms\nimplemented with the Julia language. Finally, it discusses the open issues and\nthe potential future directions that arise in the use of the Julia language in\nmachine learning.", "authors": ["Kaifeng Gao", "Gang Mei", "Francesco Piccialli", "Salvatore Cuomo", "Jingzhi Tu", "Zenan Huo"], "published": "2020-03-23T09:31:02+00:00", "updated": "2020-05-17T10:52:22+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2003.10146v2", "entry_id": "http://arxiv.org/abs/2003.10146v2"}, {"id": "1902.04622", "filepath": "papers/1902.04622.pdf", "title": "Learning Theory and Support Vector Machines - a primer", "abstract": "The main goal of statistical learning theory is to provide a fundamental\nframework for the problem of decision making and model construction based on\nsets of data. Here, we present a brief introduction to the fundamentals of\nstatistical learning theory, in particular the difference between empirical and\nstructural risk minimization, including one of its most prominent\nimplementations, i.e. the Support Vector Machine.", "authors": ["Michael Banf"], "published": "2019-02-12T20:28:09+00:00", "updated": "2019-02-12T20:28:09+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1902.04622v1", "entry_id": "http://arxiv.org/abs/1902.04622v1"}, {"id": "2312.14050", "filepath": "papers/2312.14050.pdf", "title": "Machine learning and domain decomposition methods -- a survey", "abstract": "Hybrid algorithms, which combine black-box machine learning methods with\nexperience from traditional numerical methods and domain expertise from diverse\napplication areas, are progressively gaining importance in scientific machine\nlearning and various industrial domains, especially in computational science\nand engineering. In the present survey, several promising avenues of research\nwill be examined which focus on the combination of machine learning (ML) and\ndomain decomposition methods (DDMs). The aim of this survey is to provide an\noverview of existing work within this field and to structure it into domain\ndecomposition for machine learning and machine learning-enhanced domain\ndecomposition, including: domain decomposition for classical machine learning,\ndomain decomposition to accelerate the training of physics-aware neural\nnetworks, machine learning to enhance the convergence properties or\ncomputational efficiency of DDMs, and machine learning as a discretization\nmethod in a DDM for the solution of PDEs. In each of these fields, we summarize\nexisting work and key advances within a common framework and, finally, disuss\nongoing challenges and opportunities for future research.", "authors": ["Axel Klawonn", "Martin Lanser", "Janine Weber"], "published": "2023-12-21T17:19:27+00:00", "updated": "2023-12-21T17:19:27+00:00", "categories": ["math.NA", "cs.LG", "cs.NA", "65F10, 65N22, 65N55, 68T05, 68T07"], "pdf_url": "http://arxiv.org/pdf/2312.14050v1", "entry_id": "http://arxiv.org/abs/2312.14050v1"}, {"id": "1501.04309", "filepath": "papers/1501.04309.pdf", "title": "Information Theory and its Relation to Machine Learning", "abstract": "In this position paper, I first describe a new perspective on machine\nlearning (ML) by four basic problems (or levels), namely, \"What to learn?\",\n\"How to learn?\", \"What to evaluate?\", and \"What to adjust?\". The paper stresses\nmore on the first level of \"What to learn?\", or \"Learning Target Selection\".\nTowards this primary problem within the four levels, I briefly review the\nexisting studies about the connection between information theoretical learning\n(ITL [1]) and machine learning. A theorem is given on the relation between the\nempirically-defined similarity measure and information measures. Finally, a\nconjecture is proposed for pursuing a unified mathematical interpretation to\nlearning target selection.", "authors": ["Bao-Gang Hu"], "published": "2015-01-18T14:57:02+00:00", "updated": "2015-01-18T14:57:02+00:00", "categories": ["cs.IT", "cs.LG", "math.IT"], "pdf_url": "http://arxiv.org/pdf/1501.04309v1", "entry_id": "http://arxiv.org/abs/1501.04309v1"}, {"id": "2006.15680", "filepath": "papers/2006.15680.pdf", "title": "Modeling Generalization in Machine Learning: A Methodological and Computational Study", "abstract": "As machine learning becomes more and more available to the general public,\ntheoretical questions are turning into pressing practical issues. Possibly, one\nof the most relevant concerns is the assessment of our confidence in trusting\nmachine learning predictions. In many real-world cases, it is of utmost\nimportance to estimate the capabilities of a machine learning algorithm to\ngeneralize, i.e., to provide accurate predictions on unseen data, depending on\nthe characteristics of the target problem. In this work, we perform a\nmeta-analysis of 109 publicly-available classification data sets, modeling\nmachine learning generalization as a function of a variety of data set\ncharacteristics, ranging from number of samples to intrinsic dimensionality,\nfrom class-wise feature skewness to $F1$ evaluated on test samples falling\noutside the convex hull of the training set. Experimental results demonstrate\nthe relevance of using the concept of the convex hull of the training data in\nassessing machine learning generalization, by emphasizing the difference\nbetween interpolated and extrapolated predictions. Besides several predictable\ncorrelations, we observe unexpectedly weak associations between the\ngeneralization ability of machine learning models and all metrics related to\ndimensionality, thus challenging the common assumption that the \\textit{curse\nof dimensionality} might impair generalization in machine learning.", "authors": ["Pietro Barbiero", "Giovanni Squillero", "Alberto Tonda"], "published": "2020-06-28T19:06:16+00:00", "updated": "2020-06-28T19:06:16+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2006.15680v1", "entry_id": "http://arxiv.org/abs/2006.15680v1"}, {"id": "1812.01410", "filepath": "papers/1812.01410.pdf", "title": "Compressive Classification (Machine Learning without learning)", "abstract": "Compressive learning is a framework where (so far unsupervised) learning\ntasks use not the entire dataset but a compressed summary (sketch) of it. We\npropose a compressive learning classification method, and a novel sketch\nfunction for images.", "authors": ["Vincent Schellekens", "Laurent Jacques"], "published": "2018-12-04T13:50:11+00:00", "updated": "2018-12-04T13:50:11+00:00", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1812.01410v1", "entry_id": "http://arxiv.org/abs/1812.01410v1"}, {"id": "1907.03010", "filepath": "papers/1907.03010.pdf", "title": "Financial Time Series Data Processing for Machine Learning", "abstract": "This article studies the financial time series data processing for machine\nlearning. It introduces the most frequent scaling methods, then compares the\nresulting stationarity and preservation of useful information for trend\nforecasting. It proposes an empirical test based on the capability to learn\nsimple data relationship with simple models. It also speaks about the data\nsplit method specific to time series, avoiding unwanted overfitting and\nproposes various labelling for classification and regression.", "authors": ["Fabrice Daniel"], "published": "2019-07-03T15:10:23+00:00", "updated": "2019-07-03T15:10:23+00:00", "categories": ["q-fin.ST", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1907.03010v1", "entry_id": "http://arxiv.org/abs/1907.03010v1"}, {"id": "2103.03122", "filepath": "papers/2103.03122.pdf", "title": "Machine Learning using Stata/Python", "abstract": "We present two related Stata modules, r_ml_stata and c_ml_stata, for fitting\npopular Machine Learning (ML) methods both in regression and classification\nsettings. Using the recent Stata/Python integration platform (sfi) of Stata 16,\nthese commands provide hyper-parameters' optimal tuning via K-fold\ncross-validation using greed search. More specifically, they make use of the\nPython Scikit-learn API to carry out both cross-validation and outcome/label\nprediction.", "authors": ["Giovanni Cerulli"], "published": "2021-03-03T10:31:44+00:00", "updated": "2021-03-03T10:31:44+00:00", "categories": ["stat.CO", "cs.LG", "cs.MS"], "pdf_url": "http://arxiv.org/pdf/2103.03122v1", "entry_id": "http://arxiv.org/abs/2103.03122v1"}, {"id": "2209.02057", "filepath": "papers/2209.02057.pdf", "title": "Applying Machine Learning to Life Insurance: some knowledge sharing to master it", "abstract": "Machine Learning permeates many industries, which brings new source of\nbenefits for companies. However within the life insurance industry, Machine\nLearning is not widely used in practice as over the past years statistical\nmodels have shown their efficiency for risk assessment. Thus insurers may face\ndifficulties to assess the value of the artificial intelligence. Focusing on\nthe modification of the life insurance industry over time highlights the stake\nof using Machine Learning for insurers and benefits that it can bring by\nunleashing data value. This paper reviews traditional actuarial methodologies\nfor survival modeling and extends them with Machine Learning techniques. It\npoints out differences with regular machine learning models and emphasizes\nimportance of specific implementations to face censored data with machine\nlearning models family. In complement to this article, a Python library has\nbeen developed. Different open-source Machine Learning algorithms have been\nadjusted to adapt the specificities of life insurance data, namely censoring\nand truncation. Such models can be easily applied from this SCOR library to\naccurately model life insurance risks.", "authors": ["Antoine Chancel", "Laura Bradier", "Antoine Ly", "Razvan Ionescu", "Laurene Martin", "Marguerite Sauce"], "published": "2022-09-05T17:09:03+00:00", "updated": "2022-09-27T18:42:50+00:00", "categories": ["stat.ML", "cs.CY", "cs.LG", "stat.AP"], "pdf_url": "http://arxiv.org/pdf/2209.02057v3", "entry_id": "http://arxiv.org/abs/2209.02057v3"}, {"id": "2105.03726", "filepath": "papers/2105.03726.pdf", "title": "Mental Models of Adversarial Machine Learning", "abstract": "Although machine learning is widely used in practice, little is known about\npractitioners' understanding of potential security challenges. In this work, we\nclose this substantial gap and contribute a qualitative study focusing on\ndevelopers' mental models of the machine learning pipeline and potentially\nvulnerable components. Similar studies have helped in other security fields to\ndiscover root causes or improve risk communication. Our study reveals two\n\\facets of practitioners' mental models of machine learning security. Firstly,\npractitioners often confuse machine learning security with threats and defences\nthat are not directly related to machine learning. Secondly, in contrast to\nmost academic research, our participants perceive security of machine learning\nas not solely related to individual models, but rather in the context of entire\nworkflows that consist of multiple components. Jointly with our additional\nfindings, these two facets provide a foundation to substantiate mental models\nfor machine learning security and have implications for the integration of\nadversarial machine learning into corporate workflows, \\new{decreasing\npractitioners' reported uncertainty}, and appropriate regulatory frameworks for\nmachine learning security.", "authors": ["Lukas Bieringer", "Kathrin Grosse", "Michael Backes", "Battista Biggio", "Katharina Krombholz"], "published": "2021-05-08T16:05:07+00:00", "updated": "2022-06-29T13:42:12+00:00", "categories": ["cs.CR", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2105.03726v4", "entry_id": "http://arxiv.org/abs/2105.03726v4"}, {"id": "1909.01866", "filepath": "papers/1909.01866.pdf", "title": "Understanding Bias in Machine Learning", "abstract": "Bias is known to be an impediment to fair decisions in many domains such as\nhuman resources, the public sector, health care etc. Recently, hope has been\nexpressed that the use of machine learning methods for taking such decisions\nwould diminish or even resolve the problem. At the same time, machine learning\nexperts warn that machine learning models can be biased as well. In this\narticle, our goal is to explain the issue of bias in machine learning from a\ntechnical perspective and to illustrate the impact that biased data can have on\na machine learning model. To reach such a goal, we develop interactive plots to\nvisualizing the bias learned from synthetic data.", "authors": ["Jindong Gu", "Daniela Oelke"], "published": "2019-09-02T20:36:19+00:00", "updated": "2019-09-02T20:36:19+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1909.01866v1", "entry_id": "http://arxiv.org/abs/1909.01866v1"}, {"id": "2303.09491", "filepath": "papers/2303.09491.pdf", "title": "Challenges and Opportunities in Quantum Machine Learning", "abstract": "At the intersection of machine learning and quantum computing, Quantum\nMachine Learning (QML) has the potential of accelerating data analysis,\nespecially for quantum data, with applications for quantum materials,\nbiochemistry, and high-energy physics. Nevertheless, challenges remain\nregarding the trainability of QML models. Here we review current methods and\napplications for QML. We highlight differences between quantum and classical\nmachine learning, with a focus on quantum neural networks and quantum deep\nlearning. Finally, we discuss opportunities for quantum advantage with QML.", "authors": ["M. Cerezo", "Guillaume Verdon", "Hsin-Yuan Huang", "Lukasz Cincio", "Patrick J. Coles"], "published": "2023-03-16T17:10:39+00:00", "updated": "2023-03-16T17:10:39+00:00", "categories": ["quant-ph", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2303.09491v1", "entry_id": "http://arxiv.org/abs/2303.09491v1"}, {"id": "2206.13446", "filepath": "papers/2206.13446.pdf", "title": "Pen and Paper Exercises in Machine Learning", "abstract": "This is a collection of (mostly) pen-and-paper exercises in machine learning.\nThe exercises are on the following topics: linear algebra, optimisation,\ndirected graphical models, undirected graphical models, expressive power of\ngraphical models, factor graphs and message passing, inference for hidden\nMarkov models, model-based learning (including ICA and unnormalised models),\nsampling and Monte-Carlo integration, and variational inference.", "authors": ["Michael U. Gutmann"], "published": "2022-06-27T16:53:18+00:00", "updated": "2022-06-27T16:53:18+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2206.13446v1", "entry_id": "http://arxiv.org/abs/2206.13446v1"}, {"id": "2204.07492", "filepath": "papers/2204.07492.pdf", "title": "A Machine Learning Tutorial for Operational Meteorology, Part I: Traditional Machine Learning", "abstract": "Recently, the use of machine learning in meteorology has increased greatly.\nWhile many machine learning methods are not new, university classes on machine\nlearning are largely unavailable to meteorology students and are not required\nto become a meteorologist. The lack of formal instruction has contributed to\nperception that machine learning methods are 'black boxes' and thus end-users\nare hesitant to apply the machine learning methods in their every day workflow.\nTo reduce the opaqueness of machine learning methods and lower hesitancy\ntowards machine learning in meteorology, this paper provides a survey of some\nof the most common machine learning methods. A familiar meteorological example\nis used to contextualize the machine learning methods while also discussing\nmachine learning topics using plain language. The following machine learning\nmethods are demonstrated: linear regression; logistic regression; decision\ntrees; random forest; gradient boosted decision trees; naive Bayes; and support\nvector machines. Beyond discussing the different methods, the paper also\ncontains discussions on the general machine learning process as well as best\npractices to enable readers to apply machine learning to their own datasets.\nFurthermore, all code (in the form of Jupyter notebooks and Google Colaboratory\nnotebooks) used to make the examples in the paper is provided in an effort to\ncatalyse the use of machine learning in meteorology.", "authors": ["Randy J. Chase", "David R. Harrison", "Amanda Burke", "Gary M. Lackmann", "Amy McGovern"], "published": "2022-04-15T14:48:04+00:00", "updated": "2022-06-07T14:48:09+00:00", "categories": ["physics.ao-ph", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2204.07492v2", "entry_id": "http://arxiv.org/abs/2204.07492v2"}, {"id": "2301.09753", "filepath": "papers/2301.09753.pdf", "title": "Towards Modular Machine Learning Solution Development: Benefits and Trade-offs", "abstract": "Machine learning technologies have demonstrated immense capabilities in\nvarious domains. They play a key role in the success of modern businesses.\nHowever, adoption of machine learning technologies has a lot of untouched\npotential. Cost of developing custom machine learning solutions that solve\nunique business problems is a major inhibitor to far-reaching adoption of\nmachine learning technologies. We recognize that the monolithic nature\nprevalent in today's machine learning applications stands in the way of\nefficient and cost effective customized machine learning solution development.\nIn this work we explore the benefits of modular machine learning solutions and\ndiscuss how modular machine learning solutions can overcome some of the major\nsolution engineering limitations of monolithic machine learning solutions. We\nanalyze the trade-offs between modular and monolithic machine learning\nsolutions through three deep learning problems; one text based and the two\nimage based. Our experimental results show that modular machine learning\nsolutions have a promising potential to reap the solution engineering\nadvantages of modularity while gaining performance and data advantages in a way\nthe monolithic machine learning solutions do not permit.", "authors": ["Samiyuru Menik", "Lakshmish Ramaswamy"], "published": "2023-01-23T22:54:34+00:00", "updated": "2023-01-23T22:54:34+00:00", "categories": ["cs.LG", "cs.SE"], "pdf_url": "http://arxiv.org/pdf/2301.09753v1", "entry_id": "http://arxiv.org/abs/2301.09753v1"}, {"id": "2407.19890", "filepath": "papers/2407.19890.pdf", "title": "Quantum Dynamics of Machine Learning", "abstract": "The quantum dynamic equation (QDE) of machine learning is obtained based on\nSchr\\\"odinger equation and potential energy equivalence relationship. Through\nWick rotation, the relationship between quantum dynamics and thermodynamics is\nalso established in this paper. This equation reformulates the iterative\nprocess of machine learning into a time-dependent partial differential equation\nwith a clear mathematical structure, offering a theoretical framework for\ninvestigating machine learning iterations through quantum and mathematical\ntheories. Within this framework, the fundamental iterative process, the\ndiffusion model, and the Softmax and Sigmoid functions are examined, validating\nthe proposed quantum dynamics equations. This approach not only presents a\nrigorous theoretical foundation for machine learning but also holds promise for\nsupporting the implementation of machine learning algorithms on quantum\ncomputers.", "authors": ["Peng Wang", "Maimaitiniyazi Maimaitiabudula"], "published": "2024-07-07T16:30:46+00:00", "updated": "2024-07-07T16:30:46+00:00", "categories": ["quant-ph", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2407.19890v1", "entry_id": "http://arxiv.org/abs/2407.19890v1"}, {"id": "1207.4676", "filepath": "papers/1207.4676.pdf", "title": "Proceedings of the 29th International Conference on Machine Learning (ICML-12)", "abstract": "This is an index to the papers that appear in the Proceedings of the 29th\nInternational Conference on Machine Learning (ICML-12). The conference was held\nin Edinburgh, Scotland, June 27th - July 3rd, 2012.", "authors": ["John Langford", "Joelle Pineau"], "published": "2012-07-19T14:08:22+00:00", "updated": "2012-09-16T11:24:54+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1207.4676v2", "entry_id": "http://arxiv.org/abs/1207.4676v2"}, {"id": "2103.11249", "filepath": "papers/2103.11249.pdf", "title": "SELM: Software Engineering of Machine Learning Models", "abstract": "One of the pillars of any machine learning model is its concepts. Using\nsoftware engineering, we can engineer these concepts and then develop and\nexpand them. In this article, we present a SELM framework for Software\nEngineering of machine Learning Models. We then evaluate this framework through\na case study. Using the SELM framework, we can improve a machine learning\nprocess efficiency and provide more accuracy in learning with less processing\nhardware resources and a smaller training dataset. This issue highlights the\nimportance of an interdisciplinary approach to machine learning. Therefore, in\nthis article, we have provided interdisciplinary teams' proposals for machine\nlearning.", "authors": ["Nafiseh Jafari", "Mohammad Reza Besharati", "Mohammad Izadi", "Maryam Hourali"], "published": "2021-03-20T21:43:24+00:00", "updated": "2021-03-20T21:43:24+00:00", "categories": ["cs.SE", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2103.11249v1", "entry_id": "http://arxiv.org/abs/2103.11249v1"}, {"id": "2001.11489", "filepath": "papers/2001.11489.pdf", "title": "Machine Learning in Network Security Using KNIME Analytics", "abstract": "Machine learning has more and more effect on our every day's life. This field\nkeeps growing and expanding into new areas. Machine learning is based on the\nimplementation of artificial intelligence that gives systems the capability to\nautomatically learn and enhance from experiments without being explicitly\nprogrammed. Machine Learning algorithms apply mathematical equations to analyze\ndatasets and predict values based on the dataset. In the field of\ncybersecurity, machine learning algorithms can be utilized to train and analyze\nthe Intrusion Detection Systems (IDSs) on security-related datasets. In this\npaper, we tested different machine learning algorithms to analyze NSL-KDD\ndataset using KNIME analytics.", "authors": ["Munther Abualkibash"], "published": "2019-11-18T14:10:17+00:00", "updated": "2019-11-18T14:10:17+00:00", "categories": ["cs.CR"], "pdf_url": "http://arxiv.org/pdf/2001.11489v1", "entry_id": "http://arxiv.org/abs/2001.11489v1"}, {"id": "2110.12773", "filepath": "papers/2110.12773.pdf", "title": "Scientific Machine Learning Benchmarks", "abstract": "The breakthrough in Deep Learning neural networks has transformed the use of\nAI and machine learning technologies for the analysis of very large\nexperimental datasets. These datasets are typically generated by large-scale\nexperimental facilities at national laboratories. In the context of science,\nscientific machine learning focuses on training machines to identify patterns,\ntrends, and anomalies to extract meaningful scientific insights from such\ndatasets. With a new generation of experimental facilities, the rate of data\ngeneration and the scale of data volumes will increasingly require the use of\nmore automated data analysis. At present, identifying the most appropriate\nmachine learning algorithm for the analysis of any given scientific dataset is\nstill a challenge for scientists. This is due to many different machine\nlearning frameworks, computer architectures, and machine learning models.\nHistorically, for modelling and simulation on HPC systems such problems have\nbeen addressed through benchmarking computer applications, algorithms, and\narchitectures. Extending such a benchmarking approach and identifying metrics\nfor the application of machine learning methods to scientific datasets is a new\nchallenge for both scientists and computer scientists. In this paper, we\ndescribe our approach to the development of scientific machine learning\nbenchmarks and review other approaches to benchmarking scientific machine\nlearning.", "authors": ["Jeyan Thiyagalingam", "Mallikarjun Shankar", "Geoffrey Fox", "Tony Hey"], "published": "2021-10-25T10:05:11+00:00", "updated": "2021-10-25T10:05:11+00:00", "categories": ["cs.LG", "physics.comp-ph", "I.2"], "pdf_url": "http://arxiv.org/pdf/2110.12773v1", "entry_id": "http://arxiv.org/abs/2110.12773v1"}, {"id": "1910.02544", "filepath": "papers/1910.02544.pdf", "title": "Using Deep Learning and Machine Learning to Detect Epileptic Seizure with Electroencephalography (EEG) Data", "abstract": "The prediction of epileptic seizure has always been extremely challenging in\nmedical domain. However, as the development of computer technology, the\napplication of machine learning introduced new ideas for seizure forecasting.\nApplying machine learning model onto the predication of epileptic seizure could\nhelp us obtain a better result and there have been plenty of scientists who\nhave been doing such works so that there are sufficient medical data provided\nfor researchers to do training of machine learning models.", "authors": ["Haotian Liu", "Lin Xi", "Ying Zhao", "Zhixiang Li"], "published": "2019-10-06T22:53:28+00:00", "updated": "2019-10-06T22:53:28+00:00", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1910.02544v1", "entry_id": "http://arxiv.org/abs/1910.02544v1"}, {"id": "2002.12364", "filepath": "papers/2002.12364.pdf", "title": "Theoretical Models of Learning to Learn", "abstract": "A Machine can only learn if it is biased in some way. Typically the bias is\nsupplied by hand, for example through the choice of an appropriate set of\nfeatures. However, if the learning machine is embedded within an {\\em\nenvironment} of related tasks, then it can {\\em learn} its own bias by learning\nsufficiently many tasks from the environment. In this paper two models of bias\nlearning (or equivalently, learning to learn) are introduced and the main\ntheoretical results presented. The first model is a PAC-type model based on\nempirical process theory, while the second is a hierarchical Bayes model.", "authors": ["Jonathan Baxter"], "published": "2020-02-27T13:35:26+00:00", "updated": "2020-02-27T13:35:26+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2002.12364v1", "entry_id": "http://arxiv.org/abs/2002.12364v1"}, {"id": "1908.00868", "filepath": "papers/1908.00868.pdf", "title": "Machine Learning as Ecology", "abstract": "Machine learning methods have had spectacular success on numerous problems.\nHere we show that a prominent class of learning algorithms - including Support\nVector Machines (SVMs) -- have a natural interpretation in terms of ecological\ndynamics. We use these ideas to design new online SVM algorithms that exploit\necological invasions, and benchmark performance using the MNIST dataset. Our\nwork provides a new ecological lens through which we can view statistical\nlearning and opens the possibility of designing ecosystems for machine\nlearning.\n  Supplemental code is found at https://github.com/owenhowell20/EcoSVM.", "authors": ["Owen Howell", "Cui Wenping", "Robert Marsland III", "Pankaj Mehta"], "published": "2019-08-02T14:08:17+00:00", "updated": "2019-08-23T13:52:08+00:00", "categories": ["cs.LG", "cond-mat.stat-mech", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1908.00868v2", "entry_id": "http://arxiv.org/abs/1908.00868v2"}, {"id": "0904.3664", "filepath": "papers/0904.3664.pdf", "title": "Introduction to Machine Learning: Class Notes 67577", "abstract": "Introduction to Machine learning covering Statistical Inference (Bayes, EM,\nML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),\nand PAC learning (the Formal model, VC dimension, Double Sampling theorem).", "authors": ["Amnon Shashua"], "published": "2009-04-23T11:40:57+00:00", "updated": "2009-04-23T11:40:57+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/0904.3664v1", "entry_id": "http://arxiv.org/abs/0904.3664v1"}, {"id": "1903.08801", "filepath": "papers/1903.08801.pdf", "title": "A Unified Analytical Framework for Trustable Machine Learning and Automation Running with Blockchain", "abstract": "Traditional machine learning algorithms use data from databases that are\nmutable, and therefore the data cannot be fully trusted. Also, the machine\nlearning process is difficult to automate. This paper proposes building a\ntrustable machine learning system by using blockchain technology, which can\nstore data in a permanent and immutable way. In addition, smart contracts are\nused to automate the machine learning process. This paper makes three\ncontributions. First, it establishes a link between machine learning technology\nand blockchain technology. Previously, machine learning and blockchain have\nbeen considered two independent technologies without an obvious link. Second,\nit proposes a unified analytical framework for trustable machine learning by\nusing blockchain technology. This unified framework solves both the\ntrustability and automation issues in machine learning. Third, it enables a\ncomputer to translate core machine learning implementation from a single thread\non a single machine to multiple threads on multiple machines running with\nblockchain by using a unified approach. The paper uses association rule mining\nas an example to demonstrate how trustable machine learning can be implemented\nwith blockchain, and it shows how this approach can be used to analyze opioid\nprescriptions to help combat the opioid crisis.", "authors": ["Tao Wang"], "published": "2019-03-21T02:17:08+00:00", "updated": "2019-03-21T02:17:08+00:00", "categories": ["cs.LG", "cs.CR"], "pdf_url": "http://arxiv.org/pdf/1903.08801v1", "entry_id": "http://arxiv.org/abs/1903.08801v1"}, {"id": "1911.06612", "filepath": "papers/1911.06612.pdf", "title": "Position Paper: Towards Transparent Machine Learning", "abstract": "Transparent machine learning is introduced as an alternative form of machine\nlearning, where both the model and the learning system are represented in\nsource code form. The goal of this project is to enable direct human\nunderstanding of machine learning models, giving us the ability to learn,\nverify, and refine them as programs. If solved, this technology could represent\na best-case scenario for the safety and security of AI systems going forward.", "authors": ["Dustin Juliano"], "published": "2019-11-12T10:49:55+00:00", "updated": "2019-11-12T10:49:55+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/1911.06612v1", "entry_id": "http://arxiv.org/abs/1911.06612v1"}, {"id": "1612.04251", "filepath": "papers/1612.04251.pdf", "title": "TF.Learn: TensorFlow's High-level Module for Distributed Machine Learning", "abstract": "TF.Learn is a high-level Python module for distributed machine learning\ninside TensorFlow. It provides an easy-to-use Scikit-learn style interface to\nsimplify the process of creating, configuring, training, evaluating, and\nexperimenting a machine learning model. TF.Learn integrates a wide range of\nstate-of-art machine learning algorithms built on top of TensorFlow's low level\nAPIs for small to large-scale supervised and unsupervised problems. This module\nfocuses on bringing machine learning to non-specialists using a general-purpose\nhigh-level language as well as researchers who want to implement, benchmark,\nand compare their new methods in a structured environment. Emphasis is put on\nease of use, performance, documentation, and API consistency.", "authors": ["Yuan Tang"], "published": "2016-12-13T16:00:51+00:00", "updated": "2016-12-13T16:00:51+00:00", "categories": ["cs.DC", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1612.04251v1", "entry_id": "http://arxiv.org/abs/1612.04251v1"}, {"id": "2412.00464", "filepath": "papers/2412.00464.pdf", "title": "On the Conditions for Domain Stability for Machine Learning: a Mathematical Approach", "abstract": "This work proposes a mathematical approach that (re)defines a property of\nMachine Learning models named stability and determines sufficient conditions to\nvalidate it. Machine Learning models are represented as functions, and the\ncharacteristics in scope depend upon the domain of the function, what allows us\nto adopt topological and metric spaces theory as a basis. Finally, this work\nprovides some equivalences useful to prove and test stability in Machine\nLearning models. The results suggest that whenever stability is aligned with\nthe notion of function smoothness, then the stability of Machine Learning\nmodels primarily depends upon certain topological, measurable properties of the\nclassification sets within the ML model domain.", "authors": ["Gabriel Pedroza"], "published": "2024-11-30T12:57:07+00:00", "updated": "2024-11-30T12:57:07+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML", "F.4.1; I.2.0"], "pdf_url": "http://arxiv.org/pdf/2412.00464v1", "entry_id": "http://arxiv.org/abs/2412.00464v1"}, {"id": "1907.08908", "filepath": "papers/1907.08908.pdf", "title": "Techniques for Automated Machine Learning", "abstract": "Automated machine learning (AutoML) aims to find optimal machine learning\nsolutions automatically given a machine learning problem. It could release the\nburden of data scientists from the multifarious manual tuning process and\nenable the access of domain experts to the off-the-shelf machine learning\nsolutions without extensive experience. In this paper, we review the current\ndevelopments of AutoML in terms of three categories, automated feature\nengineering (AutoFE), automated model and hyperparameter learning (AutoMHL),\nand automated deep learning (AutoDL). State-of-the-art techniques adopted in\nthe three categories are presented, including Bayesian optimization,\nreinforcement learning, evolutionary algorithm, and gradient-based approaches.\nWe summarize popular AutoML frameworks and conclude with current open\nchallenges of AutoML.", "authors": ["Yi-Wei Chen", "Qingquan Song", "Xia Hu"], "published": "2019-07-21T04:03:36+00:00", "updated": "2019-07-21T04:03:36+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1907.08908v1", "entry_id": "http://arxiv.org/abs/1907.08908v1"}, {"id": "2007.14206", "filepath": "papers/2007.14206.pdf", "title": "Machine Learning Potential Repository", "abstract": "This paper introduces a machine learning potential repository that includes\nPareto optimal machine learning potentials. It also shows the systematic\ndevelopment of accurate and fast machine learning potentials for a wide range\nof elemental systems. As a result, many Pareto optimal machine learning\npotentials are available in the repository from a website. Therefore, the\nrepository will help many scientists to perform accurate and fast atomistic\nsimulations.", "authors": ["Atsuto Seko"], "published": "2020-07-27T14:30:23+00:00", "updated": "2020-07-27T14:30:23+00:00", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci", "physics.chem-ph", "physics.data-an"], "pdf_url": "http://arxiv.org/pdf/2007.14206v1", "entry_id": "http://arxiv.org/abs/2007.14206v1"}, {"id": "1405.1304", "filepath": "papers/1405.1304.pdf", "title": "Application of Machine Learning Techniques in Aquaculture", "abstract": "In this paper we present applications of different machine learning\nalgorithms in aquaculture. Machine learning algorithms learn models from\nhistorical data. In aquaculture historical data are obtained from farm\npractices, yields, and environmental data sources. Associations between these\ndifferent variables can be obtained by applying machine learning algorithms to\nhistorical data. In this paper we present applications of different machine\nlearning algorithms in aquaculture applications.", "authors": ["Akhlaqur Rahman", "Sumaira Tasnim"], "published": "2014-05-03T14:26:42+00:00", "updated": "2014-05-03T14:26:42+00:00", "categories": ["cs.CE", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1405.1304v1", "entry_id": "http://arxiv.org/abs/1405.1304v1"}, {"id": "2412.18979", "filepath": "papers/2412.18979.pdf", "title": "Quantum memristors for neuromorphic quantum machine learning", "abstract": "Quantum machine learning may permit to realize more efficient machine\nlearning calculations with near-term quantum devices. Among the diverse quantum\nmachine learning paradigms which are currently being considered, quantum\nmemristors are promising as a way of combining, in the same quantum hardware, a\nunitary evolution with the nonlinearity provided by the measurement and\nfeedforward. Thus, an efficient way of deploying neuromorphic quantum computing\nfor quantum machine learning may be enabled.", "authors": ["Lucas Lamata"], "published": "2024-12-25T20:21:24+00:00", "updated": "2024-12-25T20:21:24+00:00", "categories": ["quant-ph", "cs.NE"], "pdf_url": "http://arxiv.org/pdf/2412.18979v1", "entry_id": "http://arxiv.org/abs/2412.18979v1"}, {"id": "1603.02185", "filepath": "papers/1603.02185.pdf", "title": "Distributed Multi-Task Learning with Shared Representation", "abstract": "We study the problem of distributed multi-task learning with shared\nrepresentation, where each machine aims to learn a separate, but related, task\nin an unknown shared low-dimensional subspaces, i.e. when the predictor matrix\nhas low rank. We consider a setting where each task is handled by a different\nmachine, with samples for the task available locally on the machine, and study\ncommunication-efficient methods for exploiting the shared structure.", "authors": ["Jialei Wang", "Mladen Kolar", "Nathan Srebro"], "published": "2016-03-07T18:11:54+00:00", "updated": "2016-03-07T18:11:54+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1603.02185v1", "entry_id": "http://arxiv.org/abs/1603.02185v1"}, {"id": "1509.00913", "filepath": "papers/1509.00913.pdf", "title": "On-the-Fly Learning in a Perpetual Learning Machine", "abstract": "Despite the promise of brain-inspired machine learning, deep neural networks\n(DNN) have frustratingly failed to bridge the deceptively large gap between\nlearning and memory. Here, we introduce a Perpetual Learning Machine; a new\ntype of DNN that is capable of brain-like dynamic 'on the fly' learning because\nit exists in a self-supervised state of Perpetual Stochastic Gradient Descent.\nThus, we provide the means to unify learning and memory within a machine\nlearning framework. We also explore the elegant duality of abstraction and\nsynthesis: the Yin and Yang of deep learning.", "authors": ["Andrew J. R. Simpson"], "published": "2015-09-03T01:30:29+00:00", "updated": "2015-09-29T16:57:42+00:00", "categories": ["cs.LG", "68Txx"], "pdf_url": "http://arxiv.org/pdf/1509.00913v3", "entry_id": "http://arxiv.org/abs/1509.00913v3"}, {"id": "1909.03550", "filepath": "papers/1909.03550.pdf", "title": "Lecture Notes: Optimization for Machine Learning", "abstract": "Lecture notes on optimization for machine learning, derived from a course at\nPrinceton University and tutorials given in MLSS, Buenos Aires, as well as\nSimons Foundation, Berkeley.", "authors": ["Elad Hazan"], "published": "2019-09-08T21:49:42+00:00", "updated": "2019-09-08T21:49:42+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1909.03550v1", "entry_id": "http://arxiv.org/abs/1909.03550v1"}, {"id": "1909.01866", "filepath": "papers/1909.01866.pdf", "title": "Understanding Bias in Machine Learning", "abstract": "Bias is known to be an impediment to fair decisions in many domains such as\nhuman resources, the public sector, health care etc. Recently, hope has been\nexpressed that the use of machine learning methods for taking such decisions\nwould diminish or even resolve the problem. At the same time, machine learning\nexperts warn that machine learning models can be biased as well. In this\narticle, our goal is to explain the issue of bias in machine learning from a\ntechnical perspective and to illustrate the impact that biased data can have on\na machine learning model. To reach such a goal, we develop interactive plots to\nvisualizing the bias learned from synthetic data.", "authors": ["Jindong Gu", "Daniela Oelke"], "published": "2019-09-02T20:36:19+00:00", "updated": "2019-09-02T20:36:19+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1909.01866v1", "entry_id": "http://arxiv.org/abs/1909.01866v1"}, {"id": "2104.05314", "filepath": "papers/2104.05314.pdf", "title": "Machine learning and deep learning", "abstract": "Today, intelligent systems that offer artificial intelligence capabilities\noften rely on machine learning. Machine learning describes the capacity of\nsystems to learn from problem-specific training data to automate the process of\nanalytical model building and solve associated tasks. Deep learning is a\nmachine learning concept based on artificial neural networks. For many\napplications, deep learning models outperform shallow machine learning models\nand traditional data analysis approaches. In this article, we summarize the\nfundamentals of machine learning and deep learning to generate a broader\nunderstanding of the methodical underpinning of current intelligent systems. In\nparticular, we provide a conceptual distinction between relevant terms and\nconcepts, explain the process of automated analytical model building through\nmachine learning and deep learning, and discuss the challenges that arise when\nimplementing such intelligent systems in the field of electronic markets and\nnetworked business. These naturally go beyond technological aspects and\nhighlight issues in human-machine interaction and artificial intelligence\nservitization.", "authors": ["Christian Janiesch", "Patrick Zschech", "Kai Heinrich"], "published": "2021-04-12T09:54:12+00:00", "updated": "2021-04-14T10:31:01+00:00", "categories": ["cs.AI"], "pdf_url": "http://arxiv.org/pdf/2104.05314v2", "entry_id": "http://arxiv.org/abs/2104.05314v2"}, {"id": "1910.12387", "filepath": "papers/1910.12387.pdf", "title": "Components of Machine Learning: Binding Bits and FLOPS", "abstract": "Many machine learning problems and methods are combinations of three\ncomponents: data, hypothesis space and loss function. Different machine\nlearning methods are obtained as combinations of different choices for the\nrepresentation of data, hypothesis space and loss function. After reviewing the\nmathematical structure of these three components, we discuss intrinsic\ntrade-offs between statistical and computational properties of machine learning\nmethods.", "authors": ["Alexander Jung"], "published": "2019-10-25T17:33:33+00:00", "updated": "2019-10-30T07:59:02+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/1910.12387v2", "entry_id": "http://arxiv.org/abs/1910.12387v2"}, {"id": "2305.15410", "filepath": "papers/2305.15410.pdf", "title": "Machine learning-assisted close-set X-ray diffraction phase identification of transition metals", "abstract": "Machine learning has been applied to the problem of X-ray diffraction phase\nprediction with promising results. In this paper, we describe a method for\nusing machine learning to predict crystal structure phases from X-ray\ndiffraction data of transition metals and their oxides. We evaluate the\nperformance of our method and compare the variety of its settings. Our results\ndemonstrate that the proposed machine learning framework achieves competitive\nperformance. This demonstrates the potential for machine learning to\nsignificantly impact the field of X-ray diffraction and crystal structure\ndetermination. Open-source implementation:\nhttps://github.com/maxnygma/NeuralXRD.", "authors": ["Maksim Zhdanov", "Andrey Zhdanov"], "published": "2023-04-28T09:29:10+00:00", "updated": "2023-04-28T09:29:10+00:00", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2305.15410v1", "entry_id": "http://arxiv.org/abs/2305.15410v1"}, {"id": "1810.11383", "filepath": "papers/1810.11383.pdf", "title": "Some Requests for Machine Learning Research from the East African Tech Scene", "abstract": "Based on 46 in-depth interviews with scientists, engineers, and CEOs, this\ndocument presents a list of concrete machine research problems, progress on\nwhich would directly benefit tech ventures in East Africa.", "authors": ["Milan Cvitkovic"], "published": "2018-10-25T02:53:14+00:00", "updated": "2018-11-08T01:03:50+00:00", "categories": ["cs.LG", "cs.CY", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1810.11383v2", "entry_id": "http://arxiv.org/abs/1810.11383v2"}, {"id": "2108.07915", "filepath": "papers/2108.07915.pdf", "title": "Data Pricing in Machine Learning Pipelines", "abstract": "Machine learning is disruptive. At the same time, machine learning can only\nsucceed by collaboration among many parties in multiple steps naturally as\npipelines in an eco-system, such as collecting data for possible machine\nlearning applications, collaboratively training models by multiple parties and\ndelivering machine learning services to end users. Data is critical and\npenetrating in the whole machine learning pipelines. As machine learning\npipelines involve many parties and, in order to be successful, have to form a\nconstructive and dynamic eco-system, marketplaces and data pricing are\nfundamental in connecting and facilitating those many parties. In this article,\nwe survey the principles and the latest research development of data pricing in\nmachine learning pipelines. We start with a brief review of data marketplaces\nand pricing desiderata. Then, we focus on pricing in three important steps in\nmachine learning pipelines. To understand pricing in the step of training data\ncollection, we review pricing raw data sets and data labels. We also\ninvestigate pricing in the step of collaborative training of machine learning\nmodels, and overview pricing machine learning models for end users in the step\nof machine learning deployment. We also discuss a series of possible future\ndirections.", "authors": ["Zicun Cong", "Xuan Luo", "Pei Jian", "Feida Zhu", "Yong Zhang"], "published": "2021-08-18T00:57:06+00:00", "updated": "2021-08-18T00:57:06+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/2108.07915v1", "entry_id": "http://arxiv.org/abs/2108.07915v1"}, {"id": "1707.03184", "filepath": "papers/1707.03184.pdf", "title": "A Survey on Resilient Machine Learning", "abstract": "Machine learning based system are increasingly being used for sensitive tasks\nsuch as security surveillance, guiding autonomous vehicle, taking investment\ndecisions, detecting and blocking network intrusion and malware etc. However,\nrecent research has shown that machine learning models are venerable to attacks\nby adversaries at all phases of machine learning (eg, training data collection,\ntraining, operation). All model classes of machine learning systems can be\nmisled by providing carefully crafted inputs making them wrongly classify\ninputs. Maliciously created input samples can affect the learning process of a\nML system by either slowing down the learning process, or affecting the\nperformance of the learned mode, or causing the system make error(s) only in\nattacker's planned scenario. Because of these developments, understanding\nsecurity of machine learning algorithms and systems is emerging as an important\nresearch area among computer security and machine learning researchers and\npractitioners. We present a survey of this emerging area in machine learning.", "authors": ["Atul Kumar", "Sameep Mehta"], "published": "2017-07-11T09:15:46+00:00", "updated": "2017-07-11T09:15:46+00:00", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1707.03184v1", "entry_id": "http://arxiv.org/abs/1707.03184v1"}, {"id": "1707.04849", "filepath": "papers/1707.04849.pdf", "title": "Minimax deviation strategies for machine learning and recognition with short learning samples", "abstract": "The article is devoted to the problem of small learning samples in machine\nlearning. The flaws of maximum likelihood learning and minimax learning are\nlooked into and the concept of minimax deviation learning is introduced that is\nfree of those flaws.", "authors": ["Michail Schlesinger", "Evgeniy Vodolazskiy"], "published": "2017-07-16T09:15:08+00:00", "updated": "2017-07-16T09:15:08+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/1707.04849v1", "entry_id": "http://arxiv.org/abs/1707.04849v1"}, {"id": "2401.11351", "filepath": "papers/2401.11351.pdf", "title": "A comprehensive review of Quantum Machine Learning: from NISQ to Fault Tolerance", "abstract": "Quantum machine learning, which involves running machine learning algorithms\non quantum devices, has garnered significant attention in both academic and\nbusiness circles. In this paper, we offer a comprehensive and unbiased review\nof the various concepts that have emerged in the field of quantum machine\nlearning. This includes techniques used in Noisy Intermediate-Scale Quantum\n(NISQ) technologies and approaches for algorithms compatible with\nfault-tolerant quantum computing hardware. Our review covers fundamental\nconcepts, algorithms, and the statistical learning theory pertinent to quantum\nmachine learning.", "authors": ["Yunfei Wang", "Junyu Liu"], "published": "2024-01-21T00:19:16+00:00", "updated": "2024-03-31T00:32:13+00:00", "categories": ["quant-ph", "cs.AI", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2401.11351v2", "entry_id": "http://arxiv.org/abs/2401.11351v2"}, {"id": "1706.08001", "filepath": "papers/1706.08001.pdf", "title": "Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of learning relational order via reinforcement learning procedure?", "abstract": "In this article, we extend the conventional framework of\nconvolutional-Restricted-Boltzmann-Machine to learn highly abstract features\namong abitrary number of time related input maps by constructing a layer of\nmultiplicative units, which capture the relations among inputs. In many cases,\nmore than two maps are strongly related, so it is wise to make multiplicative\nunit learn relations among more input maps, in other words, to find the optimal\nrelational-order of each unit. In order to enable our machine to learn\nrelational order, we developed a reinforcement-learning method whose optimality\nis proven to train the network.", "authors": ["Zizhuang Wang"], "published": "2017-06-24T20:56:27+00:00", "updated": "2017-06-24T20:56:27+00:00", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1706.08001v1", "entry_id": "http://arxiv.org/abs/1706.08001v1"}, {"id": "2303.18087", "filepath": "papers/2303.18087.pdf", "title": "Evaluation Challenges for Geospatial ML", "abstract": "As geospatial machine learning models and maps derived from their predictions\nare increasingly used for downstream analyses in science and policy, it is\nimperative to evaluate their accuracy and applicability. Geospatial machine\nlearning has key distinctions from other learning paradigms, and as such, the\ncorrect way to measure performance of spatial machine learning outputs has been\na topic of debate. In this paper, I delineate unique challenges of model\nevaluation for geospatial machine learning with global or remotely sensed\ndatasets, culminating in concrete takeaways to improve evaluations of\ngeospatial model performance.", "authors": ["Esther Rolf"], "published": "2023-03-31T14:24:06+00:00", "updated": "2023-03-31T14:24:06+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2303.18087v1", "entry_id": "http://arxiv.org/abs/2303.18087v1"}, {"id": "2007.05479", "filepath": "papers/2007.05479.pdf", "title": "Impact of Legal Requirements on Explainability in Machine Learning", "abstract": "The requirements on explainability imposed by European laws and their\nimplications for machine learning (ML) models are not always clear. In that\nperspective, our research analyzes explanation obligations imposed for private\nand public decision-making, and how they can be implemented by machine learning\ntechniques.", "authors": ["Adrien Bibal", "Michael Lognoul", "Alexandre de Streel", "Beno\u00eet Fr\u00e9nay"], "published": "2020-07-10T16:57:18+00:00", "updated": "2020-07-10T16:57:18+00:00", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2007.05479v1", "entry_id": "http://arxiv.org/abs/2007.05479v1"}, {"id": "2405.03720", "filepath": "papers/2405.03720.pdf", "title": "Spatial Transfer Learning with Simple MLP", "abstract": "First step to investigate the potential of transfer learning applied to the\nfield of spatial statistics", "authors": ["Hongjian Yang"], "published": "2024-05-05T20:39:15+00:00", "updated": "2024-05-05T20:39:15+00:00", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2405.03720v1", "entry_id": "http://arxiv.org/abs/2405.03720v1"}, {"id": "1807.06722", "filepath": "papers/1807.06722.pdf", "title": "Machine Learning Interpretability: A Science rather than a tool", "abstract": "The term \"interpretability\" is oftenly used by machine learning researchers\neach with their own intuitive understanding of it. There is no universal well\nagreed upon definition of interpretability in machine learning. As any type of\nscience discipline is mainly driven by the set of formulated questions rather\nthan by different tools in that discipline, e.g. astrophysics is the discipline\nthat learns the composition of stars, not as the discipline that use the\nspectroscopes. Similarly, we propose that machine learning interpretability\nshould be a discipline that answers specific questions related to\ninterpretability. These questions can be of statistical, causal and\ncounterfactual nature. Therefore, there is a need to look into the\ninterpretability problem of machine learning in the context of questions that\nneed to be addressed rather than different tools. We discuss about a\nhypothetical interpretability framework driven by a question based scientific\napproach rather than some specific machine learning model. Using a question\nbased notion of interpretability, we can step towards understanding the science\nof machine learning rather than its engineering. This notion will also help us\nunderstanding any specific problem more in depth rather than relying solely on\nmachine learning methods.", "authors": ["Abdul Karim", "Avinash Mishra", "MA Hakim Newton", "Abdul Sattar"], "published": "2018-07-18T00:50:18+00:00", "updated": "2018-07-25T07:23:45+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1807.06722v2", "entry_id": "http://arxiv.org/abs/1807.06722v2"}, {"id": "2202.10564", "filepath": "papers/2202.10564.pdf", "title": "Human-in-the-loop Machine Learning: A Macro-Micro Perspective", "abstract": "Though technical advance of artificial intelligence and machine learning has\nenabled many promising intelligent systems, many computing tasks are still not\nable to be fully accomplished by machine intelligence. Motivated by the\ncomplementary nature of human and machine intelligence, an emerging trend is to\ninvolve humans in the loop of machine learning and decision-making. In this\npaper, we provide a macro-micro review of human-in-the-loop machine learning.\nWe first describe major machine learning challenges which can be addressed by\nhuman intervention in the loop. Then we examine closely the latest research and\nfindings of introducing humans into each step of the lifecycle of machine\nlearning. Finally, we analyze current research gaps and point out future\nresearch directions.", "authors": ["Jiangtao Wang", "Bin Guo", "Liming Chen"], "published": "2022-02-21T22:45:59+00:00", "updated": "2022-02-21T22:45:59+00:00", "categories": ["cs.HC"], "pdf_url": "http://arxiv.org/pdf/2202.10564v1", "entry_id": "http://arxiv.org/abs/2202.10564v1"}, {"id": "2201.06921", "filepath": "papers/2201.06921.pdf", "title": "Can Machine Learning be Moral?", "abstract": "The ethics of Machine Learning has become an unavoidable topic in the AI\nCommunity. The deployment of machine learning systems in multiple social\ncontexts has resulted in a closer ethical scrutiny of the design, development,\nand application of these systems. The AI/ML community has come to terms with\nthe imperative to think about the ethical implications of machine learning, not\nonly as a product but also as a practice (Birhane, 2021; Shen et al. 2021). The\ncritical question that is troubling many debates is what can constitute an\nethically accountable machine learning system. In this paper we explore\npossibilities for ethical evaluation of machine learning methodologies. We\nscrutinize techniques, methods and technical practices in machine learning from\na relational ethics perspective, taking into consideration how machine learning\nsystems are part of the world and how they relate to different forms of agency.\nTaking a page from Phil Agre (1997) we use the notion of a critical technical\npractice as a means of analysis of machine learning approaches. Our radical\nproposal is that supervised learning appears to be the only machine learning\nmethod that is ethically defensible.", "authors": ["Miguel Sicart", "Irina Shklovski", "Mirabelle Jones"], "published": "2021-12-13T07:20:50+00:00", "updated": "2021-12-13T07:20:50+00:00", "categories": ["cs.CY", "cs.HC"], "pdf_url": "http://arxiv.org/pdf/2201.06921v1", "entry_id": "http://arxiv.org/abs/2201.06921v1"}, {"id": "1611.03969", "filepath": "papers/1611.03969.pdf", "title": "An Introduction to MM Algorithms for Machine Learning and Statistical", "abstract": "MM (majorization--minimization) algorithms are an increasingly popular tool\nfor solving optimization problems in machine learning and statistical\nestimation. This article introduces the MM algorithm framework in general and\nvia three popular example applications: Gaussian mixture regressions,\nmultinomial logistic regressions, and support vector machines. Specific\nalgorithms for the three examples are derived and numerical demonstrations are\npresented. Theoretical and practical aspects of MM algorithm design are\ndiscussed.", "authors": ["Hien D. Nguyen"], "published": "2016-11-12T08:18:38+00:00", "updated": "2016-11-12T08:18:38+00:00", "categories": ["stat.CO", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1611.03969v1", "entry_id": "http://arxiv.org/abs/1611.03969v1"}, {"id": "2004.05366", "filepath": "papers/2004.05366.pdf", "title": "In-Machine-Learning Database: Reimagining Deep Learning with Old-School SQL", "abstract": "In-database machine learning has been very popular, almost being a cliche.\nHowever, can we do it the other way around? In this work, we say \"yes\" by\napplying plain old SQL to deep learning, in a sense implementing deep learning\nalgorithms with SQL. Most deep learning frameworks, as well as generic machine\nlearning ones, share a de facto standard of multidimensional array operations,\nunderneath fancier infrastructure such as automatic differentiation. As SQL\ntables can be regarded as generalisations of (multi-dimensional) arrays, we\nhave found a way to express common deep learning operations in SQL, encouraging\na different way of thinking and thus potentially novel models. In particular,\none of the latest trend in deep learning was the introduction of sparsity in\nthe name of graph convolutional networks, whereas we take sparsity almost for\ngranted in the database world. As both databases and machine learning involve\ntransformation of datasets, we hope this work can inspire further works\nutilizing the large body of existing wisdom, algorithms and technologies in the\ndatabase field to advance the state of the art in machine learning, rather than\nmerely integerating machine learning into databases.", "authors": ["Len Du"], "published": "2020-04-11T11:00:26+00:00", "updated": "2020-04-14T18:08:28+00:00", "categories": ["cs.LG", "cs.DB", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2004.05366v2", "entry_id": "http://arxiv.org/abs/2004.05366v2"}, {"id": "2012.04105", "filepath": "papers/2012.04105.pdf", "title": "The Tribes of Machine Learning and the Realm of Computer Architecture", "abstract": "Machine learning techniques have influenced the field of computer\narchitecture like many other fields. This paper studies how the fundamental\nmachine learning techniques can be applied towards computer architecture\nproblems. We also provide a detailed survey of computer architecture research\nthat employs different machine learning methods. Finally, we present some\nfuture opportunities and the outstanding challenges that need to be overcome to\nexploit full potential of machine learning for computer architecture.", "authors": ["Ayaz Akram", "Jason Lowe-Power"], "published": "2020-12-07T23:10:51+00:00", "updated": "2020-12-07T23:10:51+00:00", "categories": ["cs.LG", "cs.AR"], "pdf_url": "http://arxiv.org/pdf/2012.04105v1", "entry_id": "http://arxiv.org/abs/2012.04105v1"}, {"id": "2004.00993", "filepath": "papers/2004.00993.pdf", "title": "Augmented Q Imitation Learning (AQIL)", "abstract": "The study of unsupervised learning can be generally divided into two\ncategories: imitation learning and reinforcement learning. In imitation\nlearning the machine learns by mimicking the behavior of an expert system\nwhereas in reinforcement learning the machine learns via direct environment\nfeedback. Traditional deep reinforcement learning takes a significant time\nbefore the machine starts to converge to an optimal policy. This paper proposes\nAugmented Q-Imitation-Learning, a method by which deep reinforcement learning\nconvergence can be accelerated by applying Q-imitation-learning as the initial\ntraining process in traditional Deep Q-learning.", "authors": ["Xiao Lei Zhang", "Anish Agarwal"], "published": "2020-03-31T18:08:23+00:00", "updated": "2020-04-05T17:16:23+00:00", "categories": ["cs.LG", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2004.00993v2", "entry_id": "http://arxiv.org/abs/2004.00993v2"}, {"id": "2009.11087", "filepath": "papers/2009.11087.pdf", "title": "Probabilistic Machine Learning for Healthcare", "abstract": "Machine learning can be used to make sense of healthcare data. Probabilistic\nmachine learning models help provide a complete picture of observed data in\nhealthcare. In this review, we examine how probabilistic machine learning can\nadvance healthcare. We consider challenges in the predictive model building\npipeline where probabilistic models can be beneficial including calibration and\nmissing data. Beyond predictive models, we also investigate the utility of\nprobabilistic machine learning models in phenotyping, in generative models for\nclinical use cases, and in reinforcement learning.", "authors": ["Irene Y. Chen", "Shalmali Joshi", "Marzyeh Ghassemi", "Rajesh Ranganath"], "published": "2020-09-23T12:14:05+00:00", "updated": "2020-09-23T12:14:05+00:00", "categories": ["stat.ML", "cs.CY", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2009.11087v1", "entry_id": "http://arxiv.org/abs/2009.11087v1"}, {"id": "2011.11819", "filepath": "papers/2011.11819.pdf", "title": "When Machine Learning Meets Privacy: A Survey and Outlook", "abstract": "The newly emerged machine learning (e.g. deep learning) methods have become a\nstrong driving force to revolutionize a wide range of industries, such as smart\nhealthcare, financial technology, and surveillance systems. Meanwhile, privacy\nhas emerged as a big concern in this machine learning-based artificial\nintelligence era. It is important to note that the problem of privacy\npreservation in the context of machine learning is quite different from that in\ntraditional data privacy protection, as machine learning can act as both friend\nand foe. Currently, the work on the preservation of privacy and machine\nlearning (ML) is still in an infancy stage, as most existing solutions only\nfocus on privacy problems during the machine learning process. Therefore, a\ncomprehensive study on the privacy preservation problems and machine learning\nis required. This paper surveys the state of the art in privacy issues and\nsolutions for machine learning. The survey covers three categories of\ninteractions between privacy and machine learning: (i) private machine\nlearning, (ii) machine learning aided privacy protection, and (iii) machine\nlearning-based privacy attack and corresponding protection schemes. The current\nresearch progress in each category is reviewed and the key challenges are\nidentified. Finally, based on our in-depth analysis of the area of privacy and\nmachine learning, we point out future research directions in this field.", "authors": ["Bo Liu", "Ming Ding", "Sina Shaham", "Wenny Rahayu", "Farhad Farokhi", "Zihuai Lin"], "published": "2020-11-24T00:52:49+00:00", "updated": "2020-11-24T00:52:49+00:00", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf_url": "http://arxiv.org/pdf/2011.11819v1", "entry_id": "http://arxiv.org/abs/2011.11819v1"}, {"id": "1810.03548", "filepath": "papers/1810.03548.pdf", "title": "Meta-Learning: A Survey", "abstract": "Meta-learning, or learning to learn, is the science of systematically\nobserving how different machine learning approaches perform on a wide range of\nlearning tasks, and then learning from this experience, or meta-data, to learn\nnew tasks much faster than otherwise possible. Not only does this dramatically\nspeed up and improve the design of machine learning pipelines or neural\narchitectures, it also allows us to replace hand-engineered algorithms with\nnovel approaches learned in a data-driven way. In this chapter, we provide an\noverview of the state of the art in this fascinating and continuously evolving\nfield.", "authors": ["Joaquin Vanschoren"], "published": "2018-10-08T16:07:11+00:00", "updated": "2018-10-08T16:07:11+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1810.03548v1", "entry_id": "http://arxiv.org/abs/1810.03548v1"}, {"id": "1812.01410", "filepath": "papers/1812.01410.pdf", "title": "Compressive Classification (Machine Learning without learning)", "abstract": "Compressive learning is a framework where (so far unsupervised) learning\ntasks use not the entire dataset but a compressed summary (sketch) of it. We\npropose a compressive learning classification method, and a novel sketch\nfunction for images.", "authors": ["Vincent Schellekens", "Laurent Jacques"], "published": "2018-12-04T13:50:11+00:00", "updated": "2018-12-04T13:50:11+00:00", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1812.01410v1", "entry_id": "http://arxiv.org/abs/1812.01410v1"}, {"id": "1909.09248", "filepath": "papers/1909.09248.pdf", "title": "Representation Learning for Electronic Health Records", "abstract": "Information in electronic health records (EHR), such as clinical narratives,\nexamination reports, lab measurements, demographics, and other patient\nencounter entries, can be transformed into appropriate data representations\nthat can be used for downstream clinical machine learning tasks using\nrepresentation learning. Learning better representations is critical to improve\nthe performance of downstream tasks. Due to the advances in machine learning,\nwe now can learn better and meaningful representations from EHR through\ndisentangling the underlying factors inside data and distilling large amounts\nof information and knowledge from heterogeneous EHR sources. In this chapter,\nwe first introduce the background of learning representations and reasons why\nwe need good EHR representations in machine learning for medicine and\nhealthcare in Section 1. Next, we explain the commonly-used machine learning\nand evaluation methods for representation learning using a deep learning\napproach in Section 2. Following that, we review recent related studies of\nlearning patient state representation from EHR for clinical machine learning\ntasks in Section 3. Finally, in Section 4 we discuss more techniques, studies,\nand challenges for learning natural language representations when free texts,\nsuch as clinical notes, examination reports, or biomedical literature are used.\nWe also discuss challenges and opportunities in these rapidly growing research\nfields.", "authors": ["Wei-Hung Weng", "Peter Szolovits"], "published": "2019-09-19T22:12:30+00:00", "updated": "2019-09-19T22:12:30+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1909.09248v1", "entry_id": "http://arxiv.org/abs/1909.09248v1"}, {"id": "1607.02450", "filepath": "papers/1607.02450.pdf", "title": "Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in Social Good Applications", "abstract": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning\nin Social Good Applications, which was held on June 24, 2016 in New York.", "authors": ["Kush R. Varshney"], "published": "2016-07-08T16:55:31+00:00", "updated": "2016-08-28T15:23:47+00:00", "categories": ["stat.ML", "cs.CY", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1607.02450v2", "entry_id": "http://arxiv.org/abs/1607.02450v2"}, {"id": "1702.08608", "filepath": "papers/1702.08608.pdf", "title": "Towards A Rigorous Science of Interpretable Machine Learning", "abstract": "As machine learning systems become ubiquitous, there has been a surge of\ninterest in interpretable machine learning: systems that provide explanation\nfor their outputs. These explanations are often used to qualitatively assess\nother criteria such as safety or non-discrimination. However, despite the\ninterest in interpretability, there is very little consensus on what\ninterpretable machine learning is and how it should be measured. In this\nposition paper, we first define interpretability and describe when\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\nfor rigorous evaluation and expose open questions towards a more rigorous\nscience of interpretable machine learning.", "authors": ["Finale Doshi-Velez", "Been Kim"], "published": "2017-02-28T02:19:20+00:00", "updated": "2017-03-02T19:32:10+00:00", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1702.08608v2", "entry_id": "http://arxiv.org/abs/1702.08608v2"}, {"id": "1906.06821", "filepath": "papers/1906.06821.pdf", "title": "A Survey of Optimization Methods from a Machine Learning Perspective", "abstract": "Machine learning develops rapidly, which has made many theoretical\nbreakthroughs and is widely applied in various fields. Optimization, as an\nimportant part of machine learning, has attracted much attention of\nresearchers. With the exponential growth of data amount and the increase of\nmodel complexity, optimization methods in machine learning face more and more\nchallenges. A lot of work on solving optimization problems or improving\noptimization methods in machine learning has been proposed successively. The\nsystematic retrospect and summary of the optimization methods from the\nperspective of machine learning are of great significance, which can offer\nguidance for both developments of optimization and machine learning research.\nIn this paper, we first describe the optimization problems in machine learning.\nThen, we introduce the principles and progresses of commonly used optimization\nmethods. Next, we summarize the applications and developments of optimization\nmethods in some popular machine learning fields. Finally, we explore and give\nsome challenges and open problems for the optimization in machine learning.", "authors": ["Shiliang Sun", "Zehui Cao", "Han Zhu", "Jing Zhao"], "published": "2019-06-17T02:54:51+00:00", "updated": "2019-10-23T08:26:31+00:00", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1906.06821v2", "entry_id": "http://arxiv.org/abs/1906.06821v2"}, {"id": "2001.04942", "filepath": "papers/2001.04942.pdf", "title": "Private Machine Learning via Randomised Response", "abstract": "We introduce a general learning framework for private machine learning based\non randomised response. Our assumption is that all actors are potentially\nadversarial and as such we trust only to release a single noisy version of an\nindividual's datapoint. We discuss a general approach that forms a consistent\nway to estimate the true underlying machine learning model and demonstrate this\nin the case of logistic regression.", "authors": ["David Barber"], "published": "2020-01-14T17:56:16+00:00", "updated": "2020-02-24T19:13:28+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2001.04942v2", "entry_id": "http://arxiv.org/abs/2001.04942v2"}, {"id": "2007.01503", "filepath": "papers/2007.01503.pdf", "title": "Mathematical Perspective of Machine Learning", "abstract": "We take a closer look at some theoretical challenges of Machine Learning as a\nfunction approximation, gradient descent as the default optimization algorithm,\nlimitations of fixed length and width networks and a different approach to RNNs\nfrom a mathematical perspective.", "authors": ["Yarema Boryshchak"], "published": "2020-07-03T05:26:02+00:00", "updated": "2020-07-03T05:26:02+00:00", "categories": ["cs.LG", "stat.ML", "68T07"], "pdf_url": "http://arxiv.org/pdf/2007.01503v1", "entry_id": "http://arxiv.org/abs/2007.01503v1"}, {"id": "1909.09246", "filepath": "papers/1909.09246.pdf", "title": "Machine Learning for Clinical Predictive Analytics", "abstract": "In this chapter, we provide a brief overview of applying machine learning\ntechniques for clinical prediction tasks. We begin with a quick introduction to\nthe concepts of machine learning and outline some of the most common machine\nlearning algorithms. Next, we demonstrate how to apply the algorithms with\nappropriate toolkits to conduct machine learning experiments for clinical\nprediction tasks. The objectives of this chapter are to (1) understand the\nbasics of machine learning techniques and the reasons behind why they are\nuseful for solving clinical prediction problems, (2) understand the intuition\nbehind some machine learning models, including regression, decision trees, and\nsupport vector machines, and (3) understand how to apply these models to\nclinical prediction problems using publicly available datasets via case\nstudies.", "authors": ["Wei-Hung Weng"], "published": "2019-09-19T22:02:00+00:00", "updated": "2019-09-19T22:02:00+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1909.09246v1", "entry_id": "http://arxiv.org/abs/1909.09246v1"}, {"id": "1711.06552", "filepath": "papers/1711.06552.pdf", "title": "Introduction to intelligent computing unit 1", "abstract": "This brief note highlights some basic concepts required toward understanding\nthe evolution of machine learning and deep learning models. The note starts\nwith an overview of artificial intelligence and its relationship to biological\nneuron that ultimately led to the evolution of todays intelligent models.", "authors": ["Isa Inuwa-Dutse"], "published": "2017-11-15T16:52:48+00:00", "updated": "2017-11-15T16:52:48+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1711.06552v1", "entry_id": "http://arxiv.org/abs/1711.06552v1"}, {"id": "1707.09562", "filepath": "papers/1707.09562.pdf", "title": "MLBench: How Good Are Machine Learning Clouds for Binary Classification Tasks on Structured Data?", "abstract": "We conduct an empirical study of machine learning functionalities provided by\nmajor cloud service providers, which we call machine learning clouds. Machine\nlearning clouds hold the promise of hiding all the sophistication of running\nlarge-scale machine learning: Instead of specifying how to run a machine\nlearning task, users only specify what machine learning task to run and the\ncloud figures out the rest. Raising the level of abstraction, however, rarely\ncomes free - a performance penalty is possible. How good, then, are current\nmachine learning clouds on real-world machine learning workloads?\n  We study this question with a focus on binary classication problems. We\npresent mlbench, a novel benchmark constructed by harvesting datasets from\nKaggle competitions. We then compare the performance of the top winning code\navailable from Kaggle with that of running machine learning clouds from both\nAzure and Amazon on mlbench. Our comparative study reveals the strength and\nweakness of existing machine learning clouds and points out potential future\ndirections for improvement.", "authors": ["Yu Liu", "Hantian Zhang", "Luyuan Zeng", "Wentao Wu", "Ce Zhang"], "published": "2017-07-29T21:59:18+00:00", "updated": "2017-10-16T11:13:32+00:00", "categories": ["cs.DC", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1707.09562v3", "entry_id": "http://arxiv.org/abs/1707.09562v3"}, {"id": "1911.00776", "filepath": "papers/1911.00776.pdf", "title": "Ten-year Survival Prediction for Breast Cancer Patients", "abstract": "This report assesses different machine learning approaches to 10-year\nsurvival prediction of breast cancer patients.", "authors": ["Changmao Li", "Han He", "Yunze Hao", "Caleb Ziems"], "published": "2019-11-02T19:53:32+00:00", "updated": "2019-11-02T19:53:32+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1911.00776v1", "entry_id": "http://arxiv.org/abs/1911.00776v1"}, {"id": "1602.00198", "filepath": "papers/1602.00198.pdf", "title": "Discussion on Mechanical Learning and Learning Machine", "abstract": "Mechanical learning is a computing system that is based on a set of simple\nand fixed rules, and can learn from incoming data. A learning machine is a\nsystem that realizes mechanical learning. Importantly, we emphasis that it is\nbased on a set of simple and fixed rules, contrasting to often called machine\nlearning that is sophisticated software based on very complicated mathematical\ntheory, and often needs human intervene for software fine tune and manual\nadjustments. Here, we discuss some basic facts and principles of such system,\nand try to lay down a framework for further study. We propose 2 directions to\napproach mechanical learning, just like Church-Turing pair: one is trying to\nrealize a learning machine, another is trying to well describe the mechanical\nlearning.", "authors": ["Chuyu Xiong"], "published": "2016-01-31T04:05:50+00:00", "updated": "2016-01-31T04:05:50+00:00", "categories": ["cs.AI"], "pdf_url": "http://arxiv.org/pdf/1602.00198v1", "entry_id": "http://arxiv.org/abs/1602.00198v1"}, {"id": "2407.19890", "filepath": "papers/2407.19890.pdf", "title": "Quantum Dynamics of Machine Learning", "abstract": "The quantum dynamic equation (QDE) of machine learning is obtained based on\nSchr\\\"odinger equation and potential energy equivalence relationship. Through\nWick rotation, the relationship between quantum dynamics and thermodynamics is\nalso established in this paper. This equation reformulates the iterative\nprocess of machine learning into a time-dependent partial differential equation\nwith a clear mathematical structure, offering a theoretical framework for\ninvestigating machine learning iterations through quantum and mathematical\ntheories. Within this framework, the fundamental iterative process, the\ndiffusion model, and the Softmax and Sigmoid functions are examined, validating\nthe proposed quantum dynamics equations. This approach not only presents a\nrigorous theoretical foundation for machine learning but also holds promise for\nsupporting the implementation of machine learning algorithms on quantum\ncomputers.", "authors": ["Peng Wang", "Maimaitiniyazi Maimaitiabudula"], "published": "2024-07-07T16:30:46+00:00", "updated": "2024-07-07T16:30:46+00:00", "categories": ["quant-ph", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2407.19890v1", "entry_id": "http://arxiv.org/abs/2407.19890v1"}, {"id": "1212.2686", "filepath": "papers/1212.2686.pdf", "title": "Joint Training of Deep Boltzmann Machines", "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior\nmethods require an initial learning pass that trains the deep Boltzmann machine\ngreedily, one layer at a time, or do not perform well on classifi- cation\ntasks.", "authors": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "published": "2012-12-12T01:59:27+00:00", "updated": "2012-12-12T01:59:27+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1212.2686v1", "entry_id": "http://arxiv.org/abs/1212.2686v1"}, {"id": "1501.04309", "filepath": "papers/1501.04309.pdf", "title": "Information Theory and its Relation to Machine Learning", "abstract": "In this position paper, I first describe a new perspective on machine\nlearning (ML) by four basic problems (or levels), namely, \"What to learn?\",\n\"How to learn?\", \"What to evaluate?\", and \"What to adjust?\". The paper stresses\nmore on the first level of \"What to learn?\", or \"Learning Target Selection\".\nTowards this primary problem within the four levels, I briefly review the\nexisting studies about the connection between information theoretical learning\n(ITL [1]) and machine learning. A theorem is given on the relation between the\nempirically-defined similarity measure and information measures. Finally, a\nconjecture is proposed for pursuing a unified mathematical interpretation to\nlearning target selection.", "authors": ["Bao-Gang Hu"], "published": "2015-01-18T14:57:02+00:00", "updated": "2015-01-18T14:57:02+00:00", "categories": ["cs.IT", "cs.LG", "math.IT"], "pdf_url": "http://arxiv.org/pdf/1501.04309v1", "entry_id": "http://arxiv.org/abs/1501.04309v1"}, {"id": "2407.05526", "filepath": "papers/2407.05526.pdf", "title": "Can Machines Learn the True Probabilities?", "abstract": "When there exists uncertainty, AI machines are designed to make decisions so\nas to reach the best expected outcomes. Expectations are based on true facts\nabout the objective environment the machines interact with, and those facts can\nbe encoded into AI models in the form of true objective probability functions.\nAccordingly, AI models involve probabilistic machine learning in which the\nprobabilities should be objectively interpreted. We prove under some basic\nassumptions when machines can learn the true objective probabilities, if any,\nand when machines cannot learn them.", "authors": ["Jinsook Kim"], "published": "2024-07-08T00:19:43+00:00", "updated": "2024-07-08T00:19:43+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2407.05526v1", "entry_id": "http://arxiv.org/abs/2407.05526v1"}, {"id": "2310.11470", "filepath": "papers/2310.11470.pdf", "title": "Classic machine learning methods", "abstract": "In this chapter, we present the main classic machine learning methods. A\nlarge part of the chapter is devoted to supervised learning techniques for\nclassification and regression, including nearest-neighbor methods, linear and\nlogistic regressions, support vector machines and tree-based algorithms. We\nalso describe the problem of overfitting as well as strategies to overcome it.\nWe finally provide a brief overview of unsupervised learning methods, namely\nfor clustering and dimensionality reduction.", "authors": ["Johann Faouzi", "Olivier Colliot"], "published": "2023-05-24T13:38:38+00:00", "updated": "2023-05-24T13:38:38+00:00", "categories": ["cs.LG", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2310.11470v1", "entry_id": "http://arxiv.org/abs/2310.11470v1"}, {"id": "1507.02188", "filepath": "papers/1507.02188.pdf", "title": "AutoCompete: A Framework for Machine Learning Competition", "abstract": "In this paper, we propose AutoCompete, a highly automated machine learning\nframework for tackling machine learning competitions. This framework has been\nlearned by us, validated and improved over a period of more than two years by\nparticipating in online machine learning competitions. It aims at minimizing\nhuman interference required to build a first useful predictive model and to\nassess the practical difficulty of a given machine learning challenge. The\nproposed system helps in identifying data types, choosing a machine learn- ing\nmodel, tuning hyper-parameters, avoiding over-fitting and optimization for a\nprovided evaluation metric. We also observe that the proposed system produces\nbetter (or comparable) results with less runtime as compared to other\napproaches.", "authors": ["Abhishek Thakur", "Artus Krohn-Grimberghe"], "published": "2015-07-08T15:07:39+00:00", "updated": "2015-07-08T15:07:39+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1507.02188v1", "entry_id": "http://arxiv.org/abs/1507.02188v1"}, {"id": "2206.13446", "filepath": "papers/2206.13446.pdf", "title": "Pen and Paper Exercises in Machine Learning", "abstract": "This is a collection of (mostly) pen-and-paper exercises in machine learning.\nThe exercises are on the following topics: linear algebra, optimisation,\ndirected graphical models, undirected graphical models, expressive power of\ngraphical models, factor graphs and message passing, inference for hidden\nMarkov models, model-based learning (including ICA and unnormalised models),\nsampling and Monte-Carlo integration, and variational inference.", "authors": ["Michael U. Gutmann"], "published": "2022-06-27T16:53:18+00:00", "updated": "2022-06-27T16:53:18+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2206.13446v1", "entry_id": "http://arxiv.org/abs/2206.13446v1"}, {"id": "1802.03830", "filepath": "papers/1802.03830.pdf", "title": "Distributed Stochastic Multi-Task Learning with Graph Regularization", "abstract": "We propose methods for distributed graph-based multi-task learning that are\nbased on weighted averaging of messages from other machines. Uniform averaging\nor diminishing stepsize in these methods would yield consensus (single task)\nlearning. We show how simply skewing the averaging weights or controlling the\nstepsize allows learning different, but related, tasks on the different\nmachines.", "authors": ["Weiran Wang", "Jialei Wang", "Mladen Kolar", "Nathan Srebro"], "published": "2018-02-11T22:23:34+00:00", "updated": "2018-02-11T22:23:34+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1802.03830v1", "entry_id": "http://arxiv.org/abs/1802.03830v1"}, {"id": "1908.04710", "filepath": "papers/1908.04710.pdf", "title": "metric-learn: Metric Learning Algorithms in Python", "abstract": "metric-learn is an open source Python package implementing supervised and\nweakly-supervised distance metric learning algorithms. As part of\nscikit-learn-contrib, it provides a unified interface compatible with\nscikit-learn which allows to easily perform cross-validation, model selection,\nand pipelining with other machine learning estimators. metric-learn is\nthoroughly tested and available on PyPi under the MIT licence.", "authors": ["William de Vazelhes", "CJ Carey", "Yuan Tang", "Nathalie Vauquier", "Aur\u00e9lien Bellet"], "published": "2019-08-13T15:52:31+00:00", "updated": "2020-07-27T14:47:52+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1908.04710v3", "entry_id": "http://arxiv.org/abs/1908.04710v3"}, {"id": "2312.03120", "filepath": "papers/2312.03120.pdf", "title": "The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning", "abstract": "With the advance of the powerful heterogeneous, parallel and distributed\ncomputing systems and ever increasing immense amount of data, machine learning\nhas become an indispensable part of cutting-edge technology, scientific\nresearch and consumer products. In this study, we present a review of modern\nmachine and deep learning. We provide a high-level overview for the latest\nadvanced machine learning algorithms, applications, and frameworks. Our\ndiscussion encompasses parallel distributed learning, deep learning as well as\nfederated learning. As a result, our work serves as an introductory text to the\nvast field of modern machine learning.", "authors": ["Omer Subasi", "Oceane Bel", "Joseph Manzano", "Kevin Barker"], "published": "2023-12-05T20:40:05+00:00", "updated": "2023-12-05T20:40:05+00:00", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf_url": "http://arxiv.org/pdf/2312.03120v1", "entry_id": "http://arxiv.org/abs/2312.03120v1"}, {"id": "2103.03122", "filepath": "papers/2103.03122.pdf", "title": "Machine Learning using Stata/Python", "abstract": "We present two related Stata modules, r_ml_stata and c_ml_stata, for fitting\npopular Machine Learning (ML) methods both in regression and classification\nsettings. Using the recent Stata/Python integration platform (sfi) of Stata 16,\nthese commands provide hyper-parameters' optimal tuning via K-fold\ncross-validation using greed search. More specifically, they make use of the\nPython Scikit-learn API to carry out both cross-validation and outcome/label\nprediction.", "authors": ["Giovanni Cerulli"], "published": "2021-03-03T10:31:44+00:00", "updated": "2021-03-03T10:31:44+00:00", "categories": ["stat.CO", "cs.LG", "cs.MS"], "pdf_url": "http://arxiv.org/pdf/2103.03122v1", "entry_id": "http://arxiv.org/abs/2103.03122v1"}, {"id": "1907.03010", "filepath": "papers/1907.03010.pdf", "title": "Financial Time Series Data Processing for Machine Learning", "abstract": "This article studies the financial time series data processing for machine\nlearning. It introduces the most frequent scaling methods, then compares the\nresulting stationarity and preservation of useful information for trend\nforecasting. It proposes an empirical test based on the capability to learn\nsimple data relationship with simple models. It also speaks about the data\nsplit method specific to time series, avoiding unwanted overfitting and\nproposes various labelling for classification and regression.", "authors": ["Fabrice Daniel"], "published": "2019-07-03T15:10:23+00:00", "updated": "2019-07-03T15:10:23+00:00", "categories": ["q-fin.ST", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1907.03010v1", "entry_id": "http://arxiv.org/abs/1907.03010v1"}, {"id": "2306.14624", "filepath": "papers/2306.14624.pdf", "title": "Insights From Insurance for Fair Machine Learning", "abstract": "We argue that insurance can act as an analogon for the social situatedness of\nmachine learning systems, hence allowing machine learning scholars to take\ninsights from the rich and interdisciplinary insurance literature. Tracing the\ninteraction of uncertainty, fairness and responsibility in insurance provides a\nfresh perspective on fairness in machine learning. We link insurance fairness\nconceptions to their machine learning relatives, and use this bridge to\nproblematize fairness as calibration. In this process, we bring to the\nforefront two themes that have been largely overlooked in the machine learning\nliterature: responsibility and aggregate-individual tensions.", "authors": ["Christian Fr\u00f6hlich", "Robert C. Williamson"], "published": "2023-06-26T11:56:00+00:00", "updated": "2024-01-23T09:43:23+00:00", "categories": ["cs.LG", "cs.CY"], "pdf_url": "http://arxiv.org/pdf/2306.14624v2", "entry_id": "http://arxiv.org/abs/2306.14624v2"}, {"id": "1811.04422", "filepath": "papers/1811.04422.pdf", "title": "An Optimal Control View of Adversarial Machine Learning", "abstract": "I describe an optimal control view of adversarial machine learning, where the\ndynamical system is the machine learner, the input are adversarial actions, and\nthe control costs are defined by the adversary's goals to do harm and be hard\nto detect. This view encompasses many types of adversarial machine learning,\nincluding test-item attacks, training-data poisoning, and adversarial reward\nshaping. The view encourages adversarial machine learning researcher to utilize\nadvances in control theory and reinforcement learning.", "authors": ["Xiaojin Zhu"], "published": "2018-11-11T14:28:34+00:00", "updated": "2018-11-11T14:28:34+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1811.04422v1", "entry_id": "http://arxiv.org/abs/1811.04422v1"}, {"id": "1902.04622", "filepath": "papers/1902.04622.pdf", "title": "Learning Theory and Support Vector Machines - a primer", "abstract": "The main goal of statistical learning theory is to provide a fundamental\nframework for the problem of decision making and model construction based on\nsets of data. Here, we present a brief introduction to the fundamentals of\nstatistical learning theory, in particular the difference between empirical and\nstructural risk minimization, including one of its most prominent\nimplementations, i.e. the Support Vector Machine.", "authors": ["Michael Banf"], "published": "2019-02-12T20:28:09+00:00", "updated": "2019-02-12T20:28:09+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1902.04622v1", "entry_id": "http://arxiv.org/abs/1902.04622v1"}, {"id": "2108.08712", "filepath": "papers/2108.08712.pdf", "title": "Teaching Uncertainty Quantification in Machine Learning through Use Cases", "abstract": "Uncertainty in machine learning is not generally taught as general knowledge\nin Machine Learning course curricula. In this paper we propose a short\ncurriculum for a course about uncertainty in machine learning, and complement\nthe course with a selection of use cases, aimed to trigger discussion and let\nstudents play with the concepts of uncertainty in a programming setting. Our\nuse cases cover the concept of output uncertainty, Bayesian neural networks and\nweight distributions, sources of uncertainty, and out of distribution\ndetection. We expect that this curriculum and set of use cases motivates the\ncommunity to adopt these important concepts into courses for safety in AI.", "authors": ["Matias Valdenegro-Toro"], "published": "2021-08-19T14:22:17+00:00", "updated": "2021-08-19T14:22:17+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2108.08712v1", "entry_id": "http://arxiv.org/abs/2108.08712v1"}, {"id": "2007.07981", "filepath": "papers/2007.07981.pdf", "title": "Differential Replication in Machine Learning", "abstract": "When deployed in the wild, machine learning models are usually confronted\nwith data and requirements that constantly vary, either because of changes in\nthe generating distribution or because external constraints change the\nenvironment where the model operates. To survive in such an ecosystem, machine\nlearning models need to adapt to new conditions by evolving over time. The idea\nof model adaptability has been studied from different perspectives. In this\npaper, we propose a solution based on reusing the knowledge acquired by the\nalready deployed machine learning models and leveraging it to train future\ngenerations. This is the idea behind differential replication of machine\nlearning models.", "authors": ["Irene Unceta", "Jordi Nin", "Oriol Pujol"], "published": "2020-07-15T20:26:49+00:00", "updated": "2020-07-15T20:26:49+00:00", "categories": ["cs.LG", "stat.ML", "cs.LG, stat.ML"], "pdf_url": "http://arxiv.org/pdf/2007.07981v1", "entry_id": "http://arxiv.org/abs/2007.07981v1"}, {"id": "1911.08587", "filepath": "papers/1911.08587.pdf", "title": "Solving machine learning optimization problems using quantum computers", "abstract": "Classical optimization algorithms in machine learning often take a long time\nto compute when applied to a multi-dimensional problem and require a huge\namount of CPU and GPU resource. Quantum parallelism has a potential to speed up\nmachine learning algorithms. We describe a generic mathematical model to\nleverage quantum parallelism to speed-up machine learning algorithms. We also\napply quantum machine learning and quantum parallelism applied to a\n$3$-dimensional image that vary with time.", "authors": ["Venkat R. Dasari", "Mee Seong Im", "Lubjana Beshaj"], "published": "2019-11-17T17:36:41+00:00", "updated": "2019-11-17T17:36:41+00:00", "categories": ["quant-ph", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1911.08587v1", "entry_id": "http://arxiv.org/abs/1911.08587v1"}, {"id": "2502.01708", "filepath": "papers/2502.01708.pdf", "title": "Aspects of Artificial Intelligence: Transforming Machine Learning Systems Naturally", "abstract": "In this paper, we study the machine learning elements which we are interested\nin together as a machine learning system, consisting of a collection of machine\nlearning elements and a collection of relations between the elements. The\nrelations we concern are algebraic operations, binary relations, and binary\nrelations with composition that can be reasoned categorically. A machine\nlearning system transformation between two systems is a map between the\nsystems, which preserves the relations we concern. The system transformations\ngiven by quotient or clustering, representable functor, and Yoneda embedding\nare highlighted and discussed by machine learning examples. An adjunction\nbetween machine learning systems, a special machine learning system\ntransformation loop, provides the optimal way of solving problems. Machine\nlearning system transformations are linked and compared by their maps at\n2-cell, natural transformations. New insights and structures can be obtained\nfrom universal properties and algebraic structures given by monads, which are\ngenerated from adjunctions.", "authors": ["Xiuzhan Guo"], "published": "2025-02-03T14:45:02+00:00", "updated": "2025-02-03T14:45:02+00:00", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.DM"], "pdf_url": "http://arxiv.org/pdf/2502.01708v1", "entry_id": "http://arxiv.org/abs/2502.01708v1"}, {"id": "2409.03632", "filepath": "papers/2409.03632.pdf", "title": "Beyond Model Interpretability: Socio-Structural Explanations in Machine Learning", "abstract": "What is it to interpret the outputs of an opaque machine learning model. One\napproach is to develop interpretable machine learning techniques. These\ntechniques aim to show how machine learning models function by providing either\nmodel centric local or global explanations, which can be based on mechanistic\ninterpretations revealing the inner working mechanisms of models or\nnonmechanistic approximations showing input feature output data relationships.\nIn this paper, we draw on social philosophy to argue that interpreting machine\nlearning outputs in certain normatively salient domains could require appealing\nto a third type of explanation that we call sociostructural explanation. The\nrelevance of this explanation type is motivated by the fact that machine\nlearning models are not isolated entities but are embedded within and shaped by\nsocial structures. Sociostructural explanations aim to illustrate how social\nstructures contribute to and partially explain the outputs of machine learning\nmodels. We demonstrate the importance of sociostructural explanations by\nexamining a racially biased healthcare allocation algorithm. Our proposal\nhighlights the need for transparency beyond model interpretability,\nunderstanding the outputs of machine learning systems could require a broader\nanalysis that extends beyond the understanding of the machine learning model\nitself.", "authors": ["Andrew Smart", "Atoosa Kasirzadeh"], "published": "2024-09-05T15:47:04+00:00", "updated": "2024-09-05T15:47:04+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/2409.03632v1", "entry_id": "http://arxiv.org/abs/2409.03632v1"}, {"id": "2301.09753", "filepath": "papers/2301.09753.pdf", "title": "Towards Modular Machine Learning Solution Development: Benefits and Trade-offs", "abstract": "Machine learning technologies have demonstrated immense capabilities in\nvarious domains. They play a key role in the success of modern businesses.\nHowever, adoption of machine learning technologies has a lot of untouched\npotential. Cost of developing custom machine learning solutions that solve\nunique business problems is a major inhibitor to far-reaching adoption of\nmachine learning technologies. We recognize that the monolithic nature\nprevalent in today's machine learning applications stands in the way of\nefficient and cost effective customized machine learning solution development.\nIn this work we explore the benefits of modular machine learning solutions and\ndiscuss how modular machine learning solutions can overcome some of the major\nsolution engineering limitations of monolithic machine learning solutions. We\nanalyze the trade-offs between modular and monolithic machine learning\nsolutions through three deep learning problems; one text based and the two\nimage based. Our experimental results show that modular machine learning\nsolutions have a promising potential to reap the solution engineering\nadvantages of modularity while gaining performance and data advantages in a way\nthe monolithic machine learning solutions do not permit.", "authors": ["Samiyuru Menik", "Lakshmish Ramaswamy"], "published": "2023-01-23T22:54:34+00:00", "updated": "2023-01-23T22:54:34+00:00", "categories": ["cs.LG", "cs.SE"], "pdf_url": "http://arxiv.org/pdf/2301.09753v1", "entry_id": "http://arxiv.org/abs/2301.09753v1"}, {"id": "2007.01977", "filepath": "papers/2007.01977.pdf", "title": "Lale: Consistent Automated Machine Learning", "abstract": "Automated machine learning makes it easier for data scientists to develop\npipelines by searching over possible choices for hyperparameters, algorithms,\nand even pipeline topologies. Unfortunately, the syntax for automated machine\nlearning tools is inconsistent with manual machine learning, with each other,\nand with error checks. Furthermore, few tools support advanced features such as\ntopology search or higher-order operators. This paper introduces Lale, a\nlibrary of high-level Python interfaces that simplifies and unifies automated\nmachine learning in a consistent way.", "authors": ["Guillaume Baudart", "Martin Hirzel", "Kiran Kate", "Parikshit Ram", "Avraham Shinnar"], "published": "2020-07-04T00:55:41+00:00", "updated": "2020-07-04T00:55:41+00:00", "categories": ["cs.LG", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2007.01977v1", "entry_id": "http://arxiv.org/abs/2007.01977v1"}, {"id": "1808.00033", "filepath": "papers/1808.00033.pdf", "title": "Techniques for Interpretable Machine Learning", "abstract": "Interpretable machine learning tackles the important problem that humans\ncannot understand the behaviors of complex machine learning models and how\nthese models arrive at a particular decision. Although many approaches have\nbeen proposed, a comprehensive understanding of the achievements and challenges\nis still lacking. We provide a survey covering existing techniques to increase\nthe interpretability of machine learning models. We also discuss crucial issues\nthat the community should consider in future work such as designing\nuser-friendly explanations and developing comprehensive evaluation metrics to\nfurther push forward the area of interpretable machine learning.", "authors": ["Mengnan Du", "Ninghao Liu", "Xia Hu"], "published": "2018-07-31T19:14:39+00:00", "updated": "2019-05-19T20:44:37+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1808.00033v3", "entry_id": "http://arxiv.org/abs/1808.00033v3"}, {"id": "2312.14050", "filepath": "papers/2312.14050.pdf", "title": "Machine learning and domain decomposition methods -- a survey", "abstract": "Hybrid algorithms, which combine black-box machine learning methods with\nexperience from traditional numerical methods and domain expertise from diverse\napplication areas, are progressively gaining importance in scientific machine\nlearning and various industrial domains, especially in computational science\nand engineering. In the present survey, several promising avenues of research\nwill be examined which focus on the combination of machine learning (ML) and\ndomain decomposition methods (DDMs). The aim of this survey is to provide an\noverview of existing work within this field and to structure it into domain\ndecomposition for machine learning and machine learning-enhanced domain\ndecomposition, including: domain decomposition for classical machine learning,\ndomain decomposition to accelerate the training of physics-aware neural\nnetworks, machine learning to enhance the convergence properties or\ncomputational efficiency of DDMs, and machine learning as a discretization\nmethod in a DDM for the solution of PDEs. In each of these fields, we summarize\nexisting work and key advances within a common framework and, finally, disuss\nongoing challenges and opportunities for future research.", "authors": ["Axel Klawonn", "Martin Lanser", "Janine Weber"], "published": "2023-12-21T17:19:27+00:00", "updated": "2023-12-21T17:19:27+00:00", "categories": ["math.NA", "cs.LG", "cs.NA", "65F10, 65N22, 65N55, 68T05, 68T07"], "pdf_url": "http://arxiv.org/pdf/2312.14050v1", "entry_id": "http://arxiv.org/abs/2312.14050v1"}, {"id": "2106.07032", "filepath": "papers/2106.07032.pdf", "title": "Category Theory in Machine Learning", "abstract": "Over the past two decades machine learning has permeated almost every realm\nof technology. At the same time, many researchers have begun using category\ntheory as a unifying language, facilitating communication between different\nscientific disciplines. It is therefore unsurprising that there is a burgeoning\ninterest in applying category theory to machine learning. We aim to document\nthe motivations, goals and common themes across these applications. We touch on\ngradient-based learning, probability, and equivariant learning.", "authors": ["Dan Shiebler", "Bruno Gavranovi\u0107", "Paul Wilson"], "published": "2021-06-13T15:58:13+00:00", "updated": "2021-06-13T15:58:13+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/2106.07032v1", "entry_id": "http://arxiv.org/abs/2106.07032v1"}, {"id": "2209.02057", "filepath": "papers/2209.02057.pdf", "title": "Applying Machine Learning to Life Insurance: some knowledge sharing to master it", "abstract": "Machine Learning permeates many industries, which brings new source of\nbenefits for companies. However within the life insurance industry, Machine\nLearning is not widely used in practice as over the past years statistical\nmodels have shown their efficiency for risk assessment. Thus insurers may face\ndifficulties to assess the value of the artificial intelligence. Focusing on\nthe modification of the life insurance industry over time highlights the stake\nof using Machine Learning for insurers and benefits that it can bring by\nunleashing data value. This paper reviews traditional actuarial methodologies\nfor survival modeling and extends them with Machine Learning techniques. It\npoints out differences with regular machine learning models and emphasizes\nimportance of specific implementations to face censored data with machine\nlearning models family. In complement to this article, a Python library has\nbeen developed. Different open-source Machine Learning algorithms have been\nadjusted to adapt the specificities of life insurance data, namely censoring\nand truncation. Such models can be easily applied from this SCOR library to\naccurately model life insurance risks.", "authors": ["Antoine Chancel", "Laura Bradier", "Antoine Ly", "Razvan Ionescu", "Laurene Martin", "Marguerite Sauce"], "published": "2022-09-05T17:09:03+00:00", "updated": "2022-09-27T18:42:50+00:00", "categories": ["stat.ML", "cs.CY", "cs.LG", "stat.AP"], "pdf_url": "http://arxiv.org/pdf/2209.02057v3", "entry_id": "http://arxiv.org/abs/2209.02057v3"}, {"id": "2105.03726", "filepath": "papers/2105.03726.pdf", "title": "Mental Models of Adversarial Machine Learning", "abstract": "Although machine learning is widely used in practice, little is known about\npractitioners' understanding of potential security challenges. In this work, we\nclose this substantial gap and contribute a qualitative study focusing on\ndevelopers' mental models of the machine learning pipeline and potentially\nvulnerable components. Similar studies have helped in other security fields to\ndiscover root causes or improve risk communication. Our study reveals two\n\\facets of practitioners' mental models of machine learning security. Firstly,\npractitioners often confuse machine learning security with threats and defences\nthat are not directly related to machine learning. Secondly, in contrast to\nmost academic research, our participants perceive security of machine learning\nas not solely related to individual models, but rather in the context of entire\nworkflows that consist of multiple components. Jointly with our additional\nfindings, these two facets provide a foundation to substantiate mental models\nfor machine learning security and have implications for the integration of\nadversarial machine learning into corporate workflows, \\new{decreasing\npractitioners' reported uncertainty}, and appropriate regulatory frameworks for\nmachine learning security.", "authors": ["Lukas Bieringer", "Kathrin Grosse", "Michael Backes", "Battista Biggio", "Katharina Krombholz"], "published": "2021-05-08T16:05:07+00:00", "updated": "2022-06-29T13:42:12+00:00", "categories": ["cs.CR", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2105.03726v4", "entry_id": "http://arxiv.org/abs/2105.03726v4"}, {"id": "2206.07090", "filepath": "papers/2206.07090.pdf", "title": "Parallelization of Machine Learning Algorithms Respectively on Single Machine and Spark", "abstract": "With the rapid development of big data technologies, how to dig out useful\ninformation from massive data becomes an essential problem. However, using\nmachine learning algorithms to analyze large data may be time-consuming and\ninefficient on the traditional single machine. To solve these problems, this\npaper has made some research on the parallelization of several classic machine\nlearning algorithms respectively on the single machine and the big data\nplatform Spark. We compare the runtime and efficiency of traditional machine\nlearning algorithms with parallelized machine learning algorithms respectively\non the single machine and Spark platform. The research results have shown\nsignificant improvement in runtime and efficiency of parallelized machine\nlearning algorithms.", "authors": ["Jiajun Shen"], "published": "2022-05-08T03:47:30+00:00", "updated": "2023-04-13T03:30:03+00:00", "categories": ["cs.DC"], "pdf_url": "http://arxiv.org/pdf/2206.07090v2", "entry_id": "http://arxiv.org/abs/2206.07090v2"}, {"id": "1705.07538", "filepath": "papers/1705.07538.pdf", "title": "Infrastructure for Usable Machine Learning: The Stanford DAWN Project", "abstract": "Despite incredible recent advances in machine learning, building machine\nlearning applications remains prohibitively time-consuming and expensive for\nall but the best-trained, best-funded engineering organizations. This expense\ncomes not from a need for new and improved statistical models but instead from\na lack of systems and tools for supporting end-to-end machine learning\napplication development, from data preparation and labeling to\nproductionization and monitoring. In this document, we outline opportunities\nfor infrastructure supporting usable, end-to-end machine learning applications\nin the context of the nascent DAWN (Data Analytics for What's Next) project at\nStanford.", "authors": ["Peter Bailis", "Kunle Olukotun", "Christopher Re", "Matei Zaharia"], "published": "2017-05-22T02:28:19+00:00", "updated": "2017-06-09T02:13:09+00:00", "categories": ["cs.LG", "cs.DB", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1705.07538v2", "entry_id": "http://arxiv.org/abs/1705.07538v2"}, {"id": "2006.15680", "filepath": "papers/2006.15680.pdf", "title": "Modeling Generalization in Machine Learning: A Methodological and Computational Study", "abstract": "As machine learning becomes more and more available to the general public,\ntheoretical questions are turning into pressing practical issues. Possibly, one\nof the most relevant concerns is the assessment of our confidence in trusting\nmachine learning predictions. In many real-world cases, it is of utmost\nimportance to estimate the capabilities of a machine learning algorithm to\ngeneralize, i.e., to provide accurate predictions on unseen data, depending on\nthe characteristics of the target problem. In this work, we perform a\nmeta-analysis of 109 publicly-available classification data sets, modeling\nmachine learning generalization as a function of a variety of data set\ncharacteristics, ranging from number of samples to intrinsic dimensionality,\nfrom class-wise feature skewness to $F1$ evaluated on test samples falling\noutside the convex hull of the training set. Experimental results demonstrate\nthe relevance of using the concept of the convex hull of the training data in\nassessing machine learning generalization, by emphasizing the difference\nbetween interpolated and extrapolated predictions. Besides several predictable\ncorrelations, we observe unexpectedly weak associations between the\ngeneralization ability of machine learning models and all metrics related to\ndimensionality, thus challenging the common assumption that the \\textit{curse\nof dimensionality} might impair generalization in machine learning.", "authors": ["Pietro Barbiero", "Giovanni Squillero", "Alberto Tonda"], "published": "2020-06-28T19:06:16+00:00", "updated": "2020-06-28T19:06:16+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2006.15680v1", "entry_id": "http://arxiv.org/abs/2006.15680v1"}, {"id": "1612.04858", "filepath": "papers/1612.04858.pdf", "title": "Bayesian Optimization for Machine Learning : A Practical Guidebook", "abstract": "The engineering of machine learning systems is still a nascent field; relying\non a seemingly daunting collection of quickly evolving tools and best\npractices. It is our hope that this guidebook will serve as a useful resource\nfor machine learning practitioners looking to take advantage of Bayesian\noptimization techniques. We outline four example machine learning problems that\ncan be solved using open source machine learning libraries, and highlight the\nbenefits of using Bayesian optimization in the context of these common machine\nlearning applications.", "authors": ["Ian Dewancker", "Michael McCourt", "Scott Clark"], "published": "2016-12-14T22:04:33+00:00", "updated": "2016-12-14T22:04:33+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/1612.04858v1", "entry_id": "http://arxiv.org/abs/1612.04858v1"}, {"id": "2103.00742", "filepath": "papers/2103.00742.pdf", "title": "Automated Machine Learning on Graphs: A Survey", "abstract": "Machine learning on graphs has been extensively studied in both academic and\nindustry. However, as the literature on graph learning booms with a vast number\nof emerging methods and techniques, it becomes increasingly difficult to\nmanually design the optimal machine learning algorithm for different\ngraph-related tasks. To solve this critical challenge, automated machine\nlearning (AutoML) on graphs which combines the strength of graph machine\nlearning and AutoML together, is gaining attention from the research community.\nTherefore, we comprehensively survey AutoML on graphs in this paper, primarily\nfocusing on hyper-parameter optimization (HPO) and neural architecture search\n(NAS) for graph machine learning. We further overview libraries related to\nautomated graph machine learning and in-depth discuss AutoGL, the first\ndedicated open-source library for AutoML on graphs. In the end, we share our\ninsights on future research directions for automated graph machine learning.\nThis paper is the first systematic and comprehensive review of automated\nmachine learning on graphs to the best of our knowledge.", "authors": ["Ziwei Zhang", "Xin Wang", "Wenwu Zhu"], "published": "2021-03-01T04:20:33+00:00", "updated": "2021-12-20T05:03:30+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/2103.00742v4", "entry_id": "http://arxiv.org/abs/2103.00742v4"}, {"id": "2102.05639", "filepath": "papers/2102.05639.pdf", "title": "Energy-Harvesting Distributed Machine Learning", "abstract": "This paper provides a first study of utilizing energy harvesting for\nsustainable machine learning in distributed networks. We consider a distributed\nlearning setup in which a machine learning model is trained over a large number\nof devices that can harvest energy from the ambient environment, and develop a\npractical learning framework with theoretical convergence guarantees. We\ndemonstrate through numerical experiments that the proposed framework can\nsignificantly outperform energy-agnostic benchmarks. Our framework is scalable,\nrequires only local estimation of the energy statistics, and can be applied to\na wide range of distributed training settings, including machine learning in\nwireless networks, edge computing, and mobile internet of things.", "authors": ["Basak Guler", "Aylin Yener"], "published": "2021-02-10T18:53:51+00:00", "updated": "2021-02-10T18:53:51+00:00", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2102.05639v1", "entry_id": "http://arxiv.org/abs/2102.05639v1"}, {"id": "1510.00633", "filepath": "papers/1510.00633.pdf", "title": "Distributed Multitask Learning", "abstract": "We consider the problem of distributed multi-task learning, where each\nmachine learns a separate, but related, task. Specifically, each machine learns\na linear predictor in high-dimensional space,where all tasks share the same\nsmall support. We present a communication-efficient estimator based on the\ndebiased lasso and show that it is comparable with the optimal centralized\nmethod.", "authors": ["Jialei Wang", "Mladen Kolar", "Nathan Srebro"], "published": "2015-10-02T16:15:30+00:00", "updated": "2015-10-02T16:15:30+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1510.00633v1", "entry_id": "http://arxiv.org/abs/1510.00633v1"}, {"id": "2003.05155", "filepath": "papers/2003.05155.pdf", "title": "Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology", "abstract": "Machine learning is an established and frequently used technique in industry\nand academia but a standard process model to improve success and efficiency of\nmachine learning applications is still missing. Project organizations and\nmachine learning practitioners have a need for guidance throughout the life\ncycle of a machine learning application to meet business expectations. We\ntherefore propose a process model for the development of machine learning\napplications, that covers six phases from defining the scope to maintaining the\ndeployed machine learning application. The first phase combines business and\ndata understanding as data availability oftentimes affects the feasibility of\nthe project. The sixth phase covers state-of-the-art approaches for monitoring\nand maintenance of a machine learning applications, as the risk of model\ndegradation in a changing environment is eminent. With each task of the\nprocess, we propose quality assurance methodology that is suitable to adress\nchallenges in machine learning development that we identify in form of risks.\nThe methodology is drawn from practical experience and scientific literature\nand has proven to be general and stable. The process model expands on CRISP-DM,\na data mining process model that enjoys strong industry support but lacks to\naddress machine learning specific tasks. Our work proposes an industry and\napplication neutral process model tailored for machine learning applications\nwith focus on technical tasks for quality assurance.", "authors": ["Stefan Studer", "Thanh Binh Bui", "Christian Drescher", "Alexander Hanuschkin", "Ludwig Winkler", "Steven Peters", "Klaus-Robert Mueller"], "published": "2020-03-11T08:25:49+00:00", "updated": "2021-02-24T14:33:24+00:00", "categories": ["cs.LG", "cs.SE", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2003.05155v2", "entry_id": "http://arxiv.org/abs/2003.05155v2"}, {"id": "2204.07492", "filepath": "papers/2204.07492.pdf", "title": "A Machine Learning Tutorial for Operational Meteorology, Part I: Traditional Machine Learning", "abstract": "Recently, the use of machine learning in meteorology has increased greatly.\nWhile many machine learning methods are not new, university classes on machine\nlearning are largely unavailable to meteorology students and are not required\nto become a meteorologist. The lack of formal instruction has contributed to\nperception that machine learning methods are 'black boxes' and thus end-users\nare hesitant to apply the machine learning methods in their every day workflow.\nTo reduce the opaqueness of machine learning methods and lower hesitancy\ntowards machine learning in meteorology, this paper provides a survey of some\nof the most common machine learning methods. A familiar meteorological example\nis used to contextualize the machine learning methods while also discussing\nmachine learning topics using plain language. The following machine learning\nmethods are demonstrated: linear regression; logistic regression; decision\ntrees; random forest; gradient boosted decision trees; naive Bayes; and support\nvector machines. Beyond discussing the different methods, the paper also\ncontains discussions on the general machine learning process as well as best\npractices to enable readers to apply machine learning to their own datasets.\nFurthermore, all code (in the form of Jupyter notebooks and Google Colaboratory\nnotebooks) used to make the examples in the paper is provided in an effort to\ncatalyse the use of machine learning in meteorology.", "authors": ["Randy J. Chase", "David R. Harrison", "Amanda Burke", "Gary M. Lackmann", "Amy McGovern"], "published": "2022-04-15T14:48:04+00:00", "updated": "2022-06-07T14:48:09+00:00", "categories": ["physics.ao-ph", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2204.07492v2", "entry_id": "http://arxiv.org/abs/2204.07492v2"}, {"id": "2212.12303", "filepath": "papers/2212.12303.pdf", "title": "Introduction to Machine Learning for Physicians: A Survival Guide for Data Deluge", "abstract": "Many modern research fields increasingly rely on collecting and analysing\nmassive, often unstructured, and unwieldy datasets. Consequently, there is\ngrowing interest in machine learning and artificial intelligence applications\nthat can harness this `data deluge'. This broad nontechnical overview provides\na gentle introduction to machine learning with a specific focus on medical and\nbiological applications. We explain the common types of machine learning\nalgorithms and typical tasks that can be solved, illustrating the basics with\nconcrete examples from healthcare. Lastly, we provide an outlook on open\nchallenges, limitations, and potential impacts of machine-learning-powered\nmedicine.", "authors": ["Ri\u010dards Marcinkevi\u010ds", "Ece Ozkan", "Julia E. Vogt"], "published": "2022-12-23T13:08:59+00:00", "updated": "2022-12-23T13:08:59+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2212.12303v1", "entry_id": "http://arxiv.org/abs/2212.12303v1"}, {"id": "2003.10146", "filepath": "papers/2003.10146.pdf", "title": "Julia Language in Machine Learning: Algorithms, Applications, and Open Issues", "abstract": "Machine learning is driving development across many fields in science and\nengineering. A simple and efficient programming language could accelerate\napplications of machine learning in various fields. Currently, the programming\nlanguages most commonly used to develop machine learning algorithms include\nPython, MATLAB, and C/C ++. However, none of these languages well balance both\nefficiency and simplicity. The Julia language is a fast, easy-to-use, and\nopen-source programming language that was originally designed for\nhigh-performance computing, which can well balance the efficiency and\nsimplicity. This paper summarizes the related research work and developments in\nthe application of the Julia language in machine learning. It first surveys the\npopular machine learning algorithms that are developed in the Julia language.\nThen, it investigates applications of the machine learning algorithms\nimplemented with the Julia language. Finally, it discusses the open issues and\nthe potential future directions that arise in the use of the Julia language in\nmachine learning.", "authors": ["Kaifeng Gao", "Gang Mei", "Francesco Piccialli", "Salvatore Cuomo", "Jingzhi Tu", "Zenan Huo"], "published": "2020-03-23T09:31:02+00:00", "updated": "2020-05-17T10:52:22+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2003.10146v2", "entry_id": "http://arxiv.org/abs/2003.10146v2"}, {"id": "2001.09608", "filepath": "papers/2001.09608.pdf", "title": "Some Insights into Lifelong Reinforcement Learning Systems", "abstract": "A lifelong reinforcement learning system is a learning system that has the\nability to learn through trail-and-error interaction with the environment over\nits lifetime. In this paper, I give some arguments to show that the traditional\nreinforcement learning paradigm fails to model this type of learning system.\nSome insights into lifelong reinforcement learning are provided, along with a\nsimplistic prototype lifelong reinforcement learning system.", "authors": ["Changjian Li"], "published": "2020-01-27T07:26:12+00:00", "updated": "2020-01-27T07:26:12+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2001.09608v1", "entry_id": "http://arxiv.org/abs/2001.09608v1"}, {"id": "1607.01400", "filepath": "papers/1607.01400.pdf", "title": "An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality in Machine Learning", "abstract": "We propose a clustering-based iterative algorithm to solve certain\noptimization problems in machine learning, where we start the algorithm by\naggregating the original data, solving the problem on aggregated data, and then\nin subsequent steps gradually disaggregate the aggregated data. We apply the\nalgorithm to common machine learning problems such as the least absolute\ndeviation regression problem, support vector machines, and semi-supervised\nsupport vector machines. We derive model-specific data aggregation and\ndisaggregation procedures. We also show optimality, convergence, and the\noptimality gap of the approximated solution in each iteration. A computational\nstudy is provided.", "authors": ["Young Woong Park", "Diego Klabjan"], "published": "2016-07-05T20:04:57+00:00", "updated": "2016-07-05T20:04:57+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1607.01400v1", "entry_id": "http://arxiv.org/abs/1607.01400v1"}, {"id": "1912.09630", "filepath": "papers/1912.09630.pdf", "title": "Practical Solutions for Machine Learning Safety in Autonomous Vehicles", "abstract": "Autonomous vehicles rely on machine learning to solve challenging tasks in\nperception and motion planning. However, automotive software safety standards\nhave not fully evolved to address the challenges of machine learning safety\nsuch as interpretability, verification, and performance limitations. In this\npaper, we review and organize practical machine learning safety techniques that\ncan complement engineering safety for machine learning based software in\nautonomous vehicles. Our organization maps safety strategies to\nstate-of-the-art machine learning techniques in order to enhance dependability\nand safety of machine learning algorithms. We also discuss security limitations\nand user experience aspects of machine learning components in autonomous\nvehicles.", "authors": ["Sina Mohseni", "Mandar Pitale", "Vasu Singh", "Zhangyang Wang"], "published": "2019-12-20T03:47:28+00:00", "updated": "2019-12-20T03:47:28+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1912.09630v1", "entry_id": "http://arxiv.org/abs/1912.09630v1"}, {"id": "1803.10311", "filepath": "papers/1803.10311.pdf", "title": "How Developers Iterate on Machine Learning Workflows -- A Survey of the Applied Machine Learning Literature", "abstract": "Machine learning workflow development is anecdotally regarded to be an\niterative process of trial-and-error with humans-in-the-loop. However, we are\nnot aware of quantitative evidence corroborating this popular belief. A\nquantitative characterization of iteration can serve as a benchmark for machine\nlearning workflow development in practice, and can aid the development of\nhuman-in-the-loop machine learning systems. To this end, we conduct a\nsmall-scale survey of the applied machine learning literature from five\ndistinct application domains. We collect and distill statistics on the role of\niteration within machine learning workflow development, and report preliminary\ntrends and insights from our investigation, as a starting point towards this\nbenchmark. Based on our findings, we finally describe desiderata for effective\nand versatile human-in-the-loop machine learning systems that can cater to\nusers in diverse domains.", "authors": ["Doris Xin", "Litian Ma", "Shuchen Song", "Aditya Parameswaran"], "published": "2018-03-27T20:38:05+00:00", "updated": "2018-05-17T22:16:31+00:00", "categories": ["cs.LG", "cs.DB", "cs.HC", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1803.10311v2", "entry_id": "http://arxiv.org/abs/1803.10311v2"}, {"id": "1605.07805", "filepath": "papers/1605.07805.pdf", "title": "Learning Moore Machines from Input-Output Traces", "abstract": "The problem of learning automata from example traces (but no equivalence or\nmembership queries) is fundamental in automata learning theory and practice. In\nthis paper we study this problem for finite state machines with inputs and\noutputs, and in particular for Moore machines. We develop three algorithms for\nsolving this problem: (1) the PTAP algorithm, which transforms a set of\ninput-output traces into an incomplete Moore machine and then completes the\nmachine with self-loops; (2) the PRPNI algorithm, which uses the well-known\nRPNI algorithm for automata learning to learn a product of automata encoding a\nMoore machine; and (3) the MooreMI algorithm, which directly learns a Moore\nmachine using PTAP extended with state merging. We prove that MooreMI has the\nfundamental identification in the limit property. We also compare the\nalgorithms experimentally in terms of the size of the learned machine and\nseveral notions of accuracy, introduced in this paper. Finally, we compare with\nOSTIA, an algorithm that learns a more general class of transducers, and find\nthat OSTIA generally does not learn a Moore machine, even when fed with a\ncharacteristic sample.", "authors": ["Georgios Giantamidis", "Stavros Tripakis"], "published": "2016-05-25T10:11:03+00:00", "updated": "2016-09-02T09:27:40+00:00", "categories": ["cs.FL", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1605.07805v2", "entry_id": "http://arxiv.org/abs/1605.07805v2"}, {"id": "2407.05520", "filepath": "papers/2407.05520.pdf", "title": "A Theory of Machine Learning", "abstract": "We critically review three major theories of machine learning and provide a\nnew theory according to which machines learn a function when the machines\nsuccessfully compute it. We show that this theory challenges common assumptions\nin the statistical and the computational learning theories, for it implies that\nlearning true probabilities is equivalent neither to obtaining a correct\ncalculation of the true probabilities nor to obtaining an almost-sure\nconvergence to them. We also briefly discuss some case studies from natural\nlanguage processing and macroeconomics from the perspective of the new theory.", "authors": ["Jinsook Kim", "Jinho Kang"], "published": "2024-07-07T23:57:10+00:00", "updated": "2024-07-07T23:57:10+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2407.05520v1", "entry_id": "http://arxiv.org/abs/2407.05520v1"}, {"id": "2303.09491", "filepath": "papers/2303.09491.pdf", "title": "Challenges and Opportunities in Quantum Machine Learning", "abstract": "At the intersection of machine learning and quantum computing, Quantum\nMachine Learning (QML) has the potential of accelerating data analysis,\nespecially for quantum data, with applications for quantum materials,\nbiochemistry, and high-energy physics. Nevertheless, challenges remain\nregarding the trainability of QML models. Here we review current methods and\napplications for QML. We highlight differences between quantum and classical\nmachine learning, with a focus on quantum neural networks and quantum deep\nlearning. Finally, we discuss opportunities for quantum advantage with QML.", "authors": ["M. Cerezo", "Guillaume Verdon", "Hsin-Yuan Huang", "Lukasz Cincio", "Patrick J. Coles"], "published": "2023-03-16T17:10:39+00:00", "updated": "2023-03-16T17:10:39+00:00", "categories": ["quant-ph", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2303.09491v1", "entry_id": "http://arxiv.org/abs/2303.09491v1"}, {"id": "2008.08080", "filepath": "papers/2008.08080.pdf", "title": "mlr3proba: An R Package for Machine Learning in Survival Analysis", "abstract": "As machine learning has become increasingly popular over the last few\ndecades, so too has the number of machine learning interfaces for implementing\nthese models. Whilst many R libraries exist for machine learning, very few\noffer extended support for survival analysis. This is problematic considering\nits importance in fields like medicine, bioinformatics, economics, engineering,\nand more. mlr3proba provides a comprehensive machine learning interface for\nsurvival analysis and connects with mlr3's general model tuning and\nbenchmarking facilities to provide a systematic infrastructure for survival\nmodeling and evaluation.", "authors": ["Raphael Sonabend", "Franz J. Kir\u00e1ly", "Andreas Bender", "Bernd Bischl", "Michel Lang"], "published": "2020-08-18T11:21:24+00:00", "updated": "2020-12-14T11:41:25+00:00", "categories": ["stat.CO", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2008.08080v2", "entry_id": "http://arxiv.org/abs/2008.08080v2"}, {"id": "1808.00033", "filepath": "papers/1808.00033.pdf", "title": "Techniques for Interpretable Machine Learning", "abstract": "Interpretable machine learning tackles the important problem that humans\ncannot understand the behaviors of complex machine learning models and how\nthese models arrive at a particular decision. Although many approaches have\nbeen proposed, a comprehensive understanding of the achievements and challenges\nis still lacking. We provide a survey covering existing techniques to increase\nthe interpretability of machine learning models. We also discuss crucial issues\nthat the community should consider in future work such as designing\nuser-friendly explanations and developing comprehensive evaluation metrics to\nfurther push forward the area of interpretable machine learning.", "authors": ["Mengnan Du", "Ninghao Liu", "Xia Hu"], "published": "2018-07-31T19:14:39+00:00", "updated": "2019-05-19T20:44:37+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1808.00033v3", "entry_id": "http://arxiv.org/abs/1808.00033v3"}, {"id": "2012.04105", "filepath": "papers/2012.04105.pdf", "title": "The Tribes of Machine Learning and the Realm of Computer Architecture", "abstract": "Machine learning techniques have influenced the field of computer\narchitecture like many other fields. This paper studies how the fundamental\nmachine learning techniques can be applied towards computer architecture\nproblems. We also provide a detailed survey of computer architecture research\nthat employs different machine learning methods. Finally, we present some\nfuture opportunities and the outstanding challenges that need to be overcome to\nexploit full potential of machine learning for computer architecture.", "authors": ["Ayaz Akram", "Jason Lowe-Power"], "published": "2020-12-07T23:10:51+00:00", "updated": "2020-12-07T23:10:51+00:00", "categories": ["cs.LG", "cs.AR"], "pdf_url": "http://arxiv.org/pdf/2012.04105v1", "entry_id": "http://arxiv.org/abs/2012.04105v1"}, {"id": "2206.07090", "filepath": "papers/2206.07090.pdf", "title": "Parallelization of Machine Learning Algorithms Respectively on Single Machine and Spark", "abstract": "With the rapid development of big data technologies, how to dig out useful\ninformation from massive data becomes an essential problem. However, using\nmachine learning algorithms to analyze large data may be time-consuming and\ninefficient on the traditional single machine. To solve these problems, this\npaper has made some research on the parallelization of several classic machine\nlearning algorithms respectively on the single machine and the big data\nplatform Spark. We compare the runtime and efficiency of traditional machine\nlearning algorithms with parallelized machine learning algorithms respectively\non the single machine and Spark platform. The research results have shown\nsignificant improvement in runtime and efficiency of parallelized machine\nlearning algorithms.", "authors": ["Jiajun Shen"], "published": "2022-05-08T03:47:30+00:00", "updated": "2023-04-13T03:30:03+00:00", "categories": ["cs.DC"], "pdf_url": "http://arxiv.org/pdf/2206.07090v2", "entry_id": "http://arxiv.org/abs/2206.07090v2"}, {"id": "2003.10146", "filepath": "papers/2003.10146.pdf", "title": "Julia Language in Machine Learning: Algorithms, Applications, and Open Issues", "abstract": "Machine learning is driving development across many fields in science and\nengineering. A simple and efficient programming language could accelerate\napplications of machine learning in various fields. Currently, the programming\nlanguages most commonly used to develop machine learning algorithms include\nPython, MATLAB, and C/C ++. However, none of these languages well balance both\nefficiency and simplicity. The Julia language is a fast, easy-to-use, and\nopen-source programming language that was originally designed for\nhigh-performance computing, which can well balance the efficiency and\nsimplicity. This paper summarizes the related research work and developments in\nthe application of the Julia language in machine learning. It first surveys the\npopular machine learning algorithms that are developed in the Julia language.\nThen, it investigates applications of the machine learning algorithms\nimplemented with the Julia language. Finally, it discusses the open issues and\nthe potential future directions that arise in the use of the Julia language in\nmachine learning.", "authors": ["Kaifeng Gao", "Gang Mei", "Francesco Piccialli", "Salvatore Cuomo", "Jingzhi Tu", "Zenan Huo"], "published": "2020-03-23T09:31:02+00:00", "updated": "2020-05-17T10:52:22+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2003.10146v2", "entry_id": "http://arxiv.org/abs/2003.10146v2"}, {"id": "1912.09630", "filepath": "papers/1912.09630.pdf", "title": "Practical Solutions for Machine Learning Safety in Autonomous Vehicles", "abstract": "Autonomous vehicles rely on machine learning to solve challenging tasks in\nperception and motion planning. However, automotive software safety standards\nhave not fully evolved to address the challenges of machine learning safety\nsuch as interpretability, verification, and performance limitations. In this\npaper, we review and organize practical machine learning safety techniques that\ncan complement engineering safety for machine learning based software in\nautonomous vehicles. Our organization maps safety strategies to\nstate-of-the-art machine learning techniques in order to enhance dependability\nand safety of machine learning algorithms. We also discuss security limitations\nand user experience aspects of machine learning components in autonomous\nvehicles.", "authors": ["Sina Mohseni", "Mandar Pitale", "Vasu Singh", "Zhangyang Wang"], "published": "2019-12-20T03:47:28+00:00", "updated": "2019-12-20T03:47:28+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1912.09630v1", "entry_id": "http://arxiv.org/abs/1912.09630v1"}, {"id": "1907.08908", "filepath": "papers/1907.08908.pdf", "title": "Techniques for Automated Machine Learning", "abstract": "Automated machine learning (AutoML) aims to find optimal machine learning\nsolutions automatically given a machine learning problem. It could release the\nburden of data scientists from the multifarious manual tuning process and\nenable the access of domain experts to the off-the-shelf machine learning\nsolutions without extensive experience. In this paper, we review the current\ndevelopments of AutoML in terms of three categories, automated feature\nengineering (AutoFE), automated model and hyperparameter learning (AutoMHL),\nand automated deep learning (AutoDL). State-of-the-art techniques adopted in\nthe three categories are presented, including Bayesian optimization,\nreinforcement learning, evolutionary algorithm, and gradient-based approaches.\nWe summarize popular AutoML frameworks and conclude with current open\nchallenges of AutoML.", "authors": ["Yi-Wei Chen", "Qingquan Song", "Xia Hu"], "published": "2019-07-21T04:03:36+00:00", "updated": "2019-07-21T04:03:36+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1907.08908v1", "entry_id": "http://arxiv.org/abs/1907.08908v1"}, {"id": "1702.08608", "filepath": "papers/1702.08608.pdf", "title": "Towards A Rigorous Science of Interpretable Machine Learning", "abstract": "As machine learning systems become ubiquitous, there has been a surge of\ninterest in interpretable machine learning: systems that provide explanation\nfor their outputs. These explanations are often used to qualitatively assess\nother criteria such as safety or non-discrimination. However, despite the\ninterest in interpretability, there is very little consensus on what\ninterpretable machine learning is and how it should be measured. In this\nposition paper, we first define interpretability and describe when\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\nfor rigorous evaluation and expose open questions towards a more rigorous\nscience of interpretable machine learning.", "authors": ["Finale Doshi-Velez", "Been Kim"], "published": "2017-02-28T02:19:20+00:00", "updated": "2017-03-02T19:32:10+00:00", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1702.08608v2", "entry_id": "http://arxiv.org/abs/1702.08608v2"}, {"id": "2412.00464", "filepath": "papers/2412.00464.pdf", "title": "On the Conditions for Domain Stability for Machine Learning: a Mathematical Approach", "abstract": "This work proposes a mathematical approach that (re)defines a property of\nMachine Learning models named stability and determines sufficient conditions to\nvalidate it. Machine Learning models are represented as functions, and the\ncharacteristics in scope depend upon the domain of the function, what allows us\nto adopt topological and metric spaces theory as a basis. Finally, this work\nprovides some equivalences useful to prove and test stability in Machine\nLearning models. The results suggest that whenever stability is aligned with\nthe notion of function smoothness, then the stability of Machine Learning\nmodels primarily depends upon certain topological, measurable properties of the\nclassification sets within the ML model domain.", "authors": ["Gabriel Pedroza"], "published": "2024-11-30T12:57:07+00:00", "updated": "2024-11-30T12:57:07+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML", "F.4.1; I.2.0"], "pdf_url": "http://arxiv.org/pdf/2412.00464v1", "entry_id": "http://arxiv.org/abs/2412.00464v1"}, {"id": "2108.07915", "filepath": "papers/2108.07915.pdf", "title": "Data Pricing in Machine Learning Pipelines", "abstract": "Machine learning is disruptive. At the same time, machine learning can only\nsucceed by collaboration among many parties in multiple steps naturally as\npipelines in an eco-system, such as collecting data for possible machine\nlearning applications, collaboratively training models by multiple parties and\ndelivering machine learning services to end users. Data is critical and\npenetrating in the whole machine learning pipelines. As machine learning\npipelines involve many parties and, in order to be successful, have to form a\nconstructive and dynamic eco-system, marketplaces and data pricing are\nfundamental in connecting and facilitating those many parties. In this article,\nwe survey the principles and the latest research development of data pricing in\nmachine learning pipelines. We start with a brief review of data marketplaces\nand pricing desiderata. Then, we focus on pricing in three important steps in\nmachine learning pipelines. To understand pricing in the step of training data\ncollection, we review pricing raw data sets and data labels. We also\ninvestigate pricing in the step of collaborative training of machine learning\nmodels, and overview pricing machine learning models for end users in the step\nof machine learning deployment. We also discuss a series of possible future\ndirections.", "authors": ["Zicun Cong", "Xuan Luo", "Pei Jian", "Feida Zhu", "Yong Zhang"], "published": "2021-08-18T00:57:06+00:00", "updated": "2021-08-18T00:57:06+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/2108.07915v1", "entry_id": "http://arxiv.org/abs/2108.07915v1"}, {"id": "1803.10311", "filepath": "papers/1803.10311.pdf", "title": "How Developers Iterate on Machine Learning Workflows -- A Survey of the Applied Machine Learning Literature", "abstract": "Machine learning workflow development is anecdotally regarded to be an\niterative process of trial-and-error with humans-in-the-loop. However, we are\nnot aware of quantitative evidence corroborating this popular belief. A\nquantitative characterization of iteration can serve as a benchmark for machine\nlearning workflow development in practice, and can aid the development of\nhuman-in-the-loop machine learning systems. To this end, we conduct a\nsmall-scale survey of the applied machine learning literature from five\ndistinct application domains. We collect and distill statistics on the role of\niteration within machine learning workflow development, and report preliminary\ntrends and insights from our investigation, as a starting point towards this\nbenchmark. Based on our findings, we finally describe desiderata for effective\nand versatile human-in-the-loop machine learning systems that can cater to\nusers in diverse domains.", "authors": ["Doris Xin", "Litian Ma", "Shuchen Song", "Aditya Parameswaran"], "published": "2018-03-27T20:38:05+00:00", "updated": "2018-05-17T22:16:31+00:00", "categories": ["cs.LG", "cs.DB", "cs.HC", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1803.10311v2", "entry_id": "http://arxiv.org/abs/1803.10311v2"}, {"id": "1612.04858", "filepath": "papers/1612.04858.pdf", "title": "Bayesian Optimization for Machine Learning : A Practical Guidebook", "abstract": "The engineering of machine learning systems is still a nascent field; relying\non a seemingly daunting collection of quickly evolving tools and best\npractices. It is our hope that this guidebook will serve as a useful resource\nfor machine learning practitioners looking to take advantage of Bayesian\noptimization techniques. We outline four example machine learning problems that\ncan be solved using open source machine learning libraries, and highlight the\nbenefits of using Bayesian optimization in the context of these common machine\nlearning applications.", "authors": ["Ian Dewancker", "Michael McCourt", "Scott Clark"], "published": "2016-12-14T22:04:33+00:00", "updated": "2016-12-14T22:04:33+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/1612.04858v1", "entry_id": "http://arxiv.org/abs/1612.04858v1"}, {"id": "1605.07805", "filepath": "papers/1605.07805.pdf", "title": "Learning Moore Machines from Input-Output Traces", "abstract": "The problem of learning automata from example traces (but no equivalence or\nmembership queries) is fundamental in automata learning theory and practice. In\nthis paper we study this problem for finite state machines with inputs and\noutputs, and in particular for Moore machines. We develop three algorithms for\nsolving this problem: (1) the PTAP algorithm, which transforms a set of\ninput-output traces into an incomplete Moore machine and then completes the\nmachine with self-loops; (2) the PRPNI algorithm, which uses the well-known\nRPNI algorithm for automata learning to learn a product of automata encoding a\nMoore machine; and (3) the MooreMI algorithm, which directly learns a Moore\nmachine using PTAP extended with state merging. We prove that MooreMI has the\nfundamental identification in the limit property. We also compare the\nalgorithms experimentally in terms of the size of the learned machine and\nseveral notions of accuracy, introduced in this paper. Finally, we compare with\nOSTIA, an algorithm that learns a more general class of transducers, and find\nthat OSTIA generally does not learn a Moore machine, even when fed with a\ncharacteristic sample.", "authors": ["Georgios Giantamidis", "Stavros Tripakis"], "published": "2016-05-25T10:11:03+00:00", "updated": "2016-09-02T09:27:40+00:00", "categories": ["cs.FL", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1605.07805v2", "entry_id": "http://arxiv.org/abs/1605.07805v2"}, {"id": "2303.18087", "filepath": "papers/2303.18087.pdf", "title": "Evaluation Challenges for Geospatial ML", "abstract": "As geospatial machine learning models and maps derived from their predictions\nare increasingly used for downstream analyses in science and policy, it is\nimperative to evaluate their accuracy and applicability. Geospatial machine\nlearning has key distinctions from other learning paradigms, and as such, the\ncorrect way to measure performance of spatial machine learning outputs has been\na topic of debate. In this paper, I delineate unique challenges of model\nevaluation for geospatial machine learning with global or remotely sensed\ndatasets, culminating in concrete takeaways to improve evaluations of\ngeospatial model performance.", "authors": ["Esther Rolf"], "published": "2023-03-31T14:24:06+00:00", "updated": "2023-03-31T14:24:06+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2303.18087v1", "entry_id": "http://arxiv.org/abs/2303.18087v1"}, {"id": "2407.05520", "filepath": "papers/2407.05520.pdf", "title": "A Theory of Machine Learning", "abstract": "We critically review three major theories of machine learning and provide a\nnew theory according to which machines learn a function when the machines\nsuccessfully compute it. We show that this theory challenges common assumptions\nin the statistical and the computational learning theories, for it implies that\nlearning true probabilities is equivalent neither to obtaining a correct\ncalculation of the true probabilities nor to obtaining an almost-sure\nconvergence to them. We also briefly discuss some case studies from natural\nlanguage processing and macroeconomics from the perspective of the new theory.", "authors": ["Jinsook Kim", "Jinho Kang"], "published": "2024-07-07T23:57:10+00:00", "updated": "2024-07-07T23:57:10+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2407.05520v1", "entry_id": "http://arxiv.org/abs/2407.05520v1"}, {"id": "2303.09491", "filepath": "papers/2303.09491.pdf", "title": "Challenges and Opportunities in Quantum Machine Learning", "abstract": "At the intersection of machine learning and quantum computing, Quantum\nMachine Learning (QML) has the potential of accelerating data analysis,\nespecially for quantum data, with applications for quantum materials,\nbiochemistry, and high-energy physics. Nevertheless, challenges remain\nregarding the trainability of QML models. Here we review current methods and\napplications for QML. We highlight differences between quantum and classical\nmachine learning, with a focus on quantum neural networks and quantum deep\nlearning. Finally, we discuss opportunities for quantum advantage with QML.", "authors": ["M. Cerezo", "Guillaume Verdon", "Hsin-Yuan Huang", "Lukasz Cincio", "Patrick J. Coles"], "published": "2023-03-16T17:10:39+00:00", "updated": "2023-03-16T17:10:39+00:00", "categories": ["quant-ph", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2303.09491v1", "entry_id": "http://arxiv.org/abs/2303.09491v1"}, {"id": "1903.08801", "filepath": "papers/1903.08801.pdf", "title": "A Unified Analytical Framework for Trustable Machine Learning and Automation Running with Blockchain", "abstract": "Traditional machine learning algorithms use data from databases that are\nmutable, and therefore the data cannot be fully trusted. Also, the machine\nlearning process is difficult to automate. This paper proposes building a\ntrustable machine learning system by using blockchain technology, which can\nstore data in a permanent and immutable way. In addition, smart contracts are\nused to automate the machine learning process. This paper makes three\ncontributions. First, it establishes a link between machine learning technology\nand blockchain technology. Previously, machine learning and blockchain have\nbeen considered two independent technologies without an obvious link. Second,\nit proposes a unified analytical framework for trustable machine learning by\nusing blockchain technology. This unified framework solves both the\ntrustability and automation issues in machine learning. Third, it enables a\ncomputer to translate core machine learning implementation from a single thread\non a single machine to multiple threads on multiple machines running with\nblockchain by using a unified approach. The paper uses association rule mining\nas an example to demonstrate how trustable machine learning can be implemented\nwith blockchain, and it shows how this approach can be used to analyze opioid\nprescriptions to help combat the opioid crisis.", "authors": ["Tao Wang"], "published": "2019-03-21T02:17:08+00:00", "updated": "2019-03-21T02:17:08+00:00", "categories": ["cs.LG", "cs.CR"], "pdf_url": "http://arxiv.org/pdf/1903.08801v1", "entry_id": "http://arxiv.org/abs/1903.08801v1"}, {"id": "2301.09753", "filepath": "papers/2301.09753.pdf", "title": "Towards Modular Machine Learning Solution Development: Benefits and Trade-offs", "abstract": "Machine learning technologies have demonstrated immense capabilities in\nvarious domains. They play a key role in the success of modern businesses.\nHowever, adoption of machine learning technologies has a lot of untouched\npotential. Cost of developing custom machine learning solutions that solve\nunique business problems is a major inhibitor to far-reaching adoption of\nmachine learning technologies. We recognize that the monolithic nature\nprevalent in today's machine learning applications stands in the way of\nefficient and cost effective customized machine learning solution development.\nIn this work we explore the benefits of modular machine learning solutions and\ndiscuss how modular machine learning solutions can overcome some of the major\nsolution engineering limitations of monolithic machine learning solutions. We\nanalyze the trade-offs between modular and monolithic machine learning\nsolutions through three deep learning problems; one text based and the two\nimage based. Our experimental results show that modular machine learning\nsolutions have a promising potential to reap the solution engineering\nadvantages of modularity while gaining performance and data advantages in a way\nthe monolithic machine learning solutions do not permit.", "authors": ["Samiyuru Menik", "Lakshmish Ramaswamy"], "published": "2023-01-23T22:54:34+00:00", "updated": "2023-01-23T22:54:34+00:00", "categories": ["cs.LG", "cs.SE"], "pdf_url": "http://arxiv.org/pdf/2301.09753v1", "entry_id": "http://arxiv.org/abs/2301.09753v1"}, {"id": "2103.11249", "filepath": "papers/2103.11249.pdf", "title": "SELM: Software Engineering of Machine Learning Models", "abstract": "One of the pillars of any machine learning model is its concepts. Using\nsoftware engineering, we can engineer these concepts and then develop and\nexpand them. In this article, we present a SELM framework for Software\nEngineering of machine Learning Models. We then evaluate this framework through\na case study. Using the SELM framework, we can improve a machine learning\nprocess efficiency and provide more accuracy in learning with less processing\nhardware resources and a smaller training dataset. This issue highlights the\nimportance of an interdisciplinary approach to machine learning. Therefore, in\nthis article, we have provided interdisciplinary teams' proposals for machine\nlearning.", "authors": ["Nafiseh Jafari", "Mohammad Reza Besharati", "Mohammad Izadi", "Maryam Hourali"], "published": "2021-03-20T21:43:24+00:00", "updated": "2021-03-20T21:43:24+00:00", "categories": ["cs.SE", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2103.11249v1", "entry_id": "http://arxiv.org/abs/2103.11249v1"}, {"id": "2009.11087", "filepath": "papers/2009.11087.pdf", "title": "Probabilistic Machine Learning for Healthcare", "abstract": "Machine learning can be used to make sense of healthcare data. Probabilistic\nmachine learning models help provide a complete picture of observed data in\nhealthcare. In this review, we examine how probabilistic machine learning can\nadvance healthcare. We consider challenges in the predictive model building\npipeline where probabilistic models can be beneficial including calibration and\nmissing data. Beyond predictive models, we also investigate the utility of\nprobabilistic machine learning models in phenotyping, in generative models for\nclinical use cases, and in reinforcement learning.", "authors": ["Irene Y. Chen", "Shalmali Joshi", "Marzyeh Ghassemi", "Rajesh Ranganath"], "published": "2020-09-23T12:14:05+00:00", "updated": "2020-09-23T12:14:05+00:00", "categories": ["stat.ML", "cs.CY", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2009.11087v1", "entry_id": "http://arxiv.org/abs/2009.11087v1"}, {"id": "2001.11489", "filepath": "papers/2001.11489.pdf", "title": "Machine Learning in Network Security Using KNIME Analytics", "abstract": "Machine learning has more and more effect on our every day's life. This field\nkeeps growing and expanding into new areas. Machine learning is based on the\nimplementation of artificial intelligence that gives systems the capability to\nautomatically learn and enhance from experiments without being explicitly\nprogrammed. Machine Learning algorithms apply mathematical equations to analyze\ndatasets and predict values based on the dataset. In the field of\ncybersecurity, machine learning algorithms can be utilized to train and analyze\nthe Intrusion Detection Systems (IDSs) on security-related datasets. In this\npaper, we tested different machine learning algorithms to analyze NSL-KDD\ndataset using KNIME analytics.", "authors": ["Munther Abualkibash"], "published": "2019-11-18T14:10:17+00:00", "updated": "2019-11-18T14:10:17+00:00", "categories": ["cs.CR"], "pdf_url": "http://arxiv.org/pdf/2001.11489v1", "entry_id": "http://arxiv.org/abs/2001.11489v1"}, {"id": "1509.00913", "filepath": "papers/1509.00913.pdf", "title": "On-the-Fly Learning in a Perpetual Learning Machine", "abstract": "Despite the promise of brain-inspired machine learning, deep neural networks\n(DNN) have frustratingly failed to bridge the deceptively large gap between\nlearning and memory. Here, we introduce a Perpetual Learning Machine; a new\ntype of DNN that is capable of brain-like dynamic 'on the fly' learning because\nit exists in a self-supervised state of Perpetual Stochastic Gradient Descent.\nThus, we provide the means to unify learning and memory within a machine\nlearning framework. We also explore the elegant duality of abstraction and\nsynthesis: the Yin and Yang of deep learning.", "authors": ["Andrew J. R. Simpson"], "published": "2015-09-03T01:30:29+00:00", "updated": "2015-09-29T16:57:42+00:00", "categories": ["cs.LG", "68Txx"], "pdf_url": "http://arxiv.org/pdf/1509.00913v3", "entry_id": "http://arxiv.org/abs/1509.00913v3"}, {"id": "2110.12773", "filepath": "papers/2110.12773.pdf", "title": "Scientific Machine Learning Benchmarks", "abstract": "The breakthrough in Deep Learning neural networks has transformed the use of\nAI and machine learning technologies for the analysis of very large\nexperimental datasets. These datasets are typically generated by large-scale\nexperimental facilities at national laboratories. In the context of science,\nscientific machine learning focuses on training machines to identify patterns,\ntrends, and anomalies to extract meaningful scientific insights from such\ndatasets. With a new generation of experimental facilities, the rate of data\ngeneration and the scale of data volumes will increasingly require the use of\nmore automated data analysis. At present, identifying the most appropriate\nmachine learning algorithm for the analysis of any given scientific dataset is\nstill a challenge for scientists. This is due to many different machine\nlearning frameworks, computer architectures, and machine learning models.\nHistorically, for modelling and simulation on HPC systems such problems have\nbeen addressed through benchmarking computer applications, algorithms, and\narchitectures. Extending such a benchmarking approach and identifying metrics\nfor the application of machine learning methods to scientific datasets is a new\nchallenge for both scientists and computer scientists. In this paper, we\ndescribe our approach to the development of scientific machine learning\nbenchmarks and review other approaches to benchmarking scientific machine\nlearning.", "authors": ["Jeyan Thiyagalingam", "Mallikarjun Shankar", "Geoffrey Fox", "Tony Hey"], "published": "2021-10-25T10:05:11+00:00", "updated": "2021-10-25T10:05:11+00:00", "categories": ["cs.LG", "physics.comp-ph", "I.2"], "pdf_url": "http://arxiv.org/pdf/2110.12773v1", "entry_id": "http://arxiv.org/abs/2110.12773v1"}, {"id": "1910.02544", "filepath": "papers/1910.02544.pdf", "title": "Using Deep Learning and Machine Learning to Detect Epileptic Seizure with Electroencephalography (EEG) Data", "abstract": "The prediction of epileptic seizure has always been extremely challenging in\nmedical domain. However, as the development of computer technology, the\napplication of machine learning introduced new ideas for seizure forecasting.\nApplying machine learning model onto the predication of epileptic seizure could\nhelp us obtain a better result and there have been plenty of scientists who\nhave been doing such works so that there are sufficient medical data provided\nfor researchers to do training of machine learning models.", "authors": ["Haotian Liu", "Lin Xi", "Ying Zhao", "Zhixiang Li"], "published": "2019-10-06T22:53:28+00:00", "updated": "2019-10-06T22:53:28+00:00", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1910.02544v1", "entry_id": "http://arxiv.org/abs/1910.02544v1"}, {"id": "1908.04710", "filepath": "papers/1908.04710.pdf", "title": "metric-learn: Metric Learning Algorithms in Python", "abstract": "metric-learn is an open source Python package implementing supervised and\nweakly-supervised distance metric learning algorithms. As part of\nscikit-learn-contrib, it provides a unified interface compatible with\nscikit-learn which allows to easily perform cross-validation, model selection,\nand pipelining with other machine learning estimators. metric-learn is\nthoroughly tested and available on PyPi under the MIT licence.", "authors": ["William de Vazelhes", "CJ Carey", "Yuan Tang", "Nathalie Vauquier", "Aur\u00e9lien Bellet"], "published": "2019-08-13T15:52:31+00:00", "updated": "2020-07-27T14:47:52+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1908.04710v3", "entry_id": "http://arxiv.org/abs/1908.04710v3"}, {"id": "1909.03550", "filepath": "papers/1909.03550.pdf", "title": "Lecture Notes: Optimization for Machine Learning", "abstract": "Lecture notes on optimization for machine learning, derived from a course at\nPrinceton University and tutorials given in MLSS, Buenos Aires, as well as\nSimons Foundation, Berkeley.", "authors": ["Elad Hazan"], "published": "2019-09-08T21:49:42+00:00", "updated": "2019-09-08T21:49:42+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1909.03550v1", "entry_id": "http://arxiv.org/abs/1909.03550v1"}, {"id": "1607.01400", "filepath": "papers/1607.01400.pdf", "title": "An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality in Machine Learning", "abstract": "We propose a clustering-based iterative algorithm to solve certain\noptimization problems in machine learning, where we start the algorithm by\naggregating the original data, solving the problem on aggregated data, and then\nin subsequent steps gradually disaggregate the aggregated data. We apply the\nalgorithm to common machine learning problems such as the least absolute\ndeviation regression problem, support vector machines, and semi-supervised\nsupport vector machines. We derive model-specific data aggregation and\ndisaggregation procedures. We also show optimality, convergence, and the\noptimality gap of the approximated solution in each iteration. A computational\nstudy is provided.", "authors": ["Young Woong Park", "Diego Klabjan"], "published": "2016-07-05T20:04:57+00:00", "updated": "2016-07-05T20:04:57+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1607.01400v1", "entry_id": "http://arxiv.org/abs/1607.01400v1"}, {"id": "1908.00868", "filepath": "papers/1908.00868.pdf", "title": "Machine Learning as Ecology", "abstract": "Machine learning methods have had spectacular success on numerous problems.\nHere we show that a prominent class of learning algorithms - including Support\nVector Machines (SVMs) -- have a natural interpretation in terms of ecological\ndynamics. We use these ideas to design new online SVM algorithms that exploit\necological invasions, and benchmark performance using the MNIST dataset. Our\nwork provides a new ecological lens through which we can view statistical\nlearning and opens the possibility of designing ecosystems for machine\nlearning.\n  Supplemental code is found at https://github.com/owenhowell20/EcoSVM.", "authors": ["Owen Howell", "Cui Wenping", "Robert Marsland III", "Pankaj Mehta"], "published": "2019-08-02T14:08:17+00:00", "updated": "2019-08-23T13:52:08+00:00", "categories": ["cs.LG", "cond-mat.stat-mech", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1908.00868v2", "entry_id": "http://arxiv.org/abs/1908.00868v2"}, {"id": "2202.10564", "filepath": "papers/2202.10564.pdf", "title": "Human-in-the-loop Machine Learning: A Macro-Micro Perspective", "abstract": "Though technical advance of artificial intelligence and machine learning has\nenabled many promising intelligent systems, many computing tasks are still not\nable to be fully accomplished by machine intelligence. Motivated by the\ncomplementary nature of human and machine intelligence, an emerging trend is to\ninvolve humans in the loop of machine learning and decision-making. In this\npaper, we provide a macro-micro review of human-in-the-loop machine learning.\nWe first describe major machine learning challenges which can be addressed by\nhuman intervention in the loop. Then we examine closely the latest research and\nfindings of introducing humans into each step of the lifecycle of machine\nlearning. Finally, we analyze current research gaps and point out future\nresearch directions.", "authors": ["Jiangtao Wang", "Bin Guo", "Liming Chen"], "published": "2022-02-21T22:45:59+00:00", "updated": "2022-02-21T22:45:59+00:00", "categories": ["cs.HC"], "pdf_url": "http://arxiv.org/pdf/2202.10564v1", "entry_id": "http://arxiv.org/abs/2202.10564v1"}, {"id": "1612.04251", "filepath": "papers/1612.04251.pdf", "title": "TF.Learn: TensorFlow's High-level Module for Distributed Machine Learning", "abstract": "TF.Learn is a high-level Python module for distributed machine learning\ninside TensorFlow. It provides an easy-to-use Scikit-learn style interface to\nsimplify the process of creating, configuring, training, evaluating, and\nexperimenting a machine learning model. TF.Learn integrates a wide range of\nstate-of-art machine learning algorithms built on top of TensorFlow's low level\nAPIs for small to large-scale supervised and unsupervised problems. This module\nfocuses on bringing machine learning to non-specialists using a general-purpose\nhigh-level language as well as researchers who want to implement, benchmark,\nand compare their new methods in a structured environment. Emphasis is put on\nease of use, performance, documentation, and API consistency.", "authors": ["Yuan Tang"], "published": "2016-12-13T16:00:51+00:00", "updated": "2016-12-13T16:00:51+00:00", "categories": ["cs.DC", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1612.04251v1", "entry_id": "http://arxiv.org/abs/1612.04251v1"}, {"id": "1910.12387", "filepath": "papers/1910.12387.pdf", "title": "Components of Machine Learning: Binding Bits and FLOPS", "abstract": "Many machine learning problems and methods are combinations of three\ncomponents: data, hypothesis space and loss function. Different machine\nlearning methods are obtained as combinations of different choices for the\nrepresentation of data, hypothesis space and loss function. After reviewing the\nmathematical structure of these three components, we discuss intrinsic\ntrade-offs between statistical and computational properties of machine learning\nmethods.", "authors": ["Alexander Jung"], "published": "2019-10-25T17:33:33+00:00", "updated": "2019-10-30T07:59:02+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/1910.12387v2", "entry_id": "http://arxiv.org/abs/1910.12387v2"}, {"id": "2212.12303", "filepath": "papers/2212.12303.pdf", "title": "Introduction to Machine Learning for Physicians: A Survival Guide for Data Deluge", "abstract": "Many modern research fields increasingly rely on collecting and analysing\nmassive, often unstructured, and unwieldy datasets. Consequently, there is\ngrowing interest in machine learning and artificial intelligence applications\nthat can harness this `data deluge'. This broad nontechnical overview provides\na gentle introduction to machine learning with a specific focus on medical and\nbiological applications. We explain the common types of machine learning\nalgorithms and typical tasks that can be solved, illustrating the basics with\nconcrete examples from healthcare. Lastly, we provide an outlook on open\nchallenges, limitations, and potential impacts of machine-learning-powered\nmedicine.", "authors": ["Ri\u010dards Marcinkevi\u010ds", "Ece Ozkan", "Julia E. Vogt"], "published": "2022-12-23T13:08:59+00:00", "updated": "2022-12-23T13:08:59+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2212.12303v1", "entry_id": "http://arxiv.org/abs/2212.12303v1"}, {"id": "1909.09246", "filepath": "papers/1909.09246.pdf", "title": "Machine Learning for Clinical Predictive Analytics", "abstract": "In this chapter, we provide a brief overview of applying machine learning\ntechniques for clinical prediction tasks. We begin with a quick introduction to\nthe concepts of machine learning and outline some of the most common machine\nlearning algorithms. Next, we demonstrate how to apply the algorithms with\nappropriate toolkits to conduct machine learning experiments for clinical\nprediction tasks. The objectives of this chapter are to (1) understand the\nbasics of machine learning techniques and the reasons behind why they are\nuseful for solving clinical prediction problems, (2) understand the intuition\nbehind some machine learning models, including regression, decision trees, and\nsupport vector machines, and (3) understand how to apply these models to\nclinical prediction problems using publicly available datasets via case\nstudies.", "authors": ["Wei-Hung Weng"], "published": "2019-09-19T22:02:00+00:00", "updated": "2019-09-19T22:02:00+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1909.09246v1", "entry_id": "http://arxiv.org/abs/1909.09246v1"}, {"id": "1405.1304", "filepath": "papers/1405.1304.pdf", "title": "Application of Machine Learning Techniques in Aquaculture", "abstract": "In this paper we present applications of different machine learning\nalgorithms in aquaculture. Machine learning algorithms learn models from\nhistorical data. In aquaculture historical data are obtained from farm\npractices, yields, and environmental data sources. Associations between these\ndifferent variables can be obtained by applying machine learning algorithms to\nhistorical data. In this paper we present applications of different machine\nlearning algorithms in aquaculture applications.", "authors": ["Akhlaqur Rahman", "Sumaira Tasnim"], "published": "2014-05-03T14:26:42+00:00", "updated": "2014-05-03T14:26:42+00:00", "categories": ["cs.CE", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1405.1304v1", "entry_id": "http://arxiv.org/abs/1405.1304v1"}, {"id": "2007.14206", "filepath": "papers/2007.14206.pdf", "title": "Machine Learning Potential Repository", "abstract": "This paper introduces a machine learning potential repository that includes\nPareto optimal machine learning potentials. It also shows the systematic\ndevelopment of accurate and fast machine learning potentials for a wide range\nof elemental systems. As a result, many Pareto optimal machine learning\npotentials are available in the repository from a website. Therefore, the\nrepository will help many scientists to perform accurate and fast atomistic\nsimulations.", "authors": ["Atsuto Seko"], "published": "2020-07-27T14:30:23+00:00", "updated": "2020-07-27T14:30:23+00:00", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci", "physics.chem-ph", "physics.data-an"], "pdf_url": "http://arxiv.org/pdf/2007.14206v1", "entry_id": "http://arxiv.org/abs/2007.14206v1"}, {"id": "1810.11383", "filepath": "papers/1810.11383.pdf", "title": "Some Requests for Machine Learning Research from the East African Tech Scene", "abstract": "Based on 46 in-depth interviews with scientists, engineers, and CEOs, this\ndocument presents a list of concrete machine research problems, progress on\nwhich would directly benefit tech ventures in East Africa.", "authors": ["Milan Cvitkovic"], "published": "2018-10-25T02:53:14+00:00", "updated": "2018-11-08T01:03:50+00:00", "categories": ["cs.LG", "cs.CY", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1810.11383v2", "entry_id": "http://arxiv.org/abs/1810.11383v2"}, {"id": "2007.05479", "filepath": "papers/2007.05479.pdf", "title": "Impact of Legal Requirements on Explainability in Machine Learning", "abstract": "The requirements on explainability imposed by European laws and their\nimplications for machine learning (ML) models are not always clear. In that\nperspective, our research analyzes explanation obligations imposed for private\nand public decision-making, and how they can be implemented by machine learning\ntechniques.", "authors": ["Adrien Bibal", "Michael Lognoul", "Alexandre de Streel", "Beno\u00eet Fr\u00e9nay"], "published": "2020-07-10T16:57:18+00:00", "updated": "2020-07-10T16:57:18+00:00", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2007.05479v1", "entry_id": "http://arxiv.org/abs/2007.05479v1"}, {"id": "1611.03969", "filepath": "papers/1611.03969.pdf", "title": "An Introduction to MM Algorithms for Machine Learning and Statistical", "abstract": "MM (majorization--minimization) algorithms are an increasingly popular tool\nfor solving optimization problems in machine learning and statistical\nestimation. This article introduces the MM algorithm framework in general and\nvia three popular example applications: Gaussian mixture regressions,\nmultinomial logistic regressions, and support vector machines. Specific\nalgorithms for the three examples are derived and numerical demonstrations are\npresented. Theoretical and practical aspects of MM algorithm design are\ndiscussed.", "authors": ["Hien D. Nguyen"], "published": "2016-11-12T08:18:38+00:00", "updated": "2016-11-12T08:18:38+00:00", "categories": ["stat.CO", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1611.03969v1", "entry_id": "http://arxiv.org/abs/1611.03969v1"}, {"id": "2412.18979", "filepath": "papers/2412.18979.pdf", "title": "Quantum memristors for neuromorphic quantum machine learning", "abstract": "Quantum machine learning may permit to realize more efficient machine\nlearning calculations with near-term quantum devices. Among the diverse quantum\nmachine learning paradigms which are currently being considered, quantum\nmemristors are promising as a way of combining, in the same quantum hardware, a\nunitary evolution with the nonlinearity provided by the measurement and\nfeedforward. Thus, an efficient way of deploying neuromorphic quantum computing\nfor quantum machine learning may be enabled.", "authors": ["Lucas Lamata"], "published": "2024-12-25T20:21:24+00:00", "updated": "2024-12-25T20:21:24+00:00", "categories": ["quant-ph", "cs.NE"], "pdf_url": "http://arxiv.org/pdf/2412.18979v1", "entry_id": "http://arxiv.org/abs/2412.18979v1"}, {"id": "2407.05526", "filepath": "papers/2407.05526.pdf", "title": "Can Machines Learn the True Probabilities?", "abstract": "When there exists uncertainty, AI machines are designed to make decisions so\nas to reach the best expected outcomes. Expectations are based on true facts\nabout the objective environment the machines interact with, and those facts can\nbe encoded into AI models in the form of true objective probability functions.\nAccordingly, AI models involve probabilistic machine learning in which the\nprobabilities should be objectively interpreted. We prove under some basic\nassumptions when machines can learn the true objective probabilities, if any,\nand when machines cannot learn them.", "authors": ["Jinsook Kim"], "published": "2024-07-08T00:19:43+00:00", "updated": "2024-07-08T00:19:43+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2407.05526v1", "entry_id": "http://arxiv.org/abs/2407.05526v1"}, {"id": "1207.4676", "filepath": "papers/1207.4676.pdf", "title": "Proceedings of the 29th International Conference on Machine Learning (ICML-12)", "abstract": "This is an index to the papers that appear in the Proceedings of the 29th\nInternational Conference on Machine Learning (ICML-12). The conference was held\nin Edinburgh, Scotland, June 27th - July 3rd, 2012.", "authors": ["John Langford", "Joelle Pineau"], "published": "2012-07-19T14:08:22+00:00", "updated": "2012-09-16T11:24:54+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1207.4676v2", "entry_id": "http://arxiv.org/abs/1207.4676v2"}, {"id": "1707.03184", "filepath": "papers/1707.03184.pdf", "title": "A Survey on Resilient Machine Learning", "abstract": "Machine learning based system are increasingly being used for sensitive tasks\nsuch as security surveillance, guiding autonomous vehicle, taking investment\ndecisions, detecting and blocking network intrusion and malware etc. However,\nrecent research has shown that machine learning models are venerable to attacks\nby adversaries at all phases of machine learning (eg, training data collection,\ntraining, operation). All model classes of machine learning systems can be\nmisled by providing carefully crafted inputs making them wrongly classify\ninputs. Maliciously created input samples can affect the learning process of a\nML system by either slowing down the learning process, or affecting the\nperformance of the learned mode, or causing the system make error(s) only in\nattacker's planned scenario. Because of these developments, understanding\nsecurity of machine learning algorithms and systems is emerging as an important\nresearch area among computer security and machine learning researchers and\npractitioners. We present a survey of this emerging area in machine learning.", "authors": ["Atul Kumar", "Sameep Mehta"], "published": "2017-07-11T09:15:46+00:00", "updated": "2017-07-11T09:15:46+00:00", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1707.03184v1", "entry_id": "http://arxiv.org/abs/1707.03184v1"}, {"id": "1812.01410", "filepath": "papers/1812.01410.pdf", "title": "Compressive Classification (Machine Learning without learning)", "abstract": "Compressive learning is a framework where (so far unsupervised) learning\ntasks use not the entire dataset but a compressed summary (sketch) of it. We\npropose a compressive learning classification method, and a novel sketch\nfunction for images.", "authors": ["Vincent Schellekens", "Laurent Jacques"], "published": "2018-12-04T13:50:11+00:00", "updated": "2018-12-04T13:50:11+00:00", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1812.01410v1", "entry_id": "http://arxiv.org/abs/1812.01410v1"}, {"id": "0904.3664", "filepath": "papers/0904.3664.pdf", "title": "Introduction to Machine Learning: Class Notes 67577", "abstract": "Introduction to Machine learning covering Statistical Inference (Bayes, EM,\nML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),\nand PAC learning (the Formal model, VC dimension, Double Sampling theorem).", "authors": ["Amnon Shashua"], "published": "2009-04-23T11:40:57+00:00", "updated": "2009-04-23T11:40:57+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/0904.3664v1", "entry_id": "http://arxiv.org/abs/0904.3664v1"}, {"id": "1706.08001", "filepath": "papers/1706.08001.pdf", "title": "Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of learning relational order via reinforcement learning procedure?", "abstract": "In this article, we extend the conventional framework of\nconvolutional-Restricted-Boltzmann-Machine to learn highly abstract features\namong abitrary number of time related input maps by constructing a layer of\nmultiplicative units, which capture the relations among inputs. In many cases,\nmore than two maps are strongly related, so it is wise to make multiplicative\nunit learn relations among more input maps, in other words, to find the optimal\nrelational-order of each unit. In order to enable our machine to learn\nrelational order, we developed a reinforcement-learning method whose optimality\nis proven to train the network.", "authors": ["Zizhuang Wang"], "published": "2017-06-24T20:56:27+00:00", "updated": "2017-06-24T20:56:27+00:00", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1706.08001v1", "entry_id": "http://arxiv.org/abs/1706.08001v1"}, {"id": "2401.11351", "filepath": "papers/2401.11351.pdf", "title": "A comprehensive review of Quantum Machine Learning: from NISQ to Fault Tolerance", "abstract": "Quantum machine learning, which involves running machine learning algorithms\non quantum devices, has garnered significant attention in both academic and\nbusiness circles. In this paper, we offer a comprehensive and unbiased review\nof the various concepts that have emerged in the field of quantum machine\nlearning. This includes techniques used in Noisy Intermediate-Scale Quantum\n(NISQ) technologies and approaches for algorithms compatible with\nfault-tolerant quantum computing hardware. Our review covers fundamental\nconcepts, algorithms, and the statistical learning theory pertinent to quantum\nmachine learning.", "authors": ["Yunfei Wang", "Junyu Liu"], "published": "2024-01-21T00:19:16+00:00", "updated": "2024-03-31T00:32:13+00:00", "categories": ["quant-ph", "cs.AI", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2401.11351v2", "entry_id": "http://arxiv.org/abs/2401.11351v2"}, {"id": "2103.00742", "filepath": "papers/2103.00742.pdf", "title": "Automated Machine Learning on Graphs: A Survey", "abstract": "Machine learning on graphs has been extensively studied in both academic and\nindustry. However, as the literature on graph learning booms with a vast number\nof emerging methods and techniques, it becomes increasingly difficult to\nmanually design the optimal machine learning algorithm for different\ngraph-related tasks. To solve this critical challenge, automated machine\nlearning (AutoML) on graphs which combines the strength of graph machine\nlearning and AutoML together, is gaining attention from the research community.\nTherefore, we comprehensively survey AutoML on graphs in this paper, primarily\nfocusing on hyper-parameter optimization (HPO) and neural architecture search\n(NAS) for graph machine learning. We further overview libraries related to\nautomated graph machine learning and in-depth discuss AutoGL, the first\ndedicated open-source library for AutoML on graphs. In the end, we share our\ninsights on future research directions for automated graph machine learning.\nThis paper is the first systematic and comprehensive review of automated\nmachine learning on graphs to the best of our knowledge.", "authors": ["Ziwei Zhang", "Xin Wang", "Wenwu Zhu"], "published": "2021-03-01T04:20:33+00:00", "updated": "2021-12-20T05:03:30+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/2103.00742v4", "entry_id": "http://arxiv.org/abs/2103.00742v4"}, {"id": "2201.06921", "filepath": "papers/2201.06921.pdf", "title": "Can Machine Learning be Moral?", "abstract": "The ethics of Machine Learning has become an unavoidable topic in the AI\nCommunity. The deployment of machine learning systems in multiple social\ncontexts has resulted in a closer ethical scrutiny of the design, development,\nand application of these systems. The AI/ML community has come to terms with\nthe imperative to think about the ethical implications of machine learning, not\nonly as a product but also as a practice (Birhane, 2021; Shen et al. 2021). The\ncritical question that is troubling many debates is what can constitute an\nethically accountable machine learning system. In this paper we explore\npossibilities for ethical evaluation of machine learning methodologies. We\nscrutinize techniques, methods and technical practices in machine learning from\na relational ethics perspective, taking into consideration how machine learning\nsystems are part of the world and how they relate to different forms of agency.\nTaking a page from Phil Agre (1997) we use the notion of a critical technical\npractice as a means of analysis of machine learning approaches. Our radical\nproposal is that supervised learning appears to be the only machine learning\nmethod that is ethically defensible.", "authors": ["Miguel Sicart", "Irina Shklovski", "Mirabelle Jones"], "published": "2021-12-13T07:20:50+00:00", "updated": "2021-12-13T07:20:50+00:00", "categories": ["cs.CY", "cs.HC"], "pdf_url": "http://arxiv.org/pdf/2201.06921v1", "entry_id": "http://arxiv.org/abs/2201.06921v1"}, {"id": "1607.02450", "filepath": "papers/1607.02450.pdf", "title": "Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in Social Good Applications", "abstract": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning\nin Social Good Applications, which was held on June 24, 2016 in New York.", "authors": ["Kush R. Varshney"], "published": "2016-07-08T16:55:31+00:00", "updated": "2016-08-28T15:23:47+00:00", "categories": ["stat.ML", "cs.CY", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1607.02450v2", "entry_id": "http://arxiv.org/abs/1607.02450v2"}, {"id": "1705.07538", "filepath": "papers/1705.07538.pdf", "title": "Infrastructure for Usable Machine Learning: The Stanford DAWN Project", "abstract": "Despite incredible recent advances in machine learning, building machine\nlearning applications remains prohibitively time-consuming and expensive for\nall but the best-trained, best-funded engineering organizations. This expense\ncomes not from a need for new and improved statistical models but instead from\na lack of systems and tools for supporting end-to-end machine learning\napplication development, from data preparation and labeling to\nproductionization and monitoring. In this document, we outline opportunities\nfor infrastructure supporting usable, end-to-end machine learning applications\nin the context of the nascent DAWN (Data Analytics for What's Next) project at\nStanford.", "authors": ["Peter Bailis", "Kunle Olukotun", "Christopher Re", "Matei Zaharia"], "published": "2017-05-22T02:28:19+00:00", "updated": "2017-06-09T02:13:09+00:00", "categories": ["cs.LG", "cs.DB", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1705.07538v2", "entry_id": "http://arxiv.org/abs/1705.07538v2"}, {"id": "2102.05639", "filepath": "papers/2102.05639.pdf", "title": "Energy-Harvesting Distributed Machine Learning", "abstract": "This paper provides a first study of utilizing energy harvesting for\nsustainable machine learning in distributed networks. We consider a distributed\nlearning setup in which a machine learning model is trained over a large number\nof devices that can harvest energy from the ambient environment, and develop a\npractical learning framework with theoretical convergence guarantees. We\ndemonstrate through numerical experiments that the proposed framework can\nsignificantly outperform energy-agnostic benchmarks. Our framework is scalable,\nrequires only local estimation of the energy statistics, and can be applied to\na wide range of distributed training settings, including machine learning in\nwireless networks, edge computing, and mobile internet of things.", "authors": ["Basak Guler", "Aylin Yener"], "published": "2021-02-10T18:53:51+00:00", "updated": "2021-02-10T18:53:51+00:00", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2102.05639v1", "entry_id": "http://arxiv.org/abs/2102.05639v1"}, {"id": "2011.11819", "filepath": "papers/2011.11819.pdf", "title": "When Machine Learning Meets Privacy: A Survey and Outlook", "abstract": "The newly emerged machine learning (e.g. deep learning) methods have become a\nstrong driving force to revolutionize a wide range of industries, such as smart\nhealthcare, financial technology, and surveillance systems. Meanwhile, privacy\nhas emerged as a big concern in this machine learning-based artificial\nintelligence era. It is important to note that the problem of privacy\npreservation in the context of machine learning is quite different from that in\ntraditional data privacy protection, as machine learning can act as both friend\nand foe. Currently, the work on the preservation of privacy and machine\nlearning (ML) is still in an infancy stage, as most existing solutions only\nfocus on privacy problems during the machine learning process. Therefore, a\ncomprehensive study on the privacy preservation problems and machine learning\nis required. This paper surveys the state of the art in privacy issues and\nsolutions for machine learning. The survey covers three categories of\ninteractions between privacy and machine learning: (i) private machine\nlearning, (ii) machine learning aided privacy protection, and (iii) machine\nlearning-based privacy attack and corresponding protection schemes. The current\nresearch progress in each category is reviewed and the key challenges are\nidentified. Finally, based on our in-depth analysis of the area of privacy and\nmachine learning, we point out future research directions in this field.", "authors": ["Bo Liu", "Ming Ding", "Sina Shaham", "Wenny Rahayu", "Farhad Farokhi", "Zihuai Lin"], "published": "2020-11-24T00:52:49+00:00", "updated": "2020-11-24T00:52:49+00:00", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf_url": "http://arxiv.org/pdf/2011.11819v1", "entry_id": "http://arxiv.org/abs/2011.11819v1"}, {"id": "1603.02185", "filepath": "papers/1603.02185.pdf", "title": "Distributed Multi-Task Learning with Shared Representation", "abstract": "We study the problem of distributed multi-task learning with shared\nrepresentation, where each machine aims to learn a separate, but related, task\nin an unknown shared low-dimensional subspaces, i.e. when the predictor matrix\nhas low rank. We consider a setting where each task is handled by a different\nmachine, with samples for the task available locally on the machine, and study\ncommunication-efficient methods for exploiting the shared structure.", "authors": ["Jialei Wang", "Mladen Kolar", "Nathan Srebro"], "published": "2016-03-07T18:11:54+00:00", "updated": "2016-03-07T18:11:54+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1603.02185v1", "entry_id": "http://arxiv.org/abs/1603.02185v1"}, {"id": "2004.05366", "filepath": "papers/2004.05366.pdf", "title": "In-Machine-Learning Database: Reimagining Deep Learning with Old-School SQL", "abstract": "In-database machine learning has been very popular, almost being a cliche.\nHowever, can we do it the other way around? In this work, we say \"yes\" by\napplying plain old SQL to deep learning, in a sense implementing deep learning\nalgorithms with SQL. Most deep learning frameworks, as well as generic machine\nlearning ones, share a de facto standard of multidimensional array operations,\nunderneath fancier infrastructure such as automatic differentiation. As SQL\ntables can be regarded as generalisations of (multi-dimensional) arrays, we\nhave found a way to express common deep learning operations in SQL, encouraging\na different way of thinking and thus potentially novel models. In particular,\none of the latest trend in deep learning was the introduction of sparsity in\nthe name of graph convolutional networks, whereas we take sparsity almost for\ngranted in the database world. As both databases and machine learning involve\ntransformation of datasets, we hope this work can inspire further works\nutilizing the large body of existing wisdom, algorithms and technologies in the\ndatabase field to advance the state of the art in machine learning, rather than\nmerely integerating machine learning into databases.", "authors": ["Len Du"], "published": "2020-04-11T11:00:26+00:00", "updated": "2020-04-14T18:08:28+00:00", "categories": ["cs.LG", "cs.DB", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2004.05366v2", "entry_id": "http://arxiv.org/abs/2004.05366v2"}, {"id": "1906.06821", "filepath": "papers/1906.06821.pdf", "title": "A Survey of Optimization Methods from a Machine Learning Perspective", "abstract": "Machine learning develops rapidly, which has made many theoretical\nbreakthroughs and is widely applied in various fields. Optimization, as an\nimportant part of machine learning, has attracted much attention of\nresearchers. With the exponential growth of data amount and the increase of\nmodel complexity, optimization methods in machine learning face more and more\nchallenges. A lot of work on solving optimization problems or improving\noptimization methods in machine learning has been proposed successively. The\nsystematic retrospect and summary of the optimization methods from the\nperspective of machine learning are of great significance, which can offer\nguidance for both developments of optimization and machine learning research.\nIn this paper, we first describe the optimization problems in machine learning.\nThen, we introduce the principles and progresses of commonly used optimization\nmethods. Next, we summarize the applications and developments of optimization\nmethods in some popular machine learning fields. Finally, we explore and give\nsome challenges and open problems for the optimization in machine learning.", "authors": ["Shiliang Sun", "Zehui Cao", "Han Zhu", "Jing Zhao"], "published": "2019-06-17T02:54:51+00:00", "updated": "2019-10-23T08:26:31+00:00", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1906.06821v2", "entry_id": "http://arxiv.org/abs/1906.06821v2"}, {"id": "2004.00993", "filepath": "papers/2004.00993.pdf", "title": "Augmented Q Imitation Learning (AQIL)", "abstract": "The study of unsupervised learning can be generally divided into two\ncategories: imitation learning and reinforcement learning. In imitation\nlearning the machine learns by mimicking the behavior of an expert system\nwhereas in reinforcement learning the machine learns via direct environment\nfeedback. Traditional deep reinforcement learning takes a significant time\nbefore the machine starts to converge to an optimal policy. This paper proposes\nAugmented Q-Imitation-Learning, a method by which deep reinforcement learning\nconvergence can be accelerated by applying Q-imitation-learning as the initial\ntraining process in traditional Deep Q-learning.", "authors": ["Xiao Lei Zhang", "Anish Agarwal"], "published": "2020-03-31T18:08:23+00:00", "updated": "2020-04-05T17:16:23+00:00", "categories": ["cs.LG", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2004.00993v2", "entry_id": "http://arxiv.org/abs/2004.00993v2"}, {"id": "2003.05155", "filepath": "papers/2003.05155.pdf", "title": "Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology", "abstract": "Machine learning is an established and frequently used technique in industry\nand academia but a standard process model to improve success and efficiency of\nmachine learning applications is still missing. Project organizations and\nmachine learning practitioners have a need for guidance throughout the life\ncycle of a machine learning application to meet business expectations. We\ntherefore propose a process model for the development of machine learning\napplications, that covers six phases from defining the scope to maintaining the\ndeployed machine learning application. The first phase combines business and\ndata understanding as data availability oftentimes affects the feasibility of\nthe project. The sixth phase covers state-of-the-art approaches for monitoring\nand maintenance of a machine learning applications, as the risk of model\ndegradation in a changing environment is eminent. With each task of the\nprocess, we propose quality assurance methodology that is suitable to adress\nchallenges in machine learning development that we identify in form of risks.\nThe methodology is drawn from practical experience and scientific literature\nand has proven to be general and stable. The process model expands on CRISP-DM,\na data mining process model that enjoys strong industry support but lacks to\naddress machine learning specific tasks. Our work proposes an industry and\napplication neutral process model tailored for machine learning applications\nwith focus on technical tasks for quality assurance.", "authors": ["Stefan Studer", "Thanh Binh Bui", "Christian Drescher", "Alexander Hanuschkin", "Ludwig Winkler", "Steven Peters", "Klaus-Robert Mueller"], "published": "2020-03-11T08:25:49+00:00", "updated": "2021-02-24T14:33:24+00:00", "categories": ["cs.LG", "cs.SE", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2003.05155v2", "entry_id": "http://arxiv.org/abs/2003.05155v2"}, {"id": "2001.04942", "filepath": "papers/2001.04942.pdf", "title": "Private Machine Learning via Randomised Response", "abstract": "We introduce a general learning framework for private machine learning based\non randomised response. Our assumption is that all actors are potentially\nadversarial and as such we trust only to release a single noisy version of an\nindividual's datapoint. We discuss a general approach that forms a consistent\nway to estimate the true underlying machine learning model and demonstrate this\nin the case of logistic regression.", "authors": ["David Barber"], "published": "2020-01-14T17:56:16+00:00", "updated": "2020-02-24T19:13:28+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2001.04942v2", "entry_id": "http://arxiv.org/abs/2001.04942v2"}, {"id": "1807.06722", "filepath": "papers/1807.06722.pdf", "title": "Machine Learning Interpretability: A Science rather than a tool", "abstract": "The term \"interpretability\" is oftenly used by machine learning researchers\neach with their own intuitive understanding of it. There is no universal well\nagreed upon definition of interpretability in machine learning. As any type of\nscience discipline is mainly driven by the set of formulated questions rather\nthan by different tools in that discipline, e.g. astrophysics is the discipline\nthat learns the composition of stars, not as the discipline that use the\nspectroscopes. Similarly, we propose that machine learning interpretability\nshould be a discipline that answers specific questions related to\ninterpretability. These questions can be of statistical, causal and\ncounterfactual nature. Therefore, there is a need to look into the\ninterpretability problem of machine learning in the context of questions that\nneed to be addressed rather than different tools. We discuss about a\nhypothetical interpretability framework driven by a question based scientific\napproach rather than some specific machine learning model. Using a question\nbased notion of interpretability, we can step towards understanding the science\nof machine learning rather than its engineering. This notion will also help us\nunderstanding any specific problem more in depth rather than relying solely on\nmachine learning methods.", "authors": ["Abdul Karim", "Avinash Mishra", "MA Hakim Newton", "Abdul Sattar"], "published": "2018-07-18T00:50:18+00:00", "updated": "2018-07-25T07:23:45+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1807.06722v2", "entry_id": "http://arxiv.org/abs/1807.06722v2"}, {"id": "1909.09248", "filepath": "papers/1909.09248.pdf", "title": "Representation Learning for Electronic Health Records", "abstract": "Information in electronic health records (EHR), such as clinical narratives,\nexamination reports, lab measurements, demographics, and other patient\nencounter entries, can be transformed into appropriate data representations\nthat can be used for downstream clinical machine learning tasks using\nrepresentation learning. Learning better representations is critical to improve\nthe performance of downstream tasks. Due to the advances in machine learning,\nwe now can learn better and meaningful representations from EHR through\ndisentangling the underlying factors inside data and distilling large amounts\nof information and knowledge from heterogeneous EHR sources. In this chapter,\nwe first introduce the background of learning representations and reasons why\nwe need good EHR representations in machine learning for medicine and\nhealthcare in Section 1. Next, we explain the commonly-used machine learning\nand evaluation methods for representation learning using a deep learning\napproach in Section 2. Following that, we review recent related studies of\nlearning patient state representation from EHR for clinical machine learning\ntasks in Section 3. Finally, in Section 4 we discuss more techniques, studies,\nand challenges for learning natural language representations when free texts,\nsuch as clinical notes, examination reports, or biomedical literature are used.\nWe also discuss challenges and opportunities in these rapidly growing research\nfields.", "authors": ["Wei-Hung Weng", "Peter Szolovits"], "published": "2019-09-19T22:12:30+00:00", "updated": "2019-09-19T22:12:30+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1909.09248v1", "entry_id": "http://arxiv.org/abs/1909.09248v1"}, {"id": "1810.03548", "filepath": "papers/1810.03548.pdf", "title": "Meta-Learning: A Survey", "abstract": "Meta-learning, or learning to learn, is the science of systematically\nobserving how different machine learning approaches perform on a wide range of\nlearning tasks, and then learning from this experience, or meta-data, to learn\nnew tasks much faster than otherwise possible. Not only does this dramatically\nspeed up and improve the design of machine learning pipelines or neural\narchitectures, it also allows us to replace hand-engineered algorithms with\nnovel approaches learned in a data-driven way. In this chapter, we provide an\noverview of the state of the art in this fascinating and continuously evolving\nfield.", "authors": ["Joaquin Vanschoren"], "published": "2018-10-08T16:07:11+00:00", "updated": "2018-10-08T16:07:11+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1810.03548v1", "entry_id": "http://arxiv.org/abs/1810.03548v1"}, {"id": "1507.02188", "filepath": "papers/1507.02188.pdf", "title": "AutoCompete: A Framework for Machine Learning Competition", "abstract": "In this paper, we propose AutoCompete, a highly automated machine learning\nframework for tackling machine learning competitions. This framework has been\nlearned by us, validated and improved over a period of more than two years by\nparticipating in online machine learning competitions. It aims at minimizing\nhuman interference required to build a first useful predictive model and to\nassess the practical difficulty of a given machine learning challenge. The\nproposed system helps in identifying data types, choosing a machine learn- ing\nmodel, tuning hyper-parameters, avoiding over-fitting and optimization for a\nprovided evaluation metric. We also observe that the proposed system produces\nbetter (or comparable) results with less runtime as compared to other\napproaches.", "authors": ["Abhishek Thakur", "Artus Krohn-Grimberghe"], "published": "2015-07-08T15:07:39+00:00", "updated": "2015-07-08T15:07:39+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1507.02188v1", "entry_id": "http://arxiv.org/abs/1507.02188v1"}, {"id": "1707.04849", "filepath": "papers/1707.04849.pdf", "title": "Minimax deviation strategies for machine learning and recognition with short learning samples", "abstract": "The article is devoted to the problem of small learning samples in machine\nlearning. The flaws of maximum likelihood learning and minimax learning are\nlooked into and the concept of minimax deviation learning is introduced that is\nfree of those flaws.", "authors": ["Michail Schlesinger", "Evgeniy Vodolazskiy"], "published": "2017-07-16T09:15:08+00:00", "updated": "2017-07-16T09:15:08+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/1707.04849v1", "entry_id": "http://arxiv.org/abs/1707.04849v1"}, {"id": "1909.01866", "filepath": "papers/1909.01866.pdf", "title": "Understanding Bias in Machine Learning", "abstract": "Bias is known to be an impediment to fair decisions in many domains such as\nhuman resources, the public sector, health care etc. Recently, hope has been\nexpressed that the use of machine learning methods for taking such decisions\nwould diminish or even resolve the problem. At the same time, machine learning\nexperts warn that machine learning models can be biased as well. In this\narticle, our goal is to explain the issue of bias in machine learning from a\ntechnical perspective and to illustrate the impact that biased data can have on\na machine learning model. To reach such a goal, we develop interactive plots to\nvisualizing the bias learned from synthetic data.", "authors": ["Jindong Gu", "Daniela Oelke"], "published": "2019-09-02T20:36:19+00:00", "updated": "2019-09-02T20:36:19+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1909.01866v1", "entry_id": "http://arxiv.org/abs/1909.01866v1"}, {"id": "1212.2686", "filepath": "papers/1212.2686.pdf", "title": "Joint Training of Deep Boltzmann Machines", "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior\nmethods require an initial learning pass that trains the deep Boltzmann machine\ngreedily, one layer at a time, or do not perform well on classifi- cation\ntasks.", "authors": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "published": "2012-12-12T01:59:27+00:00", "updated": "2012-12-12T01:59:27+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1212.2686v1", "entry_id": "http://arxiv.org/abs/1212.2686v1"}, {"id": "2204.07492", "filepath": "papers/2204.07492.pdf", "title": "A Machine Learning Tutorial for Operational Meteorology, Part I: Traditional Machine Learning", "abstract": "Recently, the use of machine learning in meteorology has increased greatly.\nWhile many machine learning methods are not new, university classes on machine\nlearning are largely unavailable to meteorology students and are not required\nto become a meteorologist. The lack of formal instruction has contributed to\nperception that machine learning methods are 'black boxes' and thus end-users\nare hesitant to apply the machine learning methods in their every day workflow.\nTo reduce the opaqueness of machine learning methods and lower hesitancy\ntowards machine learning in meteorology, this paper provides a survey of some\nof the most common machine learning methods. A familiar meteorological example\nis used to contextualize the machine learning methods while also discussing\nmachine learning topics using plain language. The following machine learning\nmethods are demonstrated: linear regression; logistic regression; decision\ntrees; random forest; gradient boosted decision trees; naive Bayes; and support\nvector machines. Beyond discussing the different methods, the paper also\ncontains discussions on the general machine learning process as well as best\npractices to enable readers to apply machine learning to their own datasets.\nFurthermore, all code (in the form of Jupyter notebooks and Google Colaboratory\nnotebooks) used to make the examples in the paper is provided in an effort to\ncatalyse the use of machine learning in meteorology.", "authors": ["Randy J. Chase", "David R. Harrison", "Amanda Burke", "Gary M. Lackmann", "Amy McGovern"], "published": "2022-04-15T14:48:04+00:00", "updated": "2022-06-07T14:48:09+00:00", "categories": ["physics.ao-ph", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2204.07492v2", "entry_id": "http://arxiv.org/abs/2204.07492v2"}, {"id": "1711.06552", "filepath": "papers/1711.06552.pdf", "title": "Introduction to intelligent computing unit 1", "abstract": "This brief note highlights some basic concepts required toward understanding\nthe evolution of machine learning and deep learning models. The note starts\nwith an overview of artificial intelligence and its relationship to biological\nneuron that ultimately led to the evolution of todays intelligent models.", "authors": ["Isa Inuwa-Dutse"], "published": "2017-11-15T16:52:48+00:00", "updated": "2017-11-15T16:52:48+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1711.06552v1", "entry_id": "http://arxiv.org/abs/1711.06552v1"}, {"id": "1602.00198", "filepath": "papers/1602.00198.pdf", "title": "Discussion on Mechanical Learning and Learning Machine", "abstract": "Mechanical learning is a computing system that is based on a set of simple\nand fixed rules, and can learn from incoming data. A learning machine is a\nsystem that realizes mechanical learning. Importantly, we emphasis that it is\nbased on a set of simple and fixed rules, contrasting to often called machine\nlearning that is sophisticated software based on very complicated mathematical\ntheory, and often needs human intervene for software fine tune and manual\nadjustments. Here, we discuss some basic facts and principles of such system,\nand try to lay down a framework for further study. We propose 2 directions to\napproach mechanical learning, just like Church-Turing pair: one is trying to\nrealize a learning machine, another is trying to well describe the mechanical\nlearning.", "authors": ["Chuyu Xiong"], "published": "2016-01-31T04:05:50+00:00", "updated": "2016-01-31T04:05:50+00:00", "categories": ["cs.AI"], "pdf_url": "http://arxiv.org/pdf/1602.00198v1", "entry_id": "http://arxiv.org/abs/1602.00198v1"}, {"id": "2106.07032", "filepath": "papers/2106.07032.pdf", "title": "Category Theory in Machine Learning", "abstract": "Over the past two decades machine learning has permeated almost every realm\nof technology. At the same time, many researchers have begun using category\ntheory as a unifying language, facilitating communication between different\nscientific disciplines. It is therefore unsurprising that there is a burgeoning\ninterest in applying category theory to machine learning. We aim to document\nthe motivations, goals and common themes across these applications. We touch on\ngradient-based learning, probability, and equivariant learning.", "authors": ["Dan Shiebler", "Bruno Gavranovi\u0107", "Paul Wilson"], "published": "2021-06-13T15:58:13+00:00", "updated": "2021-06-13T15:58:13+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/2106.07032v1", "entry_id": "http://arxiv.org/abs/2106.07032v1"}, {"id": "2007.01503", "filepath": "papers/2007.01503.pdf", "title": "Mathematical Perspective of Machine Learning", "abstract": "We take a closer look at some theoretical challenges of Machine Learning as a\nfunction approximation, gradient descent as the default optimization algorithm,\nlimitations of fixed length and width networks and a different approach to RNNs\nfrom a mathematical perspective.", "authors": ["Yarema Boryshchak"], "published": "2020-07-03T05:26:02+00:00", "updated": "2020-07-03T05:26:02+00:00", "categories": ["cs.LG", "stat.ML", "68T07"], "pdf_url": "http://arxiv.org/pdf/2007.01503v1", "entry_id": "http://arxiv.org/abs/2007.01503v1"}, {"id": "1510.00633", "filepath": "papers/1510.00633.pdf", "title": "Distributed Multitask Learning", "abstract": "We consider the problem of distributed multi-task learning, where each\nmachine learns a separate, but related, task. Specifically, each machine learns\na linear predictor in high-dimensional space,where all tasks share the same\nsmall support. We present a communication-efficient estimator based on the\ndebiased lasso and show that it is comparable with the optimal centralized\nmethod.", "authors": ["Jialei Wang", "Mladen Kolar", "Nathan Srebro"], "published": "2015-10-02T16:15:30+00:00", "updated": "2015-10-02T16:15:30+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1510.00633v1", "entry_id": "http://arxiv.org/abs/1510.00633v1"}, {"id": "1501.04309", "filepath": "papers/1501.04309.pdf", "title": "Information Theory and its Relation to Machine Learning", "abstract": "In this position paper, I first describe a new perspective on machine\nlearning (ML) by four basic problems (or levels), namely, \"What to learn?\",\n\"How to learn?\", \"What to evaluate?\", and \"What to adjust?\". The paper stresses\nmore on the first level of \"What to learn?\", or \"Learning Target Selection\".\nTowards this primary problem within the four levels, I briefly review the\nexisting studies about the connection between information theoretical learning\n(ITL [1]) and machine learning. A theorem is given on the relation between the\nempirically-defined similarity measure and information measures. Finally, a\nconjecture is proposed for pursuing a unified mathematical interpretation to\nlearning target selection.", "authors": ["Bao-Gang Hu"], "published": "2015-01-18T14:57:02+00:00", "updated": "2015-01-18T14:57:02+00:00", "categories": ["cs.IT", "cs.LG", "math.IT"], "pdf_url": "http://arxiv.org/pdf/1501.04309v1", "entry_id": "http://arxiv.org/abs/1501.04309v1"}, {"id": "2407.19890", "filepath": "papers/2407.19890.pdf", "title": "Quantum Dynamics of Machine Learning", "abstract": "The quantum dynamic equation (QDE) of machine learning is obtained based on\nSchr\\\"odinger equation and potential energy equivalence relationship. Through\nWick rotation, the relationship between quantum dynamics and thermodynamics is\nalso established in this paper. This equation reformulates the iterative\nprocess of machine learning into a time-dependent partial differential equation\nwith a clear mathematical structure, offering a theoretical framework for\ninvestigating machine learning iterations through quantum and mathematical\ntheories. Within this framework, the fundamental iterative process, the\ndiffusion model, and the Softmax and Sigmoid functions are examined, validating\nthe proposed quantum dynamics equations. This approach not only presents a\nrigorous theoretical foundation for machine learning but also holds promise for\nsupporting the implementation of machine learning algorithms on quantum\ncomputers.", "authors": ["Peng Wang", "Maimaitiniyazi Maimaitiabudula"], "published": "2024-07-07T16:30:46+00:00", "updated": "2024-07-07T16:30:46+00:00", "categories": ["quant-ph", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2407.19890v1", "entry_id": "http://arxiv.org/abs/2407.19890v1"}, {"id": "1802.03830", "filepath": "papers/1802.03830.pdf", "title": "Distributed Stochastic Multi-Task Learning with Graph Regularization", "abstract": "We propose methods for distributed graph-based multi-task learning that are\nbased on weighted averaging of messages from other machines. Uniform averaging\nor diminishing stepsize in these methods would yield consensus (single task)\nlearning. We show how simply skewing the averaging weights or controlling the\nstepsize allows learning different, but related, tasks on the different\nmachines.", "authors": ["Weiran Wang", "Jialei Wang", "Mladen Kolar", "Nathan Srebro"], "published": "2018-02-11T22:23:34+00:00", "updated": "2018-02-11T22:23:34+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1802.03830v1", "entry_id": "http://arxiv.org/abs/1802.03830v1"}, {"id": "2306.14624", "filepath": "papers/2306.14624.pdf", "title": "Insights From Insurance for Fair Machine Learning", "abstract": "We argue that insurance can act as an analogon for the social situatedness of\nmachine learning systems, hence allowing machine learning scholars to take\ninsights from the rich and interdisciplinary insurance literature. Tracing the\ninteraction of uncertainty, fairness and responsibility in insurance provides a\nfresh perspective on fairness in machine learning. We link insurance fairness\nconceptions to their machine learning relatives, and use this bridge to\nproblematize fairness as calibration. In this process, we bring to the\nforefront two themes that have been largely overlooked in the machine learning\nliterature: responsibility and aggregate-individual tensions.", "authors": ["Christian Fr\u00f6hlich", "Robert C. Williamson"], "published": "2023-06-26T11:56:00+00:00", "updated": "2024-01-23T09:43:23+00:00", "categories": ["cs.LG", "cs.CY"], "pdf_url": "http://arxiv.org/pdf/2306.14624v2", "entry_id": "http://arxiv.org/abs/2306.14624v2"}, {"id": "2310.11470", "filepath": "papers/2310.11470.pdf", "title": "Classic machine learning methods", "abstract": "In this chapter, we present the main classic machine learning methods. A\nlarge part of the chapter is devoted to supervised learning techniques for\nclassification and regression, including nearest-neighbor methods, linear and\nlogistic regressions, support vector machines and tree-based algorithms. We\nalso describe the problem of overfitting as well as strategies to overcome it.\nWe finally provide a brief overview of unsupervised learning methods, namely\nfor clustering and dimensionality reduction.", "authors": ["Johann Faouzi", "Olivier Colliot"], "published": "2023-05-24T13:38:38+00:00", "updated": "2023-05-24T13:38:38+00:00", "categories": ["cs.LG", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2310.11470v1", "entry_id": "http://arxiv.org/abs/2310.11470v1"}, {"id": "2104.05314", "filepath": "papers/2104.05314.pdf", "title": "Machine learning and deep learning", "abstract": "Today, intelligent systems that offer artificial intelligence capabilities\noften rely on machine learning. Machine learning describes the capacity of\nsystems to learn from problem-specific training data to automate the process of\nanalytical model building and solve associated tasks. Deep learning is a\nmachine learning concept based on artificial neural networks. For many\napplications, deep learning models outperform shallow machine learning models\nand traditional data analysis approaches. In this article, we summarize the\nfundamentals of machine learning and deep learning to generate a broader\nunderstanding of the methodical underpinning of current intelligent systems. In\nparticular, we provide a conceptual distinction between relevant terms and\nconcepts, explain the process of automated analytical model building through\nmachine learning and deep learning, and discuss the challenges that arise when\nimplementing such intelligent systems in the field of electronic markets and\nnetworked business. These naturally go beyond technological aspects and\nhighlight issues in human-machine interaction and artificial intelligence\nservitization.", "authors": ["Christian Janiesch", "Patrick Zschech", "Kai Heinrich"], "published": "2021-04-12T09:54:12+00:00", "updated": "2021-04-14T10:31:01+00:00", "categories": ["cs.AI"], "pdf_url": "http://arxiv.org/pdf/2104.05314v2", "entry_id": "http://arxiv.org/abs/2104.05314v2"}, {"id": "2206.13446", "filepath": "papers/2206.13446.pdf", "title": "Pen and Paper Exercises in Machine Learning", "abstract": "This is a collection of (mostly) pen-and-paper exercises in machine learning.\nThe exercises are on the following topics: linear algebra, optimisation,\ndirected graphical models, undirected graphical models, expressive power of\ngraphical models, factor graphs and message passing, inference for hidden\nMarkov models, model-based learning (including ICA and unnormalised models),\nsampling and Monte-Carlo integration, and variational inference.", "authors": ["Michael U. Gutmann"], "published": "2022-06-27T16:53:18+00:00", "updated": "2022-06-27T16:53:18+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2206.13446v1", "entry_id": "http://arxiv.org/abs/2206.13446v1"}, {"id": "2312.03120", "filepath": "papers/2312.03120.pdf", "title": "The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning", "abstract": "With the advance of the powerful heterogeneous, parallel and distributed\ncomputing systems and ever increasing immense amount of data, machine learning\nhas become an indispensable part of cutting-edge technology, scientific\nresearch and consumer products. In this study, we present a review of modern\nmachine and deep learning. We provide a high-level overview for the latest\nadvanced machine learning algorithms, applications, and frameworks. Our\ndiscussion encompasses parallel distributed learning, deep learning as well as\nfederated learning. As a result, our work serves as an introductory text to the\nvast field of modern machine learning.", "authors": ["Omer Subasi", "Oceane Bel", "Joseph Manzano", "Kevin Barker"], "published": "2023-12-05T20:40:05+00:00", "updated": "2023-12-05T20:40:05+00:00", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf_url": "http://arxiv.org/pdf/2312.03120v1", "entry_id": "http://arxiv.org/abs/2312.03120v1"}, {"id": "2103.03122", "filepath": "papers/2103.03122.pdf", "title": "Machine Learning using Stata/Python", "abstract": "We present two related Stata modules, r_ml_stata and c_ml_stata, for fitting\npopular Machine Learning (ML) methods both in regression and classification\nsettings. Using the recent Stata/Python integration platform (sfi) of Stata 16,\nthese commands provide hyper-parameters' optimal tuning via K-fold\ncross-validation using greed search. More specifically, they make use of the\nPython Scikit-learn API to carry out both cross-validation and outcome/label\nprediction.", "authors": ["Giovanni Cerulli"], "published": "2021-03-03T10:31:44+00:00", "updated": "2021-03-03T10:31:44+00:00", "categories": ["stat.CO", "cs.LG", "cs.MS"], "pdf_url": "http://arxiv.org/pdf/2103.03122v1", "entry_id": "http://arxiv.org/abs/2103.03122v1"}, {"id": "1707.09562", "filepath": "papers/1707.09562.pdf", "title": "MLBench: How Good Are Machine Learning Clouds for Binary Classification Tasks on Structured Data?", "abstract": "We conduct an empirical study of machine learning functionalities provided by\nmajor cloud service providers, which we call machine learning clouds. Machine\nlearning clouds hold the promise of hiding all the sophistication of running\nlarge-scale machine learning: Instead of specifying how to run a machine\nlearning task, users only specify what machine learning task to run and the\ncloud figures out the rest. Raising the level of abstraction, however, rarely\ncomes free - a performance penalty is possible. How good, then, are current\nmachine learning clouds on real-world machine learning workloads?\n  We study this question with a focus on binary classication problems. We\npresent mlbench, a novel benchmark constructed by harvesting datasets from\nKaggle competitions. We then compare the performance of the top winning code\navailable from Kaggle with that of running machine learning clouds from both\nAzure and Amazon on mlbench. Our comparative study reveals the strength and\nweakness of existing machine learning clouds and points out potential future\ndirections for improvement.", "authors": ["Yu Liu", "Hantian Zhang", "Luyuan Zeng", "Wentao Wu", "Ce Zhang"], "published": "2017-07-29T21:59:18+00:00", "updated": "2017-10-16T11:13:32+00:00", "categories": ["cs.DC", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1707.09562v3", "entry_id": "http://arxiv.org/abs/1707.09562v3"}, {"id": "2405.03720", "filepath": "papers/2405.03720.pdf", "title": "Spatial Transfer Learning with Simple MLP", "abstract": "First step to investigate the potential of transfer learning applied to the\nfield of spatial statistics", "authors": ["Hongjian Yang"], "published": "2024-05-05T20:39:15+00:00", "updated": "2024-05-05T20:39:15+00:00", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2405.03720v1", "entry_id": "http://arxiv.org/abs/2405.03720v1"}, {"id": "2305.15410", "filepath": "papers/2305.15410.pdf", "title": "Machine learning-assisted close-set X-ray diffraction phase identification of transition metals", "abstract": "Machine learning has been applied to the problem of X-ray diffraction phase\nprediction with promising results. In this paper, we describe a method for\nusing machine learning to predict crystal structure phases from X-ray\ndiffraction data of transition metals and their oxides. We evaluate the\nperformance of our method and compare the variety of its settings. Our results\ndemonstrate that the proposed machine learning framework achieves competitive\nperformance. This demonstrates the potential for machine learning to\nsignificantly impact the field of X-ray diffraction and crystal structure\ndetermination. Open-source implementation:\nhttps://github.com/maxnygma/NeuralXRD.", "authors": ["Maksim Zhdanov", "Andrey Zhdanov"], "published": "2023-04-28T09:29:10+00:00", "updated": "2023-04-28T09:29:10+00:00", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2305.15410v1", "entry_id": "http://arxiv.org/abs/2305.15410v1"}, {"id": "1907.03010", "filepath": "papers/1907.03010.pdf", "title": "Financial Time Series Data Processing for Machine Learning", "abstract": "This article studies the financial time series data processing for machine\nlearning. It introduces the most frequent scaling methods, then compares the\nresulting stationarity and preservation of useful information for trend\nforecasting. It proposes an empirical test based on the capability to learn\nsimple data relationship with simple models. It also speaks about the data\nsplit method specific to time series, avoiding unwanted overfitting and\nproposes various labelling for classification and regression.", "authors": ["Fabrice Daniel"], "published": "2019-07-03T15:10:23+00:00", "updated": "2019-07-03T15:10:23+00:00", "categories": ["q-fin.ST", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1907.03010v1", "entry_id": "http://arxiv.org/abs/1907.03010v1"}, {"id": "1911.00776", "filepath": "papers/1911.00776.pdf", "title": "Ten-year Survival Prediction for Breast Cancer Patients", "abstract": "This report assesses different machine learning approaches to 10-year\nsurvival prediction of breast cancer patients.", "authors": ["Changmao Li", "Han He", "Yunze Hao", "Caleb Ziems"], "published": "2019-11-02T19:53:32+00:00", "updated": "2019-11-02T19:53:32+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1911.00776v1", "entry_id": "http://arxiv.org/abs/1911.00776v1"}, {"id": "1902.04622", "filepath": "papers/1902.04622.pdf", "title": "Learning Theory and Support Vector Machines - a primer", "abstract": "The main goal of statistical learning theory is to provide a fundamental\nframework for the problem of decision making and model construction based on\nsets of data. Here, we present a brief introduction to the fundamentals of\nstatistical learning theory, in particular the difference between empirical and\nstructural risk minimization, including one of its most prominent\nimplementations, i.e. the Support Vector Machine.", "authors": ["Michael Banf"], "published": "2019-02-12T20:28:09+00:00", "updated": "2019-02-12T20:28:09+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1902.04622v1", "entry_id": "http://arxiv.org/abs/1902.04622v1"}, {"id": "1811.04422", "filepath": "papers/1811.04422.pdf", "title": "An Optimal Control View of Adversarial Machine Learning", "abstract": "I describe an optimal control view of adversarial machine learning, where the\ndynamical system is the machine learner, the input are adversarial actions, and\nthe control costs are defined by the adversary's goals to do harm and be hard\nto detect. This view encompasses many types of adversarial machine learning,\nincluding test-item attacks, training-data poisoning, and adversarial reward\nshaping. The view encourages adversarial machine learning researcher to utilize\nadvances in control theory and reinforcement learning.", "authors": ["Xiaojin Zhu"], "published": "2018-11-11T14:28:34+00:00", "updated": "2018-11-11T14:28:34+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1811.04422v1", "entry_id": "http://arxiv.org/abs/1811.04422v1"}, {"id": "2502.01708", "filepath": "papers/2502.01708.pdf", "title": "Aspects of Artificial Intelligence: Transforming Machine Learning Systems Naturally", "abstract": "In this paper, we study the machine learning elements which we are interested\nin together as a machine learning system, consisting of a collection of machine\nlearning elements and a collection of relations between the elements. The\nrelations we concern are algebraic operations, binary relations, and binary\nrelations with composition that can be reasoned categorically. A machine\nlearning system transformation between two systems is a map between the\nsystems, which preserves the relations we concern. The system transformations\ngiven by quotient or clustering, representable functor, and Yoneda embedding\nare highlighted and discussed by machine learning examples. An adjunction\nbetween machine learning systems, a special machine learning system\ntransformation loop, provides the optimal way of solving problems. Machine\nlearning system transformations are linked and compared by their maps at\n2-cell, natural transformations. New insights and structures can be obtained\nfrom universal properties and algebraic structures given by monads, which are\ngenerated from adjunctions.", "authors": ["Xiuzhan Guo"], "published": "2025-02-03T14:45:02+00:00", "updated": "2025-02-03T14:45:02+00:00", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.DM"], "pdf_url": "http://arxiv.org/pdf/2502.01708v1", "entry_id": "http://arxiv.org/abs/2502.01708v1"}, {"id": "2008.08080", "filepath": "papers/2008.08080.pdf", "title": "mlr3proba: An R Package for Machine Learning in Survival Analysis", "abstract": "As machine learning has become increasingly popular over the last few\ndecades, so too has the number of machine learning interfaces for implementing\nthese models. Whilst many R libraries exist for machine learning, very few\noffer extended support for survival analysis. This is problematic considering\nits importance in fields like medicine, bioinformatics, economics, engineering,\nand more. mlr3proba provides a comprehensive machine learning interface for\nsurvival analysis and connects with mlr3's general model tuning and\nbenchmarking facilities to provide a systematic infrastructure for survival\nmodeling and evaluation.", "authors": ["Raphael Sonabend", "Franz J. Kir\u00e1ly", "Andreas Bender", "Bernd Bischl", "Michel Lang"], "published": "2020-08-18T11:21:24+00:00", "updated": "2020-12-14T11:41:25+00:00", "categories": ["stat.CO", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2008.08080v2", "entry_id": "http://arxiv.org/abs/2008.08080v2"}, {"id": "2007.01977", "filepath": "papers/2007.01977.pdf", "title": "Lale: Consistent Automated Machine Learning", "abstract": "Automated machine learning makes it easier for data scientists to develop\npipelines by searching over possible choices for hyperparameters, algorithms,\nand even pipeline topologies. Unfortunately, the syntax for automated machine\nlearning tools is inconsistent with manual machine learning, with each other,\nand with error checks. Furthermore, few tools support advanced features such as\ntopology search or higher-order operators. This paper introduces Lale, a\nlibrary of high-level Python interfaces that simplifies and unifies automated\nmachine learning in a consistent way.", "authors": ["Guillaume Baudart", "Martin Hirzel", "Kiran Kate", "Parikshit Ram", "Avraham Shinnar"], "published": "2020-07-04T00:55:41+00:00", "updated": "2020-07-04T00:55:41+00:00", "categories": ["cs.LG", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2007.01977v1", "entry_id": "http://arxiv.org/abs/2007.01977v1"}, {"id": "2409.03632", "filepath": "papers/2409.03632.pdf", "title": "Beyond Model Interpretability: Socio-Structural Explanations in Machine Learning", "abstract": "What is it to interpret the outputs of an opaque machine learning model. One\napproach is to develop interpretable machine learning techniques. These\ntechniques aim to show how machine learning models function by providing either\nmodel centric local or global explanations, which can be based on mechanistic\ninterpretations revealing the inner working mechanisms of models or\nnonmechanistic approximations showing input feature output data relationships.\nIn this paper, we draw on social philosophy to argue that interpreting machine\nlearning outputs in certain normatively salient domains could require appealing\nto a third type of explanation that we call sociostructural explanation. The\nrelevance of this explanation type is motivated by the fact that machine\nlearning models are not isolated entities but are embedded within and shaped by\nsocial structures. Sociostructural explanations aim to illustrate how social\nstructures contribute to and partially explain the outputs of machine learning\nmodels. We demonstrate the importance of sociostructural explanations by\nexamining a racially biased healthcare allocation algorithm. Our proposal\nhighlights the need for transparency beyond model interpretability,\nunderstanding the outputs of machine learning systems could require a broader\nanalysis that extends beyond the understanding of the machine learning model\nitself.", "authors": ["Andrew Smart", "Atoosa Kasirzadeh"], "published": "2024-09-05T15:47:04+00:00", "updated": "2024-09-05T15:47:04+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/2409.03632v1", "entry_id": "http://arxiv.org/abs/2409.03632v1"}, {"id": "2007.07981", "filepath": "papers/2007.07981.pdf", "title": "Differential Replication in Machine Learning", "abstract": "When deployed in the wild, machine learning models are usually confronted\nwith data and requirements that constantly vary, either because of changes in\nthe generating distribution or because external constraints change the\nenvironment where the model operates. To survive in such an ecosystem, machine\nlearning models need to adapt to new conditions by evolving over time. The idea\nof model adaptability has been studied from different perspectives. In this\npaper, we propose a solution based on reusing the knowledge acquired by the\nalready deployed machine learning models and leveraging it to train future\ngenerations. This is the idea behind differential replication of machine\nlearning models.", "authors": ["Irene Unceta", "Jordi Nin", "Oriol Pujol"], "published": "2020-07-15T20:26:49+00:00", "updated": "2020-07-15T20:26:49+00:00", "categories": ["cs.LG", "stat.ML", "cs.LG, stat.ML"], "pdf_url": "http://arxiv.org/pdf/2007.07981v1", "entry_id": "http://arxiv.org/abs/2007.07981v1"}, {"id": "1911.08587", "filepath": "papers/1911.08587.pdf", "title": "Solving machine learning optimization problems using quantum computers", "abstract": "Classical optimization algorithms in machine learning often take a long time\nto compute when applied to a multi-dimensional problem and require a huge\namount of CPU and GPU resource. Quantum parallelism has a potential to speed up\nmachine learning algorithms. We describe a generic mathematical model to\nleverage quantum parallelism to speed-up machine learning algorithms. We also\napply quantum machine learning and quantum parallelism applied to a\n$3$-dimensional image that vary with time.", "authors": ["Venkat R. Dasari", "Mee Seong Im", "Lubjana Beshaj"], "published": "2019-11-17T17:36:41+00:00", "updated": "2019-11-17T17:36:41+00:00", "categories": ["quant-ph", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1911.08587v1", "entry_id": "http://arxiv.org/abs/1911.08587v1"}, {"id": "2312.14050", "filepath": "papers/2312.14050.pdf", "title": "Machine learning and domain decomposition methods -- a survey", "abstract": "Hybrid algorithms, which combine black-box machine learning methods with\nexperience from traditional numerical methods and domain expertise from diverse\napplication areas, are progressively gaining importance in scientific machine\nlearning and various industrial domains, especially in computational science\nand engineering. In the present survey, several promising avenues of research\nwill be examined which focus on the combination of machine learning (ML) and\ndomain decomposition methods (DDMs). The aim of this survey is to provide an\noverview of existing work within this field and to structure it into domain\ndecomposition for machine learning and machine learning-enhanced domain\ndecomposition, including: domain decomposition for classical machine learning,\ndomain decomposition to accelerate the training of physics-aware neural\nnetworks, machine learning to enhance the convergence properties or\ncomputational efficiency of DDMs, and machine learning as a discretization\nmethod in a DDM for the solution of PDEs. In each of these fields, we summarize\nexisting work and key advances within a common framework and, finally, disuss\nongoing challenges and opportunities for future research.", "authors": ["Axel Klawonn", "Martin Lanser", "Janine Weber"], "published": "2023-12-21T17:19:27+00:00", "updated": "2023-12-21T17:19:27+00:00", "categories": ["math.NA", "cs.LG", "cs.NA", "65F10, 65N22, 65N55, 68T05, 68T07"], "pdf_url": "http://arxiv.org/pdf/2312.14050v1", "entry_id": "http://arxiv.org/abs/2312.14050v1"}, {"id": "1911.06612", "filepath": "papers/1911.06612.pdf", "title": "Position Paper: Towards Transparent Machine Learning", "abstract": "Transparent machine learning is introduced as an alternative form of machine\nlearning, where both the model and the learning system are represented in\nsource code form. The goal of this project is to enable direct human\nunderstanding of machine learning models, giving us the ability to learn,\nverify, and refine them as programs. If solved, this technology could represent\na best-case scenario for the safety and security of AI systems going forward.", "authors": ["Dustin Juliano"], "published": "2019-11-12T10:49:55+00:00", "updated": "2019-11-12T10:49:55+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/1911.06612v1", "entry_id": "http://arxiv.org/abs/1911.06612v1"}, {"id": "2001.09608", "filepath": "papers/2001.09608.pdf", "title": "Some Insights into Lifelong Reinforcement Learning Systems", "abstract": "A lifelong reinforcement learning system is a learning system that has the\nability to learn through trail-and-error interaction with the environment over\nits lifetime. In this paper, I give some arguments to show that the traditional\nreinforcement learning paradigm fails to model this type of learning system.\nSome insights into lifelong reinforcement learning are provided, along with a\nsimplistic prototype lifelong reinforcement learning system.", "authors": ["Changjian Li"], "published": "2020-01-27T07:26:12+00:00", "updated": "2020-01-27T07:26:12+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2001.09608v1", "entry_id": "http://arxiv.org/abs/2001.09608v1"}, {"id": "2209.02057", "filepath": "papers/2209.02057.pdf", "title": "Applying Machine Learning to Life Insurance: some knowledge sharing to master it", "abstract": "Machine Learning permeates many industries, which brings new source of\nbenefits for companies. However within the life insurance industry, Machine\nLearning is not widely used in practice as over the past years statistical\nmodels have shown their efficiency for risk assessment. Thus insurers may face\ndifficulties to assess the value of the artificial intelligence. Focusing on\nthe modification of the life insurance industry over time highlights the stake\nof using Machine Learning for insurers and benefits that it can bring by\nunleashing data value. This paper reviews traditional actuarial methodologies\nfor survival modeling and extends them with Machine Learning techniques. It\npoints out differences with regular machine learning models and emphasizes\nimportance of specific implementations to face censored data with machine\nlearning models family. In complement to this article, a Python library has\nbeen developed. Different open-source Machine Learning algorithms have been\nadjusted to adapt the specificities of life insurance data, namely censoring\nand truncation. Such models can be easily applied from this SCOR library to\naccurately model life insurance risks.", "authors": ["Antoine Chancel", "Laura Bradier", "Antoine Ly", "Razvan Ionescu", "Laurene Martin", "Marguerite Sauce"], "published": "2022-09-05T17:09:03+00:00", "updated": "2022-09-27T18:42:50+00:00", "categories": ["stat.ML", "cs.CY", "cs.LG", "stat.AP"], "pdf_url": "http://arxiv.org/pdf/2209.02057v3", "entry_id": "http://arxiv.org/abs/2209.02057v3"}, {"id": "2002.12364", "filepath": "papers/2002.12364.pdf", "title": "Theoretical Models of Learning to Learn", "abstract": "A Machine can only learn if it is biased in some way. Typically the bias is\nsupplied by hand, for example through the choice of an appropriate set of\nfeatures. However, if the learning machine is embedded within an {\\em\nenvironment} of related tasks, then it can {\\em learn} its own bias by learning\nsufficiently many tasks from the environment. In this paper two models of bias\nlearning (or equivalently, learning to learn) are introduced and the main\ntheoretical results presented. The first model is a PAC-type model based on\nempirical process theory, while the second is a hierarchical Bayes model.", "authors": ["Jonathan Baxter"], "published": "2020-02-27T13:35:26+00:00", "updated": "2020-02-27T13:35:26+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2002.12364v1", "entry_id": "http://arxiv.org/abs/2002.12364v1"}, {"id": "2105.03726", "filepath": "papers/2105.03726.pdf", "title": "Mental Models of Adversarial Machine Learning", "abstract": "Although machine learning is widely used in practice, little is known about\npractitioners' understanding of potential security challenges. In this work, we\nclose this substantial gap and contribute a qualitative study focusing on\ndevelopers' mental models of the machine learning pipeline and potentially\nvulnerable components. Similar studies have helped in other security fields to\ndiscover root causes or improve risk communication. Our study reveals two\n\\facets of practitioners' mental models of machine learning security. Firstly,\npractitioners often confuse machine learning security with threats and defences\nthat are not directly related to machine learning. Secondly, in contrast to\nmost academic research, our participants perceive security of machine learning\nas not solely related to individual models, but rather in the context of entire\nworkflows that consist of multiple components. Jointly with our additional\nfindings, these two facets provide a foundation to substantiate mental models\nfor machine learning security and have implications for the integration of\nadversarial machine learning into corporate workflows, \\new{decreasing\npractitioners' reported uncertainty}, and appropriate regulatory frameworks for\nmachine learning security.", "authors": ["Lukas Bieringer", "Kathrin Grosse", "Michael Backes", "Battista Biggio", "Katharina Krombholz"], "published": "2021-05-08T16:05:07+00:00", "updated": "2022-06-29T13:42:12+00:00", "categories": ["cs.CR", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2105.03726v4", "entry_id": "http://arxiv.org/abs/2105.03726v4"}, {"id": "2108.08712", "filepath": "papers/2108.08712.pdf", "title": "Teaching Uncertainty Quantification in Machine Learning through Use Cases", "abstract": "Uncertainty in machine learning is not generally taught as general knowledge\nin Machine Learning course curricula. In this paper we propose a short\ncurriculum for a course about uncertainty in machine learning, and complement\nthe course with a selection of use cases, aimed to trigger discussion and let\nstudents play with the concepts of uncertainty in a programming setting. Our\nuse cases cover the concept of output uncertainty, Bayesian neural networks and\nweight distributions, sources of uncertainty, and out of distribution\ndetection. We expect that this curriculum and set of use cases motivates the\ncommunity to adopt these important concepts into courses for safety in AI.", "authors": ["Matias Valdenegro-Toro"], "published": "2021-08-19T14:22:17+00:00", "updated": "2021-08-19T14:22:17+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2108.08712v1", "entry_id": "http://arxiv.org/abs/2108.08712v1"}, {"id": "2006.15680", "filepath": "papers/2006.15680.pdf", "title": "Modeling Generalization in Machine Learning: A Methodological and Computational Study", "abstract": "As machine learning becomes more and more available to the general public,\ntheoretical questions are turning into pressing practical issues. Possibly, one\nof the most relevant concerns is the assessment of our confidence in trusting\nmachine learning predictions. In many real-world cases, it is of utmost\nimportance to estimate the capabilities of a machine learning algorithm to\ngeneralize, i.e., to provide accurate predictions on unseen data, depending on\nthe characteristics of the target problem. In this work, we perform a\nmeta-analysis of 109 publicly-available classification data sets, modeling\nmachine learning generalization as a function of a variety of data set\ncharacteristics, ranging from number of samples to intrinsic dimensionality,\nfrom class-wise feature skewness to $F1$ evaluated on test samples falling\noutside the convex hull of the training set. Experimental results demonstrate\nthe relevance of using the concept of the convex hull of the training data in\nassessing machine learning generalization, by emphasizing the difference\nbetween interpolated and extrapolated predictions. Besides several predictable\ncorrelations, we observe unexpectedly weak associations between the\ngeneralization ability of machine learning models and all metrics related to\ndimensionality, thus challenging the common assumption that the \\textit{curse\nof dimensionality} might impair generalization in machine learning.", "authors": ["Pietro Barbiero", "Giovanni Squillero", "Alberto Tonda"], "published": "2020-06-28T19:06:16+00:00", "updated": "2020-06-28T19:06:16+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2006.15680v1", "entry_id": "http://arxiv.org/abs/2006.15680v1"}, {"id": "2001.04942", "filepath": "papers/2001.04942.pdf", "title": "Private Machine Learning via Randomised Response", "abstract": "We introduce a general learning framework for private machine learning based\non randomised response. Our assumption is that all actors are potentially\nadversarial and as such we trust only to release a single noisy version of an\nindividual's datapoint. We discuss a general approach that forms a consistent\nway to estimate the true underlying machine learning model and demonstrate this\nin the case of logistic regression.", "authors": ["David Barber"], "published": "2020-01-14T17:56:16+00:00", "updated": "2020-02-24T19:13:28+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2001.04942v2", "entry_id": "http://arxiv.org/abs/2001.04942v2"}, {"id": "1607.02450", "filepath": "papers/1607.02450.pdf", "title": "Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in Social Good Applications", "abstract": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning\nin Social Good Applications, which was held on June 24, 2016 in New York.", "authors": ["Kush R. Varshney"], "published": "2016-07-08T16:55:31+00:00", "updated": "2016-08-28T15:23:47+00:00", "categories": ["stat.ML", "cs.CY", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1607.02450v2", "entry_id": "http://arxiv.org/abs/1607.02450v2"}, {"id": "1909.03550", "filepath": "papers/1909.03550.pdf", "title": "Lecture Notes: Optimization for Machine Learning", "abstract": "Lecture notes on optimization for machine learning, derived from a course at\nPrinceton University and tutorials given in MLSS, Buenos Aires, as well as\nSimons Foundation, Berkeley.", "authors": ["Elad Hazan"], "published": "2019-09-08T21:49:42+00:00", "updated": "2019-09-08T21:49:42+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1909.03550v1", "entry_id": "http://arxiv.org/abs/1909.03550v1"}, {"id": "2407.19890", "filepath": "papers/2407.19890.pdf", "title": "Quantum Dynamics of Machine Learning", "abstract": "The quantum dynamic equation (QDE) of machine learning is obtained based on\nSchr\\\"odinger equation and potential energy equivalence relationship. Through\nWick rotation, the relationship between quantum dynamics and thermodynamics is\nalso established in this paper. This equation reformulates the iterative\nprocess of machine learning into a time-dependent partial differential equation\nwith a clear mathematical structure, offering a theoretical framework for\ninvestigating machine learning iterations through quantum and mathematical\ntheories. Within this framework, the fundamental iterative process, the\ndiffusion model, and the Softmax and Sigmoid functions are examined, validating\nthe proposed quantum dynamics equations. This approach not only presents a\nrigorous theoretical foundation for machine learning but also holds promise for\nsupporting the implementation of machine learning algorithms on quantum\ncomputers.", "authors": ["Peng Wang", "Maimaitiniyazi Maimaitiabudula"], "published": "2024-07-07T16:30:46+00:00", "updated": "2024-07-07T16:30:46+00:00", "categories": ["quant-ph", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2407.19890v1", "entry_id": "http://arxiv.org/abs/2407.19890v1"}, {"id": "1212.2686", "filepath": "papers/1212.2686.pdf", "title": "Joint Training of Deep Boltzmann Machines", "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior\nmethods require an initial learning pass that trains the deep Boltzmann machine\ngreedily, one layer at a time, or do not perform well on classifi- cation\ntasks.", "authors": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "published": "2012-12-12T01:59:27+00:00", "updated": "2012-12-12T01:59:27+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1212.2686v1", "entry_id": "http://arxiv.org/abs/1212.2686v1"}, {"id": "2306.14624", "filepath": "papers/2306.14624.pdf", "title": "Insights From Insurance for Fair Machine Learning", "abstract": "We argue that insurance can act as an analogon for the social situatedness of\nmachine learning systems, hence allowing machine learning scholars to take\ninsights from the rich and interdisciplinary insurance literature. Tracing the\ninteraction of uncertainty, fairness and responsibility in insurance provides a\nfresh perspective on fairness in machine learning. We link insurance fairness\nconceptions to their machine learning relatives, and use this bridge to\nproblematize fairness as calibration. In this process, we bring to the\nforefront two themes that have been largely overlooked in the machine learning\nliterature: responsibility and aggregate-individual tensions.", "authors": ["Christian Fr\u00f6hlich", "Robert C. Williamson"], "published": "2023-06-26T11:56:00+00:00", "updated": "2024-01-23T09:43:23+00:00", "categories": ["cs.LG", "cs.CY"], "pdf_url": "http://arxiv.org/pdf/2306.14624v2", "entry_id": "http://arxiv.org/abs/2306.14624v2"}, {"id": "2108.08712", "filepath": "papers/2108.08712.pdf", "title": "Teaching Uncertainty Quantification in Machine Learning through Use Cases", "abstract": "Uncertainty in machine learning is not generally taught as general knowledge\nin Machine Learning course curricula. In this paper we propose a short\ncurriculum for a course about uncertainty in machine learning, and complement\nthe course with a selection of use cases, aimed to trigger discussion and let\nstudents play with the concepts of uncertainty in a programming setting. Our\nuse cases cover the concept of output uncertainty, Bayesian neural networks and\nweight distributions, sources of uncertainty, and out of distribution\ndetection. We expect that this curriculum and set of use cases motivates the\ncommunity to adopt these important concepts into courses for safety in AI.", "authors": ["Matias Valdenegro-Toro"], "published": "2021-08-19T14:22:17+00:00", "updated": "2021-08-19T14:22:17+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2108.08712v1", "entry_id": "http://arxiv.org/abs/2108.08712v1"}, {"id": "2206.07090", "filepath": "papers/2206.07090.pdf", "title": "Parallelization of Machine Learning Algorithms Respectively on Single Machine and Spark", "abstract": "With the rapid development of big data technologies, how to dig out useful\ninformation from massive data becomes an essential problem. However, using\nmachine learning algorithms to analyze large data may be time-consuming and\ninefficient on the traditional single machine. To solve these problems, this\npaper has made some research on the parallelization of several classic machine\nlearning algorithms respectively on the single machine and the big data\nplatform Spark. We compare the runtime and efficiency of traditional machine\nlearning algorithms with parallelized machine learning algorithms respectively\non the single machine and Spark platform. The research results have shown\nsignificant improvement in runtime and efficiency of parallelized machine\nlearning algorithms.", "authors": ["Jiajun Shen"], "published": "2022-05-08T03:47:30+00:00", "updated": "2023-04-13T03:30:03+00:00", "categories": ["cs.DC"], "pdf_url": "http://arxiv.org/pdf/2206.07090v2", "entry_id": "http://arxiv.org/abs/2206.07090v2"}, {"id": "2212.12303", "filepath": "papers/2212.12303.pdf", "title": "Introduction to Machine Learning for Physicians: A Survival Guide for Data Deluge", "abstract": "Many modern research fields increasingly rely on collecting and analysing\nmassive, often unstructured, and unwieldy datasets. Consequently, there is\ngrowing interest in machine learning and artificial intelligence applications\nthat can harness this `data deluge'. This broad nontechnical overview provides\na gentle introduction to machine learning with a specific focus on medical and\nbiological applications. We explain the common types of machine learning\nalgorithms and typical tasks that can be solved, illustrating the basics with\nconcrete examples from healthcare. Lastly, we provide an outlook on open\nchallenges, limitations, and potential impacts of machine-learning-powered\nmedicine.", "authors": ["Ri\u010dards Marcinkevi\u010ds", "Ece Ozkan", "Julia E. Vogt"], "published": "2022-12-23T13:08:59+00:00", "updated": "2022-12-23T13:08:59+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2212.12303v1", "entry_id": "http://arxiv.org/abs/2212.12303v1"}, {"id": "1607.01400", "filepath": "papers/1607.01400.pdf", "title": "An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality in Machine Learning", "abstract": "We propose a clustering-based iterative algorithm to solve certain\noptimization problems in machine learning, where we start the algorithm by\naggregating the original data, solving the problem on aggregated data, and then\nin subsequent steps gradually disaggregate the aggregated data. We apply the\nalgorithm to common machine learning problems such as the least absolute\ndeviation regression problem, support vector machines, and semi-supervised\nsupport vector machines. We derive model-specific data aggregation and\ndisaggregation procedures. We also show optimality, convergence, and the\noptimality gap of the approximated solution in each iteration. A computational\nstudy is provided.", "authors": ["Young Woong Park", "Diego Klabjan"], "published": "2016-07-05T20:04:57+00:00", "updated": "2016-07-05T20:04:57+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1607.01400v1", "entry_id": "http://arxiv.org/abs/1607.01400v1"}, {"id": "2312.03120", "filepath": "papers/2312.03120.pdf", "title": "The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning", "abstract": "With the advance of the powerful heterogeneous, parallel and distributed\ncomputing systems and ever increasing immense amount of data, machine learning\nhas become an indispensable part of cutting-edge technology, scientific\nresearch and consumer products. In this study, we present a review of modern\nmachine and deep learning. We provide a high-level overview for the latest\nadvanced machine learning algorithms, applications, and frameworks. Our\ndiscussion encompasses parallel distributed learning, deep learning as well as\nfederated learning. As a result, our work serves as an introductory text to the\nvast field of modern machine learning.", "authors": ["Omer Subasi", "Oceane Bel", "Joseph Manzano", "Kevin Barker"], "published": "2023-12-05T20:40:05+00:00", "updated": "2023-12-05T20:40:05+00:00", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf_url": "http://arxiv.org/pdf/2312.03120v1", "entry_id": "http://arxiv.org/abs/2312.03120v1"}, {"id": "2008.08080", "filepath": "papers/2008.08080.pdf", "title": "mlr3proba: An R Package for Machine Learning in Survival Analysis", "abstract": "As machine learning has become increasingly popular over the last few\ndecades, so too has the number of machine learning interfaces for implementing\nthese models. Whilst many R libraries exist for machine learning, very few\noffer extended support for survival analysis. This is problematic considering\nits importance in fields like medicine, bioinformatics, economics, engineering,\nand more. mlr3proba provides a comprehensive machine learning interface for\nsurvival analysis and connects with mlr3's general model tuning and\nbenchmarking facilities to provide a systematic infrastructure for survival\nmodeling and evaluation.", "authors": ["Raphael Sonabend", "Franz J. Kir\u00e1ly", "Andreas Bender", "Bernd Bischl", "Michel Lang"], "published": "2020-08-18T11:21:24+00:00", "updated": "2020-12-14T11:41:25+00:00", "categories": ["stat.CO", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2008.08080v2", "entry_id": "http://arxiv.org/abs/2008.08080v2"}, {"id": "2007.07981", "filepath": "papers/2007.07981.pdf", "title": "Differential Replication in Machine Learning", "abstract": "When deployed in the wild, machine learning models are usually confronted\nwith data and requirements that constantly vary, either because of changes in\nthe generating distribution or because external constraints change the\nenvironment where the model operates. To survive in such an ecosystem, machine\nlearning models need to adapt to new conditions by evolving over time. The idea\nof model adaptability has been studied from different perspectives. In this\npaper, we propose a solution based on reusing the knowledge acquired by the\nalready deployed machine learning models and leveraging it to train future\ngenerations. This is the idea behind differential replication of machine\nlearning models.", "authors": ["Irene Unceta", "Jordi Nin", "Oriol Pujol"], "published": "2020-07-15T20:26:49+00:00", "updated": "2020-07-15T20:26:49+00:00", "categories": ["cs.LG", "stat.ML", "cs.LG, stat.ML"], "pdf_url": "http://arxiv.org/pdf/2007.07981v1", "entry_id": "http://arxiv.org/abs/2007.07981v1"}, {"id": "2108.07915", "filepath": "papers/2108.07915.pdf", "title": "Data Pricing in Machine Learning Pipelines", "abstract": "Machine learning is disruptive. At the same time, machine learning can only\nsucceed by collaboration among many parties in multiple steps naturally as\npipelines in an eco-system, such as collecting data for possible machine\nlearning applications, collaboratively training models by multiple parties and\ndelivering machine learning services to end users. Data is critical and\npenetrating in the whole machine learning pipelines. As machine learning\npipelines involve many parties and, in order to be successful, have to form a\nconstructive and dynamic eco-system, marketplaces and data pricing are\nfundamental in connecting and facilitating those many parties. In this article,\nwe survey the principles and the latest research development of data pricing in\nmachine learning pipelines. We start with a brief review of data marketplaces\nand pricing desiderata. Then, we focus on pricing in three important steps in\nmachine learning pipelines. To understand pricing in the step of training data\ncollection, we review pricing raw data sets and data labels. We also\ninvestigate pricing in the step of collaborative training of machine learning\nmodels, and overview pricing machine learning models for end users in the step\nof machine learning deployment. We also discuss a series of possible future\ndirections.", "authors": ["Zicun Cong", "Xuan Luo", "Pei Jian", "Feida Zhu", "Yong Zhang"], "published": "2021-08-18T00:57:06+00:00", "updated": "2021-08-18T00:57:06+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/2108.07915v1", "entry_id": "http://arxiv.org/abs/2108.07915v1"}, {"id": "1911.08587", "filepath": "papers/1911.08587.pdf", "title": "Solving machine learning optimization problems using quantum computers", "abstract": "Classical optimization algorithms in machine learning often take a long time\nto compute when applied to a multi-dimensional problem and require a huge\namount of CPU and GPU resource. Quantum parallelism has a potential to speed up\nmachine learning algorithms. We describe a generic mathematical model to\nleverage quantum parallelism to speed-up machine learning algorithms. We also\napply quantum machine learning and quantum parallelism applied to a\n$3$-dimensional image that vary with time.", "authors": ["Venkat R. Dasari", "Mee Seong Im", "Lubjana Beshaj"], "published": "2019-11-17T17:36:41+00:00", "updated": "2019-11-17T17:36:41+00:00", "categories": ["quant-ph", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1911.08587v1", "entry_id": "http://arxiv.org/abs/1911.08587v1"}, {"id": "1909.09246", "filepath": "papers/1909.09246.pdf", "title": "Machine Learning for Clinical Predictive Analytics", "abstract": "In this chapter, we provide a brief overview of applying machine learning\ntechniques for clinical prediction tasks. We begin with a quick introduction to\nthe concepts of machine learning and outline some of the most common machine\nlearning algorithms. Next, we demonstrate how to apply the algorithms with\nappropriate toolkits to conduct machine learning experiments for clinical\nprediction tasks. The objectives of this chapter are to (1) understand the\nbasics of machine learning techniques and the reasons behind why they are\nuseful for solving clinical prediction problems, (2) understand the intuition\nbehind some machine learning models, including regression, decision trees, and\nsupport vector machines, and (3) understand how to apply these models to\nclinical prediction problems using publicly available datasets via case\nstudies.", "authors": ["Wei-Hung Weng"], "published": "2019-09-19T22:02:00+00:00", "updated": "2019-09-19T22:02:00+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1909.09246v1", "entry_id": "http://arxiv.org/abs/1909.09246v1"}, {"id": "2305.15410", "filepath": "papers/2305.15410.pdf", "title": "Machine learning-assisted close-set X-ray diffraction phase identification of transition metals", "abstract": "Machine learning has been applied to the problem of X-ray diffraction phase\nprediction with promising results. In this paper, we describe a method for\nusing machine learning to predict crystal structure phases from X-ray\ndiffraction data of transition metals and their oxides. We evaluate the\nperformance of our method and compare the variety of its settings. Our results\ndemonstrate that the proposed machine learning framework achieves competitive\nperformance. This demonstrates the potential for machine learning to\nsignificantly impact the field of X-ray diffraction and crystal structure\ndetermination. Open-source implementation:\nhttps://github.com/maxnygma/NeuralXRD.", "authors": ["Maksim Zhdanov", "Andrey Zhdanov"], "published": "2023-04-28T09:29:10+00:00", "updated": "2023-04-28T09:29:10+00:00", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2305.15410v1", "entry_id": "http://arxiv.org/abs/2305.15410v1"}, {"id": "1909.01866", "filepath": "papers/1909.01866.pdf", "title": "Understanding Bias in Machine Learning", "abstract": "Bias is known to be an impediment to fair decisions in many domains such as\nhuman resources, the public sector, health care etc. Recently, hope has been\nexpressed that the use of machine learning methods for taking such decisions\nwould diminish or even resolve the problem. At the same time, machine learning\nexperts warn that machine learning models can be biased as well. In this\narticle, our goal is to explain the issue of bias in machine learning from a\ntechnical perspective and to illustrate the impact that biased data can have on\na machine learning model. To reach such a goal, we develop interactive plots to\nvisualizing the bias learned from synthetic data.", "authors": ["Jindong Gu", "Daniela Oelke"], "published": "2019-09-02T20:36:19+00:00", "updated": "2019-09-02T20:36:19+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1909.01866v1", "entry_id": "http://arxiv.org/abs/1909.01866v1"}, {"id": "1707.09562", "filepath": "papers/1707.09562.pdf", "title": "MLBench: How Good Are Machine Learning Clouds for Binary Classification Tasks on Structured Data?", "abstract": "We conduct an empirical study of machine learning functionalities provided by\nmajor cloud service providers, which we call machine learning clouds. Machine\nlearning clouds hold the promise of hiding all the sophistication of running\nlarge-scale machine learning: Instead of specifying how to run a machine\nlearning task, users only specify what machine learning task to run and the\ncloud figures out the rest. Raising the level of abstraction, however, rarely\ncomes free - a performance penalty is possible. How good, then, are current\nmachine learning clouds on real-world machine learning workloads?\n  We study this question with a focus on binary classication problems. We\npresent mlbench, a novel benchmark constructed by harvesting datasets from\nKaggle competitions. We then compare the performance of the top winning code\navailable from Kaggle with that of running machine learning clouds from both\nAzure and Amazon on mlbench. Our comparative study reveals the strength and\nweakness of existing machine learning clouds and points out potential future\ndirections for improvement.", "authors": ["Yu Liu", "Hantian Zhang", "Luyuan Zeng", "Wentao Wu", "Ce Zhang"], "published": "2017-07-29T21:59:18+00:00", "updated": "2017-10-16T11:13:32+00:00", "categories": ["cs.DC", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1707.09562v3", "entry_id": "http://arxiv.org/abs/1707.09562v3"}, {"id": "1808.00033", "filepath": "papers/1808.00033.pdf", "title": "Techniques for Interpretable Machine Learning", "abstract": "Interpretable machine learning tackles the important problem that humans\ncannot understand the behaviors of complex machine learning models and how\nthese models arrive at a particular decision. Although many approaches have\nbeen proposed, a comprehensive understanding of the achievements and challenges\nis still lacking. We provide a survey covering existing techniques to increase\nthe interpretability of machine learning models. We also discuss crucial issues\nthat the community should consider in future work such as designing\nuser-friendly explanations and developing comprehensive evaluation metrics to\nfurther push forward the area of interpretable machine learning.", "authors": ["Mengnan Du", "Ninghao Liu", "Xia Hu"], "published": "2018-07-31T19:14:39+00:00", "updated": "2019-05-19T20:44:37+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1808.00033v3", "entry_id": "http://arxiv.org/abs/1808.00033v3"}, {"id": "1507.02188", "filepath": "papers/1507.02188.pdf", "title": "AutoCompete: A Framework for Machine Learning Competition", "abstract": "In this paper, we propose AutoCompete, a highly automated machine learning\nframework for tackling machine learning competitions. This framework has been\nlearned by us, validated and improved over a period of more than two years by\nparticipating in online machine learning competitions. It aims at minimizing\nhuman interference required to build a first useful predictive model and to\nassess the practical difficulty of a given machine learning challenge. The\nproposed system helps in identifying data types, choosing a machine learn- ing\nmodel, tuning hyper-parameters, avoiding over-fitting and optimization for a\nprovided evaluation metric. We also observe that the proposed system produces\nbetter (or comparable) results with less runtime as compared to other\napproaches.", "authors": ["Abhishek Thakur", "Artus Krohn-Grimberghe"], "published": "2015-07-08T15:07:39+00:00", "updated": "2015-07-08T15:07:39+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1507.02188v1", "entry_id": "http://arxiv.org/abs/1507.02188v1"}, {"id": "1612.04858", "filepath": "papers/1612.04858.pdf", "title": "Bayesian Optimization for Machine Learning : A Practical Guidebook", "abstract": "The engineering of machine learning systems is still a nascent field; relying\non a seemingly daunting collection of quickly evolving tools and best\npractices. It is our hope that this guidebook will serve as a useful resource\nfor machine learning practitioners looking to take advantage of Bayesian\noptimization techniques. We outline four example machine learning problems that\ncan be solved using open source machine learning libraries, and highlight the\nbenefits of using Bayesian optimization in the context of these common machine\nlearning applications.", "authors": ["Ian Dewancker", "Michael McCourt", "Scott Clark"], "published": "2016-12-14T22:04:33+00:00", "updated": "2016-12-14T22:04:33+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/1612.04858v1", "entry_id": "http://arxiv.org/abs/1612.04858v1"}, {"id": "2110.12773", "filepath": "papers/2110.12773.pdf", "title": "Scientific Machine Learning Benchmarks", "abstract": "The breakthrough in Deep Learning neural networks has transformed the use of\nAI and machine learning technologies for the analysis of very large\nexperimental datasets. These datasets are typically generated by large-scale\nexperimental facilities at national laboratories. In the context of science,\nscientific machine learning focuses on training machines to identify patterns,\ntrends, and anomalies to extract meaningful scientific insights from such\ndatasets. With a new generation of experimental facilities, the rate of data\ngeneration and the scale of data volumes will increasingly require the use of\nmore automated data analysis. At present, identifying the most appropriate\nmachine learning algorithm for the analysis of any given scientific dataset is\nstill a challenge for scientists. This is due to many different machine\nlearning frameworks, computer architectures, and machine learning models.\nHistorically, for modelling and simulation on HPC systems such problems have\nbeen addressed through benchmarking computer applications, algorithms, and\narchitectures. Extending such a benchmarking approach and identifying metrics\nfor the application of machine learning methods to scientific datasets is a new\nchallenge for both scientists and computer scientists. In this paper, we\ndescribe our approach to the development of scientific machine learning\nbenchmarks and review other approaches to benchmarking scientific machine\nlearning.", "authors": ["Jeyan Thiyagalingam", "Mallikarjun Shankar", "Geoffrey Fox", "Tony Hey"], "published": "2021-10-25T10:05:11+00:00", "updated": "2021-10-25T10:05:11+00:00", "categories": ["cs.LG", "physics.comp-ph", "I.2"], "pdf_url": "http://arxiv.org/pdf/2110.12773v1", "entry_id": "http://arxiv.org/abs/2110.12773v1"}, {"id": "1702.08608", "filepath": "papers/1702.08608.pdf", "title": "Towards A Rigorous Science of Interpretable Machine Learning", "abstract": "As machine learning systems become ubiquitous, there has been a surge of\ninterest in interpretable machine learning: systems that provide explanation\nfor their outputs. These explanations are often used to qualitatively assess\nother criteria such as safety or non-discrimination. However, despite the\ninterest in interpretability, there is very little consensus on what\ninterpretable machine learning is and how it should be measured. In this\nposition paper, we first define interpretability and describe when\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\nfor rigorous evaluation and expose open questions towards a more rigorous\nscience of interpretable machine learning.", "authors": ["Finale Doshi-Velez", "Been Kim"], "published": "2017-02-28T02:19:20+00:00", "updated": "2017-03-02T19:32:10+00:00", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1702.08608v2", "entry_id": "http://arxiv.org/abs/1702.08608v2"}, {"id": "1911.06612", "filepath": "papers/1911.06612.pdf", "title": "Position Paper: Towards Transparent Machine Learning", "abstract": "Transparent machine learning is introduced as an alternative form of machine\nlearning, where both the model and the learning system are represented in\nsource code form. The goal of this project is to enable direct human\nunderstanding of machine learning models, giving us the ability to learn,\nverify, and refine them as programs. If solved, this technology could represent\na best-case scenario for the safety and security of AI systems going forward.", "authors": ["Dustin Juliano"], "published": "2019-11-12T10:49:55+00:00", "updated": "2019-11-12T10:49:55+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/1911.06612v1", "entry_id": "http://arxiv.org/abs/1911.06612v1"}, {"id": "2103.11249", "filepath": "papers/2103.11249.pdf", "title": "SELM: Software Engineering of Machine Learning Models", "abstract": "One of the pillars of any machine learning model is its concepts. Using\nsoftware engineering, we can engineer these concepts and then develop and\nexpand them. In this article, we present a SELM framework for Software\nEngineering of machine Learning Models. We then evaluate this framework through\na case study. Using the SELM framework, we can improve a machine learning\nprocess efficiency and provide more accuracy in learning with less processing\nhardware resources and a smaller training dataset. This issue highlights the\nimportance of an interdisciplinary approach to machine learning. Therefore, in\nthis article, we have provided interdisciplinary teams' proposals for machine\nlearning.", "authors": ["Nafiseh Jafari", "Mohammad Reza Besharati", "Mohammad Izadi", "Maryam Hourali"], "published": "2021-03-20T21:43:24+00:00", "updated": "2021-03-20T21:43:24+00:00", "categories": ["cs.SE", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2103.11249v1", "entry_id": "http://arxiv.org/abs/2103.11249v1"}, {"id": "2301.09753", "filepath": "papers/2301.09753.pdf", "title": "Towards Modular Machine Learning Solution Development: Benefits and Trade-offs", "abstract": "Machine learning technologies have demonstrated immense capabilities in\nvarious domains. They play a key role in the success of modern businesses.\nHowever, adoption of machine learning technologies has a lot of untouched\npotential. Cost of developing custom machine learning solutions that solve\nunique business problems is a major inhibitor to far-reaching adoption of\nmachine learning technologies. We recognize that the monolithic nature\nprevalent in today's machine learning applications stands in the way of\nefficient and cost effective customized machine learning solution development.\nIn this work we explore the benefits of modular machine learning solutions and\ndiscuss how modular machine learning solutions can overcome some of the major\nsolution engineering limitations of monolithic machine learning solutions. We\nanalyze the trade-offs between modular and monolithic machine learning\nsolutions through three deep learning problems; one text based and the two\nimage based. Our experimental results show that modular machine learning\nsolutions have a promising potential to reap the solution engineering\nadvantages of modularity while gaining performance and data advantages in a way\nthe monolithic machine learning solutions do not permit.", "authors": ["Samiyuru Menik", "Lakshmish Ramaswamy"], "published": "2023-01-23T22:54:34+00:00", "updated": "2023-01-23T22:54:34+00:00", "categories": ["cs.LG", "cs.SE"], "pdf_url": "http://arxiv.org/pdf/2301.09753v1", "entry_id": "http://arxiv.org/abs/2301.09753v1"}, {"id": "2001.11489", "filepath": "papers/2001.11489.pdf", "title": "Machine Learning in Network Security Using KNIME Analytics", "abstract": "Machine learning has more and more effect on our every day's life. This field\nkeeps growing and expanding into new areas. Machine learning is based on the\nimplementation of artificial intelligence that gives systems the capability to\nautomatically learn and enhance from experiments without being explicitly\nprogrammed. Machine Learning algorithms apply mathematical equations to analyze\ndatasets and predict values based on the dataset. In the field of\ncybersecurity, machine learning algorithms can be utilized to train and analyze\nthe Intrusion Detection Systems (IDSs) on security-related datasets. In this\npaper, we tested different machine learning algorithms to analyze NSL-KDD\ndataset using KNIME analytics.", "authors": ["Munther Abualkibash"], "published": "2019-11-18T14:10:17+00:00", "updated": "2019-11-18T14:10:17+00:00", "categories": ["cs.CR"], "pdf_url": "http://arxiv.org/pdf/2001.11489v1", "entry_id": "http://arxiv.org/abs/2001.11489v1"}, {"id": "2007.01977", "filepath": "papers/2007.01977.pdf", "title": "Lale: Consistent Automated Machine Learning", "abstract": "Automated machine learning makes it easier for data scientists to develop\npipelines by searching over possible choices for hyperparameters, algorithms,\nand even pipeline topologies. Unfortunately, the syntax for automated machine\nlearning tools is inconsistent with manual machine learning, with each other,\nand with error checks. Furthermore, few tools support advanced features such as\ntopology search or higher-order operators. This paper introduces Lale, a\nlibrary of high-level Python interfaces that simplifies and unifies automated\nmachine learning in a consistent way.", "authors": ["Guillaume Baudart", "Martin Hirzel", "Kiran Kate", "Parikshit Ram", "Avraham Shinnar"], "published": "2020-07-04T00:55:41+00:00", "updated": "2020-07-04T00:55:41+00:00", "categories": ["cs.LG", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2007.01977v1", "entry_id": "http://arxiv.org/abs/2007.01977v1"}, {"id": "1910.02544", "filepath": "papers/1910.02544.pdf", "title": "Using Deep Learning and Machine Learning to Detect Epileptic Seizure with Electroencephalography (EEG) Data", "abstract": "The prediction of epileptic seizure has always been extremely challenging in\nmedical domain. However, as the development of computer technology, the\napplication of machine learning introduced new ideas for seizure forecasting.\nApplying machine learning model onto the predication of epileptic seizure could\nhelp us obtain a better result and there have been plenty of scientists who\nhave been doing such works so that there are sufficient medical data provided\nfor researchers to do training of machine learning models.", "authors": ["Haotian Liu", "Lin Xi", "Ying Zhao", "Zhixiang Li"], "published": "2019-10-06T22:53:28+00:00", "updated": "2019-10-06T22:53:28+00:00", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1910.02544v1", "entry_id": "http://arxiv.org/abs/1910.02544v1"}, {"id": "1705.07538", "filepath": "papers/1705.07538.pdf", "title": "Infrastructure for Usable Machine Learning: The Stanford DAWN Project", "abstract": "Despite incredible recent advances in machine learning, building machine\nlearning applications remains prohibitively time-consuming and expensive for\nall but the best-trained, best-funded engineering organizations. This expense\ncomes not from a need for new and improved statistical models but instead from\na lack of systems and tools for supporting end-to-end machine learning\napplication development, from data preparation and labeling to\nproductionization and monitoring. In this document, we outline opportunities\nfor infrastructure supporting usable, end-to-end machine learning applications\nin the context of the nascent DAWN (Data Analytics for What's Next) project at\nStanford.", "authors": ["Peter Bailis", "Kunle Olukotun", "Christopher Re", "Matei Zaharia"], "published": "2017-05-22T02:28:19+00:00", "updated": "2017-06-09T02:13:09+00:00", "categories": ["cs.LG", "cs.DB", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1705.07538v2", "entry_id": "http://arxiv.org/abs/1705.07538v2"}, {"id": "1907.08908", "filepath": "papers/1907.08908.pdf", "title": "Techniques for Automated Machine Learning", "abstract": "Automated machine learning (AutoML) aims to find optimal machine learning\nsolutions automatically given a machine learning problem. It could release the\nburden of data scientists from the multifarious manual tuning process and\nenable the access of domain experts to the off-the-shelf machine learning\nsolutions without extensive experience. In this paper, we review the current\ndevelopments of AutoML in terms of three categories, automated feature\nengineering (AutoFE), automated model and hyperparameter learning (AutoMHL),\nand automated deep learning (AutoDL). State-of-the-art techniques adopted in\nthe three categories are presented, including Bayesian optimization,\nreinforcement learning, evolutionary algorithm, and gradient-based approaches.\nWe summarize popular AutoML frameworks and conclude with current open\nchallenges of AutoML.", "authors": ["Yi-Wei Chen", "Qingquan Song", "Xia Hu"], "published": "2019-07-21T04:03:36+00:00", "updated": "2019-07-21T04:03:36+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1907.08908v1", "entry_id": "http://arxiv.org/abs/1907.08908v1"}, {"id": "2003.05155", "filepath": "papers/2003.05155.pdf", "title": "Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology", "abstract": "Machine learning is an established and frequently used technique in industry\nand academia but a standard process model to improve success and efficiency of\nmachine learning applications is still missing. Project organizations and\nmachine learning practitioners have a need for guidance throughout the life\ncycle of a machine learning application to meet business expectations. We\ntherefore propose a process model for the development of machine learning\napplications, that covers six phases from defining the scope to maintaining the\ndeployed machine learning application. The first phase combines business and\ndata understanding as data availability oftentimes affects the feasibility of\nthe project. The sixth phase covers state-of-the-art approaches for monitoring\nand maintenance of a machine learning applications, as the risk of model\ndegradation in a changing environment is eminent. With each task of the\nprocess, we propose quality assurance methodology that is suitable to adress\nchallenges in machine learning development that we identify in form of risks.\nThe methodology is drawn from practical experience and scientific literature\nand has proven to be general and stable. The process model expands on CRISP-DM,\na data mining process model that enjoys strong industry support but lacks to\naddress machine learning specific tasks. Our work proposes an industry and\napplication neutral process model tailored for machine learning applications\nwith focus on technical tasks for quality assurance.", "authors": ["Stefan Studer", "Thanh Binh Bui", "Christian Drescher", "Alexander Hanuschkin", "Ludwig Winkler", "Steven Peters", "Klaus-Robert Mueller"], "published": "2020-03-11T08:25:49+00:00", "updated": "2021-02-24T14:33:24+00:00", "categories": ["cs.LG", "cs.SE", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2003.05155v2", "entry_id": "http://arxiv.org/abs/2003.05155v2"}, {"id": "1612.04251", "filepath": "papers/1612.04251.pdf", "title": "TF.Learn: TensorFlow's High-level Module for Distributed Machine Learning", "abstract": "TF.Learn is a high-level Python module for distributed machine learning\ninside TensorFlow. It provides an easy-to-use Scikit-learn style interface to\nsimplify the process of creating, configuring, training, evaluating, and\nexperimenting a machine learning model. TF.Learn integrates a wide range of\nstate-of-art machine learning algorithms built on top of TensorFlow's low level\nAPIs for small to large-scale supervised and unsupervised problems. This module\nfocuses on bringing machine learning to non-specialists using a general-purpose\nhigh-level language as well as researchers who want to implement, benchmark,\nand compare their new methods in a structured environment. Emphasis is put on\nease of use, performance, documentation, and API consistency.", "authors": ["Yuan Tang"], "published": "2016-12-13T16:00:51+00:00", "updated": "2016-12-13T16:00:51+00:00", "categories": ["cs.DC", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1612.04251v1", "entry_id": "http://arxiv.org/abs/1612.04251v1"}, {"id": "1405.1304", "filepath": "papers/1405.1304.pdf", "title": "Application of Machine Learning Techniques in Aquaculture", "abstract": "In this paper we present applications of different machine learning\nalgorithms in aquaculture. Machine learning algorithms learn models from\nhistorical data. In aquaculture historical data are obtained from farm\npractices, yields, and environmental data sources. Associations between these\ndifferent variables can be obtained by applying machine learning algorithms to\nhistorical data. In this paper we present applications of different machine\nlearning algorithms in aquaculture applications.", "authors": ["Akhlaqur Rahman", "Sumaira Tasnim"], "published": "2014-05-03T14:26:42+00:00", "updated": "2014-05-03T14:26:42+00:00", "categories": ["cs.CE", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1405.1304v1", "entry_id": "http://arxiv.org/abs/1405.1304v1"}, {"id": "2104.05314", "filepath": "papers/2104.05314.pdf", "title": "Machine learning and deep learning", "abstract": "Today, intelligent systems that offer artificial intelligence capabilities\noften rely on machine learning. Machine learning describes the capacity of\nsystems to learn from problem-specific training data to automate the process of\nanalytical model building and solve associated tasks. Deep learning is a\nmachine learning concept based on artificial neural networks. For many\napplications, deep learning models outperform shallow machine learning models\nand traditional data analysis approaches. In this article, we summarize the\nfundamentals of machine learning and deep learning to generate a broader\nunderstanding of the methodical underpinning of current intelligent systems. In\nparticular, we provide a conceptual distinction between relevant terms and\nconcepts, explain the process of automated analytical model building through\nmachine learning and deep learning, and discuss the challenges that arise when\nimplementing such intelligent systems in the field of electronic markets and\nnetworked business. These naturally go beyond technological aspects and\nhighlight issues in human-machine interaction and artificial intelligence\nservitization.", "authors": ["Christian Janiesch", "Patrick Zschech", "Kai Heinrich"], "published": "2021-04-12T09:54:12+00:00", "updated": "2021-04-14T10:31:01+00:00", "categories": ["cs.AI"], "pdf_url": "http://arxiv.org/pdf/2104.05314v2", "entry_id": "http://arxiv.org/abs/2104.05314v2"}, {"id": "1810.11383", "filepath": "papers/1810.11383.pdf", "title": "Some Requests for Machine Learning Research from the East African Tech Scene", "abstract": "Based on 46 in-depth interviews with scientists, engineers, and CEOs, this\ndocument presents a list of concrete machine research problems, progress on\nwhich would directly benefit tech ventures in East Africa.", "authors": ["Milan Cvitkovic"], "published": "2018-10-25T02:53:14+00:00", "updated": "2018-11-08T01:03:50+00:00", "categories": ["cs.LG", "cs.CY", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1810.11383v2", "entry_id": "http://arxiv.org/abs/1810.11383v2"}, {"id": "2007.14206", "filepath": "papers/2007.14206.pdf", "title": "Machine Learning Potential Repository", "abstract": "This paper introduces a machine learning potential repository that includes\nPareto optimal machine learning potentials. It also shows the systematic\ndevelopment of accurate and fast machine learning potentials for a wide range\nof elemental systems. As a result, many Pareto optimal machine learning\npotentials are available in the repository from a website. Therefore, the\nrepository will help many scientists to perform accurate and fast atomistic\nsimulations.", "authors": ["Atsuto Seko"], "published": "2020-07-27T14:30:23+00:00", "updated": "2020-07-27T14:30:23+00:00", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci", "physics.chem-ph", "physics.data-an"], "pdf_url": "http://arxiv.org/pdf/2007.14206v1", "entry_id": "http://arxiv.org/abs/2007.14206v1"}, {"id": "2012.04105", "filepath": "papers/2012.04105.pdf", "title": "The Tribes of Machine Learning and the Realm of Computer Architecture", "abstract": "Machine learning techniques have influenced the field of computer\narchitecture like many other fields. This paper studies how the fundamental\nmachine learning techniques can be applied towards computer architecture\nproblems. We also provide a detailed survey of computer architecture research\nthat employs different machine learning methods. Finally, we present some\nfuture opportunities and the outstanding challenges that need to be overcome to\nexploit full potential of machine learning for computer architecture.", "authors": ["Ayaz Akram", "Jason Lowe-Power"], "published": "2020-12-07T23:10:51+00:00", "updated": "2020-12-07T23:10:51+00:00", "categories": ["cs.LG", "cs.AR"], "pdf_url": "http://arxiv.org/pdf/2012.04105v1", "entry_id": "http://arxiv.org/abs/2012.04105v1"}, {"id": "1611.03969", "filepath": "papers/1611.03969.pdf", "title": "An Introduction to MM Algorithms for Machine Learning and Statistical", "abstract": "MM (majorization--minimization) algorithms are an increasingly popular tool\nfor solving optimization problems in machine learning and statistical\nestimation. This article introduces the MM algorithm framework in general and\nvia three popular example applications: Gaussian mixture regressions,\nmultinomial logistic regressions, and support vector machines. Specific\nalgorithms for the three examples are derived and numerical demonstrations are\npresented. Theoretical and practical aspects of MM algorithm design are\ndiscussed.", "authors": ["Hien D. Nguyen"], "published": "2016-11-12T08:18:38+00:00", "updated": "2016-11-12T08:18:38+00:00", "categories": ["stat.CO", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1611.03969v1", "entry_id": "http://arxiv.org/abs/1611.03969v1"}, {"id": "2407.05526", "filepath": "papers/2407.05526.pdf", "title": "Can Machines Learn the True Probabilities?", "abstract": "When there exists uncertainty, AI machines are designed to make decisions so\nas to reach the best expected outcomes. Expectations are based on true facts\nabout the objective environment the machines interact with, and those facts can\nbe encoded into AI models in the form of true objective probability functions.\nAccordingly, AI models involve probabilistic machine learning in which the\nprobabilities should be objectively interpreted. We prove under some basic\nassumptions when machines can learn the true objective probabilities, if any,\nand when machines cannot learn them.", "authors": ["Jinsook Kim"], "published": "2024-07-08T00:19:43+00:00", "updated": "2024-07-08T00:19:43+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2407.05526v1", "entry_id": "http://arxiv.org/abs/2407.05526v1"}, {"id": "1707.03184", "filepath": "papers/1707.03184.pdf", "title": "A Survey on Resilient Machine Learning", "abstract": "Machine learning based system are increasingly being used for sensitive tasks\nsuch as security surveillance, guiding autonomous vehicle, taking investment\ndecisions, detecting and blocking network intrusion and malware etc. However,\nrecent research has shown that machine learning models are venerable to attacks\nby adversaries at all phases of machine learning (eg, training data collection,\ntraining, operation). All model classes of machine learning systems can be\nmisled by providing carefully crafted inputs making them wrongly classify\ninputs. Maliciously created input samples can affect the learning process of a\nML system by either slowing down the learning process, or affecting the\nperformance of the learned mode, or causing the system make error(s) only in\nattacker's planned scenario. Because of these developments, understanding\nsecurity of machine learning algorithms and systems is emerging as an important\nresearch area among computer security and machine learning researchers and\npractitioners. We present a survey of this emerging area in machine learning.", "authors": ["Atul Kumar", "Sameep Mehta"], "published": "2017-07-11T09:15:46+00:00", "updated": "2017-07-11T09:15:46+00:00", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1707.03184v1", "entry_id": "http://arxiv.org/abs/1707.03184v1"}, {"id": "2007.01503", "filepath": "papers/2007.01503.pdf", "title": "Mathematical Perspective of Machine Learning", "abstract": "We take a closer look at some theoretical challenges of Machine Learning as a\nfunction approximation, gradient descent as the default optimization algorithm,\nlimitations of fixed length and width networks and a different approach to RNNs\nfrom a mathematical perspective.", "authors": ["Yarema Boryshchak"], "published": "2020-07-03T05:26:02+00:00", "updated": "2020-07-03T05:26:02+00:00", "categories": ["cs.LG", "stat.ML", "68T07"], "pdf_url": "http://arxiv.org/pdf/2007.01503v1", "entry_id": "http://arxiv.org/abs/2007.01503v1"}, {"id": "1812.01410", "filepath": "papers/1812.01410.pdf", "title": "Compressive Classification (Machine Learning without learning)", "abstract": "Compressive learning is a framework where (so far unsupervised) learning\ntasks use not the entire dataset but a compressed summary (sketch) of it. We\npropose a compressive learning classification method, and a novel sketch\nfunction for images.", "authors": ["Vincent Schellekens", "Laurent Jacques"], "published": "2018-12-04T13:50:11+00:00", "updated": "2018-12-04T13:50:11+00:00", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1812.01410v1", "entry_id": "http://arxiv.org/abs/1812.01410v1"}, {"id": "2001.09608", "filepath": "papers/2001.09608.pdf", "title": "Some Insights into Lifelong Reinforcement Learning Systems", "abstract": "A lifelong reinforcement learning system is a learning system that has the\nability to learn through trail-and-error interaction with the environment over\nits lifetime. In this paper, I give some arguments to show that the traditional\nreinforcement learning paradigm fails to model this type of learning system.\nSome insights into lifelong reinforcement learning are provided, along with a\nsimplistic prototype lifelong reinforcement learning system.", "authors": ["Changjian Li"], "published": "2020-01-27T07:26:12+00:00", "updated": "2020-01-27T07:26:12+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2001.09608v1", "entry_id": "http://arxiv.org/abs/2001.09608v1"}, {"id": "2202.10564", "filepath": "papers/2202.10564.pdf", "title": "Human-in-the-loop Machine Learning: A Macro-Micro Perspective", "abstract": "Though technical advance of artificial intelligence and machine learning has\nenabled many promising intelligent systems, many computing tasks are still not\nable to be fully accomplished by machine intelligence. Motivated by the\ncomplementary nature of human and machine intelligence, an emerging trend is to\ninvolve humans in the loop of machine learning and decision-making. In this\npaper, we provide a macro-micro review of human-in-the-loop machine learning.\nWe first describe major machine learning challenges which can be addressed by\nhuman intervention in the loop. Then we examine closely the latest research and\nfindings of introducing humans into each step of the lifecycle of machine\nlearning. Finally, we analyze current research gaps and point out future\nresearch directions.", "authors": ["Jiangtao Wang", "Bin Guo", "Liming Chen"], "published": "2022-02-21T22:45:59+00:00", "updated": "2022-02-21T22:45:59+00:00", "categories": ["cs.HC"], "pdf_url": "http://arxiv.org/pdf/2202.10564v1", "entry_id": "http://arxiv.org/abs/2202.10564v1"}, {"id": "1509.00913", "filepath": "papers/1509.00913.pdf", "title": "On-the-Fly Learning in a Perpetual Learning Machine", "abstract": "Despite the promise of brain-inspired machine learning, deep neural networks\n(DNN) have frustratingly failed to bridge the deceptively large gap between\nlearning and memory. Here, we introduce a Perpetual Learning Machine; a new\ntype of DNN that is capable of brain-like dynamic 'on the fly' learning because\nit exists in a self-supervised state of Perpetual Stochastic Gradient Descent.\nThus, we provide the means to unify learning and memory within a machine\nlearning framework. We also explore the elegant duality of abstraction and\nsynthesis: the Yin and Yang of deep learning.", "authors": ["Andrew J. R. Simpson"], "published": "2015-09-03T01:30:29+00:00", "updated": "2015-09-29T16:57:42+00:00", "categories": ["cs.LG", "68Txx"], "pdf_url": "http://arxiv.org/pdf/1509.00913v3", "entry_id": "http://arxiv.org/abs/1509.00913v3"}, {"id": "2201.06921", "filepath": "papers/2201.06921.pdf", "title": "Can Machine Learning be Moral?", "abstract": "The ethics of Machine Learning has become an unavoidable topic in the AI\nCommunity. The deployment of machine learning systems in multiple social\ncontexts has resulted in a closer ethical scrutiny of the design, development,\nand application of these systems. The AI/ML community has come to terms with\nthe imperative to think about the ethical implications of machine learning, not\nonly as a product but also as a practice (Birhane, 2021; Shen et al. 2021). The\ncritical question that is troubling many debates is what can constitute an\nethically accountable machine learning system. In this paper we explore\npossibilities for ethical evaluation of machine learning methodologies. We\nscrutinize techniques, methods and technical practices in machine learning from\na relational ethics perspective, taking into consideration how machine learning\nsystems are part of the world and how they relate to different forms of agency.\nTaking a page from Phil Agre (1997) we use the notion of a critical technical\npractice as a means of analysis of machine learning approaches. Our radical\nproposal is that supervised learning appears to be the only machine learning\nmethod that is ethically defensible.", "authors": ["Miguel Sicart", "Irina Shklovski", "Mirabelle Jones"], "published": "2021-12-13T07:20:50+00:00", "updated": "2021-12-13T07:20:50+00:00", "categories": ["cs.CY", "cs.HC"], "pdf_url": "http://arxiv.org/pdf/2201.06921v1", "entry_id": "http://arxiv.org/abs/2201.06921v1"}, {"id": "2412.18979", "filepath": "papers/2412.18979.pdf", "title": "Quantum memristors for neuromorphic quantum machine learning", "abstract": "Quantum machine learning may permit to realize more efficient machine\nlearning calculations with near-term quantum devices. Among the diverse quantum\nmachine learning paradigms which are currently being considered, quantum\nmemristors are promising as a way of combining, in the same quantum hardware, a\nunitary evolution with the nonlinearity provided by the measurement and\nfeedforward. Thus, an efficient way of deploying neuromorphic quantum computing\nfor quantum machine learning may be enabled.", "authors": ["Lucas Lamata"], "published": "2024-12-25T20:21:24+00:00", "updated": "2024-12-25T20:21:24+00:00", "categories": ["quant-ph", "cs.NE"], "pdf_url": "http://arxiv.org/pdf/2412.18979v1", "entry_id": "http://arxiv.org/abs/2412.18979v1"}, {"id": "2002.12364", "filepath": "papers/2002.12364.pdf", "title": "Theoretical Models of Learning to Learn", "abstract": "A Machine can only learn if it is biased in some way. Typically the bias is\nsupplied by hand, for example through the choice of an appropriate set of\nfeatures. However, if the learning machine is embedded within an {\\em\nenvironment} of related tasks, then it can {\\em learn} its own bias by learning\nsufficiently many tasks from the environment. In this paper two models of bias\nlearning (or equivalently, learning to learn) are introduced and the main\ntheoretical results presented. The first model is a PAC-type model based on\nempirical process theory, while the second is a hierarchical Bayes model.", "authors": ["Jonathan Baxter"], "published": "2020-02-27T13:35:26+00:00", "updated": "2020-02-27T13:35:26+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2002.12364v1", "entry_id": "http://arxiv.org/abs/2002.12364v1"}, {"id": "1811.04422", "filepath": "papers/1811.04422.pdf", "title": "An Optimal Control View of Adversarial Machine Learning", "abstract": "I describe an optimal control view of adversarial machine learning, where the\ndynamical system is the machine learner, the input are adversarial actions, and\nthe control costs are defined by the adversary's goals to do harm and be hard\nto detect. This view encompasses many types of adversarial machine learning,\nincluding test-item attacks, training-data poisoning, and adversarial reward\nshaping. The view encourages adversarial machine learning researcher to utilize\nadvances in control theory and reinforcement learning.", "authors": ["Xiaojin Zhu"], "published": "2018-11-11T14:28:34+00:00", "updated": "2018-11-11T14:28:34+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1811.04422v1", "entry_id": "http://arxiv.org/abs/1811.04422v1"}, {"id": "1807.06722", "filepath": "papers/1807.06722.pdf", "title": "Machine Learning Interpretability: A Science rather than a tool", "abstract": "The term \"interpretability\" is oftenly used by machine learning researchers\neach with their own intuitive understanding of it. There is no universal well\nagreed upon definition of interpretability in machine learning. As any type of\nscience discipline is mainly driven by the set of formulated questions rather\nthan by different tools in that discipline, e.g. astrophysics is the discipline\nthat learns the composition of stars, not as the discipline that use the\nspectroscopes. Similarly, we propose that machine learning interpretability\nshould be a discipline that answers specific questions related to\ninterpretability. These questions can be of statistical, causal and\ncounterfactual nature. Therefore, there is a need to look into the\ninterpretability problem of machine learning in the context of questions that\nneed to be addressed rather than different tools. We discuss about a\nhypothetical interpretability framework driven by a question based scientific\napproach rather than some specific machine learning model. Using a question\nbased notion of interpretability, we can step towards understanding the science\nof machine learning rather than its engineering. This notion will also help us\nunderstanding any specific problem more in depth rather than relying solely on\nmachine learning methods.", "authors": ["Abdul Karim", "Avinash Mishra", "MA Hakim Newton", "Abdul Sattar"], "published": "2018-07-18T00:50:18+00:00", "updated": "2018-07-25T07:23:45+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1807.06722v2", "entry_id": "http://arxiv.org/abs/1807.06722v2"}, {"id": "2007.05479", "filepath": "papers/2007.05479.pdf", "title": "Impact of Legal Requirements on Explainability in Machine Learning", "abstract": "The requirements on explainability imposed by European laws and their\nimplications for machine learning (ML) models are not always clear. In that\nperspective, our research analyzes explanation obligations imposed for private\nand public decision-making, and how they can be implemented by machine learning\ntechniques.", "authors": ["Adrien Bibal", "Michael Lognoul", "Alexandre de Streel", "Beno\u00eet Fr\u00e9nay"], "published": "2020-07-10T16:57:18+00:00", "updated": "2020-07-10T16:57:18+00:00", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2007.05479v1", "entry_id": "http://arxiv.org/abs/2007.05479v1"}, {"id": "1910.12387", "filepath": "papers/1910.12387.pdf", "title": "Components of Machine Learning: Binding Bits and FLOPS", "abstract": "Many machine learning problems and methods are combinations of three\ncomponents: data, hypothesis space and loss function. Different machine\nlearning methods are obtained as combinations of different choices for the\nrepresentation of data, hypothesis space and loss function. After reviewing the\nmathematical structure of these three components, we discuss intrinsic\ntrade-offs between statistical and computational properties of machine learning\nmethods.", "authors": ["Alexander Jung"], "published": "2019-10-25T17:33:33+00:00", "updated": "2019-10-30T07:59:02+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/1910.12387v2", "entry_id": "http://arxiv.org/abs/1910.12387v2"}, {"id": "1706.08001", "filepath": "papers/1706.08001.pdf", "title": "Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of learning relational order via reinforcement learning procedure?", "abstract": "In this article, we extend the conventional framework of\nconvolutional-Restricted-Boltzmann-Machine to learn highly abstract features\namong abitrary number of time related input maps by constructing a layer of\nmultiplicative units, which capture the relations among inputs. In many cases,\nmore than two maps are strongly related, so it is wise to make multiplicative\nunit learn relations among more input maps, in other words, to find the optimal\nrelational-order of each unit. In order to enable our machine to learn\nrelational order, we developed a reinforcement-learning method whose optimality\nis proven to train the network.", "authors": ["Zizhuang Wang"], "published": "2017-06-24T20:56:27+00:00", "updated": "2017-06-24T20:56:27+00:00", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1706.08001v1", "entry_id": "http://arxiv.org/abs/1706.08001v1"}, {"id": "2004.05366", "filepath": "papers/2004.05366.pdf", "title": "In-Machine-Learning Database: Reimagining Deep Learning with Old-School SQL", "abstract": "In-database machine learning has been very popular, almost being a cliche.\nHowever, can we do it the other way around? In this work, we say \"yes\" by\napplying plain old SQL to deep learning, in a sense implementing deep learning\nalgorithms with SQL. Most deep learning frameworks, as well as generic machine\nlearning ones, share a de facto standard of multidimensional array operations,\nunderneath fancier infrastructure such as automatic differentiation. As SQL\ntables can be regarded as generalisations of (multi-dimensional) arrays, we\nhave found a way to express common deep learning operations in SQL, encouraging\na different way of thinking and thus potentially novel models. In particular,\none of the latest trend in deep learning was the introduction of sparsity in\nthe name of graph convolutional networks, whereas we take sparsity almost for\ngranted in the database world. As both databases and machine learning involve\ntransformation of datasets, we hope this work can inspire further works\nutilizing the large body of existing wisdom, algorithms and technologies in the\ndatabase field to advance the state of the art in machine learning, rather than\nmerely integerating machine learning into databases.", "authors": ["Len Du"], "published": "2020-04-11T11:00:26+00:00", "updated": "2020-04-14T18:08:28+00:00", "categories": ["cs.LG", "cs.DB", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2004.05366v2", "entry_id": "http://arxiv.org/abs/2004.05366v2"}, {"id": "1711.06552", "filepath": "papers/1711.06552.pdf", "title": "Introduction to intelligent computing unit 1", "abstract": "This brief note highlights some basic concepts required toward understanding\nthe evolution of machine learning and deep learning models. The note starts\nwith an overview of artificial intelligence and its relationship to biological\nneuron that ultimately led to the evolution of todays intelligent models.", "authors": ["Isa Inuwa-Dutse"], "published": "2017-11-15T16:52:48+00:00", "updated": "2017-11-15T16:52:48+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1711.06552v1", "entry_id": "http://arxiv.org/abs/1711.06552v1"}, {"id": "1707.04849", "filepath": "papers/1707.04849.pdf", "title": "Minimax deviation strategies for machine learning and recognition with short learning samples", "abstract": "The article is devoted to the problem of small learning samples in machine\nlearning. The flaws of maximum likelihood learning and minimax learning are\nlooked into and the concept of minimax deviation learning is introduced that is\nfree of those flaws.", "authors": ["Michail Schlesinger", "Evgeniy Vodolazskiy"], "published": "2017-07-16T09:15:08+00:00", "updated": "2017-07-16T09:15:08+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/1707.04849v1", "entry_id": "http://arxiv.org/abs/1707.04849v1"}, {"id": "2401.11351", "filepath": "papers/2401.11351.pdf", "title": "A comprehensive review of Quantum Machine Learning: from NISQ to Fault Tolerance", "abstract": "Quantum machine learning, which involves running machine learning algorithms\non quantum devices, has garnered significant attention in both academic and\nbusiness circles. In this paper, we offer a comprehensive and unbiased review\nof the various concepts that have emerged in the field of quantum machine\nlearning. This includes techniques used in Noisy Intermediate-Scale Quantum\n(NISQ) technologies and approaches for algorithms compatible with\nfault-tolerant quantum computing hardware. Our review covers fundamental\nconcepts, algorithms, and the statistical learning theory pertinent to quantum\nmachine learning.", "authors": ["Yunfei Wang", "Junyu Liu"], "published": "2024-01-21T00:19:16+00:00", "updated": "2024-03-31T00:32:13+00:00", "categories": ["quant-ph", "cs.AI", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2401.11351v2", "entry_id": "http://arxiv.org/abs/2401.11351v2"}, {"id": "1603.02185", "filepath": "papers/1603.02185.pdf", "title": "Distributed Multi-Task Learning with Shared Representation", "abstract": "We study the problem of distributed multi-task learning with shared\nrepresentation, where each machine aims to learn a separate, but related, task\nin an unknown shared low-dimensional subspaces, i.e. when the predictor matrix\nhas low rank. We consider a setting where each task is handled by a different\nmachine, with samples for the task available locally on the machine, and study\ncommunication-efficient methods for exploiting the shared structure.", "authors": ["Jialei Wang", "Mladen Kolar", "Nathan Srebro"], "published": "2016-03-07T18:11:54+00:00", "updated": "2016-03-07T18:11:54+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1603.02185v1", "entry_id": "http://arxiv.org/abs/1603.02185v1"}, {"id": "1810.03548", "filepath": "papers/1810.03548.pdf", "title": "Meta-Learning: A Survey", "abstract": "Meta-learning, or learning to learn, is the science of systematically\nobserving how different machine learning approaches perform on a wide range of\nlearning tasks, and then learning from this experience, or meta-data, to learn\nnew tasks much faster than otherwise possible. Not only does this dramatically\nspeed up and improve the design of machine learning pipelines or neural\narchitectures, it also allows us to replace hand-engineered algorithms with\nnovel approaches learned in a data-driven way. In this chapter, we provide an\noverview of the state of the art in this fascinating and continuously evolving\nfield.", "authors": ["Joaquin Vanschoren"], "published": "2018-10-08T16:07:11+00:00", "updated": "2018-10-08T16:07:11+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1810.03548v1", "entry_id": "http://arxiv.org/abs/1810.03548v1"}, {"id": "1903.08801", "filepath": "papers/1903.08801.pdf", "title": "A Unified Analytical Framework for Trustable Machine Learning and Automation Running with Blockchain", "abstract": "Traditional machine learning algorithms use data from databases that are\nmutable, and therefore the data cannot be fully trusted. Also, the machine\nlearning process is difficult to automate. This paper proposes building a\ntrustable machine learning system by using blockchain technology, which can\nstore data in a permanent and immutable way. In addition, smart contracts are\nused to automate the machine learning process. This paper makes three\ncontributions. First, it establishes a link between machine learning technology\nand blockchain technology. Previously, machine learning and blockchain have\nbeen considered two independent technologies without an obvious link. Second,\nit proposes a unified analytical framework for trustable machine learning by\nusing blockchain technology. This unified framework solves both the\ntrustability and automation issues in machine learning. Third, it enables a\ncomputer to translate core machine learning implementation from a single thread\non a single machine to multiple threads on multiple machines running with\nblockchain by using a unified approach. The paper uses association rule mining\nas an example to demonstrate how trustable machine learning can be implemented\nwith blockchain, and it shows how this approach can be used to analyze opioid\nprescriptions to help combat the opioid crisis.", "authors": ["Tao Wang"], "published": "2019-03-21T02:17:08+00:00", "updated": "2019-03-21T02:17:08+00:00", "categories": ["cs.LG", "cs.CR"], "pdf_url": "http://arxiv.org/pdf/1903.08801v1", "entry_id": "http://arxiv.org/abs/1903.08801v1"}, {"id": "2303.18087", "filepath": "papers/2303.18087.pdf", "title": "Evaluation Challenges for Geospatial ML", "abstract": "As geospatial machine learning models and maps derived from their predictions\nare increasingly used for downstream analyses in science and policy, it is\nimperative to evaluate their accuracy and applicability. Geospatial machine\nlearning has key distinctions from other learning paradigms, and as such, the\ncorrect way to measure performance of spatial machine learning outputs has been\na topic of debate. In this paper, I delineate unique challenges of model\nevaluation for geospatial machine learning with global or remotely sensed\ndatasets, culminating in concrete takeaways to improve evaluations of\ngeospatial model performance.", "authors": ["Esther Rolf"], "published": "2023-03-31T14:24:06+00:00", "updated": "2023-03-31T14:24:06+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2303.18087v1", "entry_id": "http://arxiv.org/abs/2303.18087v1"}, {"id": "0904.3664", "filepath": "papers/0904.3664.pdf", "title": "Introduction to Machine Learning: Class Notes 67577", "abstract": "Introduction to Machine learning covering Statistical Inference (Bayes, EM,\nML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),\nand PAC learning (the Formal model, VC dimension, Double Sampling theorem).", "authors": ["Amnon Shashua"], "published": "2009-04-23T11:40:57+00:00", "updated": "2009-04-23T11:40:57+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/0904.3664v1", "entry_id": "http://arxiv.org/abs/0904.3664v1"}, {"id": "1911.00776", "filepath": "papers/1911.00776.pdf", "title": "Ten-year Survival Prediction for Breast Cancer Patients", "abstract": "This report assesses different machine learning approaches to 10-year\nsurvival prediction of breast cancer patients.", "authors": ["Changmao Li", "Han He", "Yunze Hao", "Caleb Ziems"], "published": "2019-11-02T19:53:32+00:00", "updated": "2019-11-02T19:53:32+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1911.00776v1", "entry_id": "http://arxiv.org/abs/1911.00776v1"}, {"id": "1908.04710", "filepath": "papers/1908.04710.pdf", "title": "metric-learn: Metric Learning Algorithms in Python", "abstract": "metric-learn is an open source Python package implementing supervised and\nweakly-supervised distance metric learning algorithms. As part of\nscikit-learn-contrib, it provides a unified interface compatible with\nscikit-learn which allows to easily perform cross-validation, model selection,\nand pipelining with other machine learning estimators. metric-learn is\nthoroughly tested and available on PyPi under the MIT licence.", "authors": ["William de Vazelhes", "CJ Carey", "Yuan Tang", "Nathalie Vauquier", "Aur\u00e9lien Bellet"], "published": "2019-08-13T15:52:31+00:00", "updated": "2020-07-27T14:47:52+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1908.04710v3", "entry_id": "http://arxiv.org/abs/1908.04710v3"}, {"id": "2405.03720", "filepath": "papers/2405.03720.pdf", "title": "Spatial Transfer Learning with Simple MLP", "abstract": "First step to investigate the potential of transfer learning applied to the\nfield of spatial statistics", "authors": ["Hongjian Yang"], "published": "2024-05-05T20:39:15+00:00", "updated": "2024-05-05T20:39:15+00:00", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2405.03720v1", "entry_id": "http://arxiv.org/abs/2405.03720v1"}, {"id": "2106.07032", "filepath": "papers/2106.07032.pdf", "title": "Category Theory in Machine Learning", "abstract": "Over the past two decades machine learning has permeated almost every realm\nof technology. At the same time, many researchers have begun using category\ntheory as a unifying language, facilitating communication between different\nscientific disciplines. It is therefore unsurprising that there is a burgeoning\ninterest in applying category theory to machine learning. We aim to document\nthe motivations, goals and common themes across these applications. We touch on\ngradient-based learning, probability, and equivariant learning.", "authors": ["Dan Shiebler", "Bruno Gavranovi\u0107", "Paul Wilson"], "published": "2021-06-13T15:58:13+00:00", "updated": "2021-06-13T15:58:13+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/2106.07032v1", "entry_id": "http://arxiv.org/abs/2106.07032v1"}, {"id": "2102.05639", "filepath": "papers/2102.05639.pdf", "title": "Energy-Harvesting Distributed Machine Learning", "abstract": "This paper provides a first study of utilizing energy harvesting for\nsustainable machine learning in distributed networks. We consider a distributed\nlearning setup in which a machine learning model is trained over a large number\nof devices that can harvest energy from the ambient environment, and develop a\npractical learning framework with theoretical convergence guarantees. We\ndemonstrate through numerical experiments that the proposed framework can\nsignificantly outperform energy-agnostic benchmarks. Our framework is scalable,\nrequires only local estimation of the energy statistics, and can be applied to\na wide range of distributed training settings, including machine learning in\nwireless networks, edge computing, and mobile internet of things.", "authors": ["Basak Guler", "Aylin Yener"], "published": "2021-02-10T18:53:51+00:00", "updated": "2021-02-10T18:53:51+00:00", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2102.05639v1", "entry_id": "http://arxiv.org/abs/2102.05639v1"}, {"id": "2011.11819", "filepath": "papers/2011.11819.pdf", "title": "When Machine Learning Meets Privacy: A Survey and Outlook", "abstract": "The newly emerged machine learning (e.g. deep learning) methods have become a\nstrong driving force to revolutionize a wide range of industries, such as smart\nhealthcare, financial technology, and surveillance systems. Meanwhile, privacy\nhas emerged as a big concern in this machine learning-based artificial\nintelligence era. It is important to note that the problem of privacy\npreservation in the context of machine learning is quite different from that in\ntraditional data privacy protection, as machine learning can act as both friend\nand foe. Currently, the work on the preservation of privacy and machine\nlearning (ML) is still in an infancy stage, as most existing solutions only\nfocus on privacy problems during the machine learning process. Therefore, a\ncomprehensive study on the privacy preservation problems and machine learning\nis required. This paper surveys the state of the art in privacy issues and\nsolutions for machine learning. The survey covers three categories of\ninteractions between privacy and machine learning: (i) private machine\nlearning, (ii) machine learning aided privacy protection, and (iii) machine\nlearning-based privacy attack and corresponding protection schemes. The current\nresearch progress in each category is reviewed and the key challenges are\nidentified. Finally, based on our in-depth analysis of the area of privacy and\nmachine learning, we point out future research directions in this field.", "authors": ["Bo Liu", "Ming Ding", "Sina Shaham", "Wenny Rahayu", "Farhad Farokhi", "Zihuai Lin"], "published": "2020-11-24T00:52:49+00:00", "updated": "2020-11-24T00:52:49+00:00", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf_url": "http://arxiv.org/pdf/2011.11819v1", "entry_id": "http://arxiv.org/abs/2011.11819v1"}, {"id": "1207.4676", "filepath": "papers/1207.4676.pdf", "title": "Proceedings of the 29th International Conference on Machine Learning (ICML-12)", "abstract": "This is an index to the papers that appear in the Proceedings of the 29th\nInternational Conference on Machine Learning (ICML-12). The conference was held\nin Edinburgh, Scotland, June 27th - July 3rd, 2012.", "authors": ["John Langford", "Joelle Pineau"], "published": "2012-07-19T14:08:22+00:00", "updated": "2012-09-16T11:24:54+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1207.4676v2", "entry_id": "http://arxiv.org/abs/1207.4676v2"}, {"id": "2004.00993", "filepath": "papers/2004.00993.pdf", "title": "Augmented Q Imitation Learning (AQIL)", "abstract": "The study of unsupervised learning can be generally divided into two\ncategories: imitation learning and reinforcement learning. In imitation\nlearning the machine learns by mimicking the behavior of an expert system\nwhereas in reinforcement learning the machine learns via direct environment\nfeedback. Traditional deep reinforcement learning takes a significant time\nbefore the machine starts to converge to an optimal policy. This paper proposes\nAugmented Q-Imitation-Learning, a method by which deep reinforcement learning\nconvergence can be accelerated by applying Q-imitation-learning as the initial\ntraining process in traditional Deep Q-learning.", "authors": ["Xiao Lei Zhang", "Anish Agarwal"], "published": "2020-03-31T18:08:23+00:00", "updated": "2020-04-05T17:16:23+00:00", "categories": ["cs.LG", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2004.00993v2", "entry_id": "http://arxiv.org/abs/2004.00993v2"}, {"id": "2204.07492", "filepath": "papers/2204.07492.pdf", "title": "A Machine Learning Tutorial for Operational Meteorology, Part I: Traditional Machine Learning", "abstract": "Recently, the use of machine learning in meteorology has increased greatly.\nWhile many machine learning methods are not new, university classes on machine\nlearning are largely unavailable to meteorology students and are not required\nto become a meteorologist. The lack of formal instruction has contributed to\nperception that machine learning methods are 'black boxes' and thus end-users\nare hesitant to apply the machine learning methods in their every day workflow.\nTo reduce the opaqueness of machine learning methods and lower hesitancy\ntowards machine learning in meteorology, this paper provides a survey of some\nof the most common machine learning methods. A familiar meteorological example\nis used to contextualize the machine learning methods while also discussing\nmachine learning topics using plain language. The following machine learning\nmethods are demonstrated: linear regression; logistic regression; decision\ntrees; random forest; gradient boosted decision trees; naive Bayes; and support\nvector machines. Beyond discussing the different methods, the paper also\ncontains discussions on the general machine learning process as well as best\npractices to enable readers to apply machine learning to their own datasets.\nFurthermore, all code (in the form of Jupyter notebooks and Google Colaboratory\nnotebooks) used to make the examples in the paper is provided in an effort to\ncatalyse the use of machine learning in meteorology.", "authors": ["Randy J. Chase", "David R. Harrison", "Amanda Burke", "Gary M. Lackmann", "Amy McGovern"], "published": "2022-04-15T14:48:04+00:00", "updated": "2022-06-07T14:48:09+00:00", "categories": ["physics.ao-ph", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2204.07492v2", "entry_id": "http://arxiv.org/abs/2204.07492v2"}, {"id": "1802.03830", "filepath": "papers/1802.03830.pdf", "title": "Distributed Stochastic Multi-Task Learning with Graph Regularization", "abstract": "We propose methods for distributed graph-based multi-task learning that are\nbased on weighted averaging of messages from other machines. Uniform averaging\nor diminishing stepsize in these methods would yield consensus (single task)\nlearning. We show how simply skewing the averaging weights or controlling the\nstepsize allows learning different, but related, tasks on the different\nmachines.", "authors": ["Weiran Wang", "Jialei Wang", "Mladen Kolar", "Nathan Srebro"], "published": "2018-02-11T22:23:34+00:00", "updated": "2018-02-11T22:23:34+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1802.03830v1", "entry_id": "http://arxiv.org/abs/1802.03830v1"}, {"id": "2009.11087", "filepath": "papers/2009.11087.pdf", "title": "Probabilistic Machine Learning for Healthcare", "abstract": "Machine learning can be used to make sense of healthcare data. Probabilistic\nmachine learning models help provide a complete picture of observed data in\nhealthcare. In this review, we examine how probabilistic machine learning can\nadvance healthcare. We consider challenges in the predictive model building\npipeline where probabilistic models can be beneficial including calibration and\nmissing data. Beyond predictive models, we also investigate the utility of\nprobabilistic machine learning models in phenotyping, in generative models for\nclinical use cases, and in reinforcement learning.", "authors": ["Irene Y. Chen", "Shalmali Joshi", "Marzyeh Ghassemi", "Rajesh Ranganath"], "published": "2020-09-23T12:14:05+00:00", "updated": "2020-09-23T12:14:05+00:00", "categories": ["stat.ML", "cs.CY", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2009.11087v1", "entry_id": "http://arxiv.org/abs/2009.11087v1"}, {"id": "1909.09248", "filepath": "papers/1909.09248.pdf", "title": "Representation Learning for Electronic Health Records", "abstract": "Information in electronic health records (EHR), such as clinical narratives,\nexamination reports, lab measurements, demographics, and other patient\nencounter entries, can be transformed into appropriate data representations\nthat can be used for downstream clinical machine learning tasks using\nrepresentation learning. Learning better representations is critical to improve\nthe performance of downstream tasks. Due to the advances in machine learning,\nwe now can learn better and meaningful representations from EHR through\ndisentangling the underlying factors inside data and distilling large amounts\nof information and knowledge from heterogeneous EHR sources. In this chapter,\nwe first introduce the background of learning representations and reasons why\nwe need good EHR representations in machine learning for medicine and\nhealthcare in Section 1. Next, we explain the commonly-used machine learning\nand evaluation methods for representation learning using a deep learning\napproach in Section 2. Following that, we review recent related studies of\nlearning patient state representation from EHR for clinical machine learning\ntasks in Section 3. Finally, in Section 4 we discuss more techniques, studies,\nand challenges for learning natural language representations when free texts,\nsuch as clinical notes, examination reports, or biomedical literature are used.\nWe also discuss challenges and opportunities in these rapidly growing research\nfields.", "authors": ["Wei-Hung Weng", "Peter Szolovits"], "published": "2019-09-19T22:12:30+00:00", "updated": "2019-09-19T22:12:30+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1909.09248v1", "entry_id": "http://arxiv.org/abs/1909.09248v1"}, {"id": "1906.06821", "filepath": "papers/1906.06821.pdf", "title": "A Survey of Optimization Methods from a Machine Learning Perspective", "abstract": "Machine learning develops rapidly, which has made many theoretical\nbreakthroughs and is widely applied in various fields. Optimization, as an\nimportant part of machine learning, has attracted much attention of\nresearchers. With the exponential growth of data amount and the increase of\nmodel complexity, optimization methods in machine learning face more and more\nchallenges. A lot of work on solving optimization problems or improving\noptimization methods in machine learning has been proposed successively. The\nsystematic retrospect and summary of the optimization methods from the\nperspective of machine learning are of great significance, which can offer\nguidance for both developments of optimization and machine learning research.\nIn this paper, we first describe the optimization problems in machine learning.\nThen, we introduce the principles and progresses of commonly used optimization\nmethods. Next, we summarize the applications and developments of optimization\nmethods in some popular machine learning fields. Finally, we explore and give\nsome challenges and open problems for the optimization in machine learning.", "authors": ["Shiliang Sun", "Zehui Cao", "Han Zhu", "Jing Zhao"], "published": "2019-06-17T02:54:51+00:00", "updated": "2019-10-23T08:26:31+00:00", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1906.06821v2", "entry_id": "http://arxiv.org/abs/1906.06821v2"}, {"id": "2412.00464", "filepath": "papers/2412.00464.pdf", "title": "On the Conditions for Domain Stability for Machine Learning: a Mathematical Approach", "abstract": "This work proposes a mathematical approach that (re)defines a property of\nMachine Learning models named stability and determines sufficient conditions to\nvalidate it. Machine Learning models are represented as functions, and the\ncharacteristics in scope depend upon the domain of the function, what allows us\nto adopt topological and metric spaces theory as a basis. Finally, this work\nprovides some equivalences useful to prove and test stability in Machine\nLearning models. The results suggest that whenever stability is aligned with\nthe notion of function smoothness, then the stability of Machine Learning\nmodels primarily depends upon certain topological, measurable properties of the\nclassification sets within the ML model domain.", "authors": ["Gabriel Pedroza"], "published": "2024-11-30T12:57:07+00:00", "updated": "2024-11-30T12:57:07+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML", "F.4.1; I.2.0"], "pdf_url": "http://arxiv.org/pdf/2412.00464v1", "entry_id": "http://arxiv.org/abs/2412.00464v1"}, {"id": "1510.00633", "filepath": "papers/1510.00633.pdf", "title": "Distributed Multitask Learning", "abstract": "We consider the problem of distributed multi-task learning, where each\nmachine learns a separate, but related, task. Specifically, each machine learns\na linear predictor in high-dimensional space,where all tasks share the same\nsmall support. We present a communication-efficient estimator based on the\ndebiased lasso and show that it is comparable with the optimal centralized\nmethod.", "authors": ["Jialei Wang", "Mladen Kolar", "Nathan Srebro"], "published": "2015-10-02T16:15:30+00:00", "updated": "2015-10-02T16:15:30+00:00", "categories": ["stat.ML", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1510.00633v1", "entry_id": "http://arxiv.org/abs/1510.00633v1"}, {"id": "1908.00868", "filepath": "papers/1908.00868.pdf", "title": "Machine Learning as Ecology", "abstract": "Machine learning methods have had spectacular success on numerous problems.\nHere we show that a prominent class of learning algorithms - including Support\nVector Machines (SVMs) -- have a natural interpretation in terms of ecological\ndynamics. We use these ideas to design new online SVM algorithms that exploit\necological invasions, and benchmark performance using the MNIST dataset. Our\nwork provides a new ecological lens through which we can view statistical\nlearning and opens the possibility of designing ecosystems for machine\nlearning.\n  Supplemental code is found at https://github.com/owenhowell20/EcoSVM.", "authors": ["Owen Howell", "Cui Wenping", "Robert Marsland III", "Pankaj Mehta"], "published": "2019-08-02T14:08:17+00:00", "updated": "2019-08-23T13:52:08+00:00", "categories": ["cs.LG", "cond-mat.stat-mech", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1908.00868v2", "entry_id": "http://arxiv.org/abs/1908.00868v2"}, {"id": "1605.07805", "filepath": "papers/1605.07805.pdf", "title": "Learning Moore Machines from Input-Output Traces", "abstract": "The problem of learning automata from example traces (but no equivalence or\nmembership queries) is fundamental in automata learning theory and practice. In\nthis paper we study this problem for finite state machines with inputs and\noutputs, and in particular for Moore machines. We develop three algorithms for\nsolving this problem: (1) the PTAP algorithm, which transforms a set of\ninput-output traces into an incomplete Moore machine and then completes the\nmachine with self-loops; (2) the PRPNI algorithm, which uses the well-known\nRPNI algorithm for automata learning to learn a product of automata encoding a\nMoore machine; and (3) the MooreMI algorithm, which directly learns a Moore\nmachine using PTAP extended with state merging. We prove that MooreMI has the\nfundamental identification in the limit property. We also compare the\nalgorithms experimentally in terms of the size of the learned machine and\nseveral notions of accuracy, introduced in this paper. Finally, we compare with\nOSTIA, an algorithm that learns a more general class of transducers, and find\nthat OSTIA generally does not learn a Moore machine, even when fed with a\ncharacteristic sample.", "authors": ["Georgios Giantamidis", "Stavros Tripakis"], "published": "2016-05-25T10:11:03+00:00", "updated": "2016-09-02T09:27:40+00:00", "categories": ["cs.FL", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/1605.07805v2", "entry_id": "http://arxiv.org/abs/1605.07805v2"}, {"id": "1803.10311", "filepath": "papers/1803.10311.pdf", "title": "How Developers Iterate on Machine Learning Workflows -- A Survey of the Applied Machine Learning Literature", "abstract": "Machine learning workflow development is anecdotally regarded to be an\niterative process of trial-and-error with humans-in-the-loop. However, we are\nnot aware of quantitative evidence corroborating this popular belief. A\nquantitative characterization of iteration can serve as a benchmark for machine\nlearning workflow development in practice, and can aid the development of\nhuman-in-the-loop machine learning systems. To this end, we conduct a\nsmall-scale survey of the applied machine learning literature from five\ndistinct application domains. We collect and distill statistics on the role of\niteration within machine learning workflow development, and report preliminary\ntrends and insights from our investigation, as a starting point towards this\nbenchmark. Based on our findings, we finally describe desiderata for effective\nand versatile human-in-the-loop machine learning systems that can cater to\nusers in diverse domains.", "authors": ["Doris Xin", "Litian Ma", "Shuchen Song", "Aditya Parameswaran"], "published": "2018-03-27T20:38:05+00:00", "updated": "2018-05-17T22:16:31+00:00", "categories": ["cs.LG", "cs.DB", "cs.HC", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1803.10311v2", "entry_id": "http://arxiv.org/abs/1803.10311v2"}, {"id": "2303.09491", "filepath": "papers/2303.09491.pdf", "title": "Challenges and Opportunities in Quantum Machine Learning", "abstract": "At the intersection of machine learning and quantum computing, Quantum\nMachine Learning (QML) has the potential of accelerating data analysis,\nespecially for quantum data, with applications for quantum materials,\nbiochemistry, and high-energy physics. Nevertheless, challenges remain\nregarding the trainability of QML models. Here we review current methods and\napplications for QML. We highlight differences between quantum and classical\nmachine learning, with a focus on quantum neural networks and quantum deep\nlearning. Finally, we discuss opportunities for quantum advantage with QML.", "authors": ["M. Cerezo", "Guillaume Verdon", "Hsin-Yuan Huang", "Lukasz Cincio", "Patrick J. Coles"], "published": "2023-03-16T17:10:39+00:00", "updated": "2023-03-16T17:10:39+00:00", "categories": ["quant-ph", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2303.09491v1", "entry_id": "http://arxiv.org/abs/2303.09491v1"}, {"id": "2003.10146", "filepath": "papers/2003.10146.pdf", "title": "Julia Language in Machine Learning: Algorithms, Applications, and Open Issues", "abstract": "Machine learning is driving development across many fields in science and\nengineering. A simple and efficient programming language could accelerate\napplications of machine learning in various fields. Currently, the programming\nlanguages most commonly used to develop machine learning algorithms include\nPython, MATLAB, and C/C ++. However, none of these languages well balance both\nefficiency and simplicity. The Julia language is a fast, easy-to-use, and\nopen-source programming language that was originally designed for\nhigh-performance computing, which can well balance the efficiency and\nsimplicity. This paper summarizes the related research work and developments in\nthe application of the Julia language in machine learning. It first surveys the\npopular machine learning algorithms that are developed in the Julia language.\nThen, it investigates applications of the machine learning algorithms\nimplemented with the Julia language. Finally, it discusses the open issues and\nthe potential future directions that arise in the use of the Julia language in\nmachine learning.", "authors": ["Kaifeng Gao", "Gang Mei", "Francesco Piccialli", "Salvatore Cuomo", "Jingzhi Tu", "Zenan Huo"], "published": "2020-03-23T09:31:02+00:00", "updated": "2020-05-17T10:52:22+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2003.10146v2", "entry_id": "http://arxiv.org/abs/2003.10146v2"}, {"id": "2006.15680", "filepath": "papers/2006.15680.pdf", "title": "Modeling Generalization in Machine Learning: A Methodological and Computational Study", "abstract": "As machine learning becomes more and more available to the general public,\ntheoretical questions are turning into pressing practical issues. Possibly, one\nof the most relevant concerns is the assessment of our confidence in trusting\nmachine learning predictions. In many real-world cases, it is of utmost\nimportance to estimate the capabilities of a machine learning algorithm to\ngeneralize, i.e., to provide accurate predictions on unseen data, depending on\nthe characteristics of the target problem. In this work, we perform a\nmeta-analysis of 109 publicly-available classification data sets, modeling\nmachine learning generalization as a function of a variety of data set\ncharacteristics, ranging from number of samples to intrinsic dimensionality,\nfrom class-wise feature skewness to $F1$ evaluated on test samples falling\noutside the convex hull of the training set. Experimental results demonstrate\nthe relevance of using the concept of the convex hull of the training data in\nassessing machine learning generalization, by emphasizing the difference\nbetween interpolated and extrapolated predictions. Besides several predictable\ncorrelations, we observe unexpectedly weak associations between the\ngeneralization ability of machine learning models and all metrics related to\ndimensionality, thus challenging the common assumption that the \\textit{curse\nof dimensionality} might impair generalization in machine learning.", "authors": ["Pietro Barbiero", "Giovanni Squillero", "Alberto Tonda"], "published": "2020-06-28T19:06:16+00:00", "updated": "2020-06-28T19:06:16+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2006.15680v1", "entry_id": "http://arxiv.org/abs/2006.15680v1"}, {"id": "2407.05520", "filepath": "papers/2407.05520.pdf", "title": "A Theory of Machine Learning", "abstract": "We critically review three major theories of machine learning and provide a\nnew theory according to which machines learn a function when the machines\nsuccessfully compute it. We show that this theory challenges common assumptions\nin the statistical and the computational learning theories, for it implies that\nlearning true probabilities is equivalent neither to obtaining a correct\ncalculation of the true probabilities nor to obtaining an almost-sure\nconvergence to them. We also briefly discuss some case studies from natural\nlanguage processing and macroeconomics from the perspective of the new theory.", "authors": ["Jinsook Kim", "Jinho Kang"], "published": "2024-07-07T23:57:10+00:00", "updated": "2024-07-07T23:57:10+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2407.05520v1", "entry_id": "http://arxiv.org/abs/2407.05520v1"}, {"id": "2209.02057", "filepath": "papers/2209.02057.pdf", "title": "Applying Machine Learning to Life Insurance: some knowledge sharing to master it", "abstract": "Machine Learning permeates many industries, which brings new source of\nbenefits for companies. However within the life insurance industry, Machine\nLearning is not widely used in practice as over the past years statistical\nmodels have shown their efficiency for risk assessment. Thus insurers may face\ndifficulties to assess the value of the artificial intelligence. Focusing on\nthe modification of the life insurance industry over time highlights the stake\nof using Machine Learning for insurers and benefits that it can bring by\nunleashing data value. This paper reviews traditional actuarial methodologies\nfor survival modeling and extends them with Machine Learning techniques. It\npoints out differences with regular machine learning models and emphasizes\nimportance of specific implementations to face censored data with machine\nlearning models family. In complement to this article, a Python library has\nbeen developed. Different open-source Machine Learning algorithms have been\nadjusted to adapt the specificities of life insurance data, namely censoring\nand truncation. Such models can be easily applied from this SCOR library to\naccurately model life insurance risks.", "authors": ["Antoine Chancel", "Laura Bradier", "Antoine Ly", "Razvan Ionescu", "Laurene Martin", "Marguerite Sauce"], "published": "2022-09-05T17:09:03+00:00", "updated": "2022-09-27T18:42:50+00:00", "categories": ["stat.ML", "cs.CY", "cs.LG", "stat.AP"], "pdf_url": "http://arxiv.org/pdf/2209.02057v3", "entry_id": "http://arxiv.org/abs/2209.02057v3"}, {"id": "2312.14050", "filepath": "papers/2312.14050.pdf", "title": "Machine learning and domain decomposition methods -- a survey", "abstract": "Hybrid algorithms, which combine black-box machine learning methods with\nexperience from traditional numerical methods and domain expertise from diverse\napplication areas, are progressively gaining importance in scientific machine\nlearning and various industrial domains, especially in computational science\nand engineering. In the present survey, several promising avenues of research\nwill be examined which focus on the combination of machine learning (ML) and\ndomain decomposition methods (DDMs). The aim of this survey is to provide an\noverview of existing work within this field and to structure it into domain\ndecomposition for machine learning and machine learning-enhanced domain\ndecomposition, including: domain decomposition for classical machine learning,\ndomain decomposition to accelerate the training of physics-aware neural\nnetworks, machine learning to enhance the convergence properties or\ncomputational efficiency of DDMs, and machine learning as a discretization\nmethod in a DDM for the solution of PDEs. In each of these fields, we summarize\nexisting work and key advances within a common framework and, finally, disuss\nongoing challenges and opportunities for future research.", "authors": ["Axel Klawonn", "Martin Lanser", "Janine Weber"], "published": "2023-12-21T17:19:27+00:00", "updated": "2023-12-21T17:19:27+00:00", "categories": ["math.NA", "cs.LG", "cs.NA", "65F10, 65N22, 65N55, 68T05, 68T07"], "pdf_url": "http://arxiv.org/pdf/2312.14050v1", "entry_id": "http://arxiv.org/abs/2312.14050v1"}, {"id": "2409.03632", "filepath": "papers/2409.03632.pdf", "title": "Beyond Model Interpretability: Socio-Structural Explanations in Machine Learning", "abstract": "What is it to interpret the outputs of an opaque machine learning model. One\napproach is to develop interpretable machine learning techniques. These\ntechniques aim to show how machine learning models function by providing either\nmodel centric local or global explanations, which can be based on mechanistic\ninterpretations revealing the inner working mechanisms of models or\nnonmechanistic approximations showing input feature output data relationships.\nIn this paper, we draw on social philosophy to argue that interpreting machine\nlearning outputs in certain normatively salient domains could require appealing\nto a third type of explanation that we call sociostructural explanation. The\nrelevance of this explanation type is motivated by the fact that machine\nlearning models are not isolated entities but are embedded within and shaped by\nsocial structures. Sociostructural explanations aim to illustrate how social\nstructures contribute to and partially explain the outputs of machine learning\nmodels. We demonstrate the importance of sociostructural explanations by\nexamining a racially biased healthcare allocation algorithm. Our proposal\nhighlights the need for transparency beyond model interpretability,\nunderstanding the outputs of machine learning systems could require a broader\nanalysis that extends beyond the understanding of the machine learning model\nitself.", "authors": ["Andrew Smart", "Atoosa Kasirzadeh"], "published": "2024-09-05T15:47:04+00:00", "updated": "2024-09-05T15:47:04+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/2409.03632v1", "entry_id": "http://arxiv.org/abs/2409.03632v1"}, {"id": "2502.01708", "filepath": "papers/2502.01708.pdf", "title": "Aspects of Artificial Intelligence: Transforming Machine Learning Systems Naturally", "abstract": "In this paper, we study the machine learning elements which we are interested\nin together as a machine learning system, consisting of a collection of machine\nlearning elements and a collection of relations between the elements. The\nrelations we concern are algebraic operations, binary relations, and binary\nrelations with composition that can be reasoned categorically. A machine\nlearning system transformation between two systems is a map between the\nsystems, which preserves the relations we concern. The system transformations\ngiven by quotient or clustering, representable functor, and Yoneda embedding\nare highlighted and discussed by machine learning examples. An adjunction\nbetween machine learning systems, a special machine learning system\ntransformation loop, provides the optimal way of solving problems. Machine\nlearning system transformations are linked and compared by their maps at\n2-cell, natural transformations. New insights and structures can be obtained\nfrom universal properties and algebraic structures given by monads, which are\ngenerated from adjunctions.", "authors": ["Xiuzhan Guo"], "published": "2025-02-03T14:45:02+00:00", "updated": "2025-02-03T14:45:02+00:00", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.DM"], "pdf_url": "http://arxiv.org/pdf/2502.01708v1", "entry_id": "http://arxiv.org/abs/2502.01708v1"}, {"id": "1902.04622", "filepath": "papers/1902.04622.pdf", "title": "Learning Theory and Support Vector Machines - a primer", "abstract": "The main goal of statistical learning theory is to provide a fundamental\nframework for the problem of decision making and model construction based on\nsets of data. Here, we present a brief introduction to the fundamentals of\nstatistical learning theory, in particular the difference between empirical and\nstructural risk minimization, including one of its most prominent\nimplementations, i.e. the Support Vector Machine.", "authors": ["Michael Banf"], "published": "2019-02-12T20:28:09+00:00", "updated": "2019-02-12T20:28:09+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1902.04622v1", "entry_id": "http://arxiv.org/abs/1902.04622v1"}, {"id": "1907.03010", "filepath": "papers/1907.03010.pdf", "title": "Financial Time Series Data Processing for Machine Learning", "abstract": "This article studies the financial time series data processing for machine\nlearning. It introduces the most frequent scaling methods, then compares the\nresulting stationarity and preservation of useful information for trend\nforecasting. It proposes an empirical test based on the capability to learn\nsimple data relationship with simple models. It also speaks about the data\nsplit method specific to time series, avoiding unwanted overfitting and\nproposes various labelling for classification and regression.", "authors": ["Fabrice Daniel"], "published": "2019-07-03T15:10:23+00:00", "updated": "2019-07-03T15:10:23+00:00", "categories": ["q-fin.ST", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1907.03010v1", "entry_id": "http://arxiv.org/abs/1907.03010v1"}, {"id": "2103.03122", "filepath": "papers/2103.03122.pdf", "title": "Machine Learning using Stata/Python", "abstract": "We present two related Stata modules, r_ml_stata and c_ml_stata, for fitting\npopular Machine Learning (ML) methods both in regression and classification\nsettings. Using the recent Stata/Python integration platform (sfi) of Stata 16,\nthese commands provide hyper-parameters' optimal tuning via K-fold\ncross-validation using greed search. More specifically, they make use of the\nPython Scikit-learn API to carry out both cross-validation and outcome/label\nprediction.", "authors": ["Giovanni Cerulli"], "published": "2021-03-03T10:31:44+00:00", "updated": "2021-03-03T10:31:44+00:00", "categories": ["stat.CO", "cs.LG", "cs.MS"], "pdf_url": "http://arxiv.org/pdf/2103.03122v1", "entry_id": "http://arxiv.org/abs/2103.03122v1"}, {"id": "2310.11470", "filepath": "papers/2310.11470.pdf", "title": "Classic machine learning methods", "abstract": "In this chapter, we present the main classic machine learning methods. A\nlarge part of the chapter is devoted to supervised learning techniques for\nclassification and regression, including nearest-neighbor methods, linear and\nlogistic regressions, support vector machines and tree-based algorithms. We\nalso describe the problem of overfitting as well as strategies to overcome it.\nWe finally provide a brief overview of unsupervised learning methods, namely\nfor clustering and dimensionality reduction.", "authors": ["Johann Faouzi", "Olivier Colliot"], "published": "2023-05-24T13:38:38+00:00", "updated": "2023-05-24T13:38:38+00:00", "categories": ["cs.LG", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2310.11470v1", "entry_id": "http://arxiv.org/abs/2310.11470v1"}, {"id": "1501.04309", "filepath": "papers/1501.04309.pdf", "title": "Information Theory and its Relation to Machine Learning", "abstract": "In this position paper, I first describe a new perspective on machine\nlearning (ML) by four basic problems (or levels), namely, \"What to learn?\",\n\"How to learn?\", \"What to evaluate?\", and \"What to adjust?\". The paper stresses\nmore on the first level of \"What to learn?\", or \"Learning Target Selection\".\nTowards this primary problem within the four levels, I briefly review the\nexisting studies about the connection between information theoretical learning\n(ITL [1]) and machine learning. A theorem is given on the relation between the\nempirically-defined similarity measure and information measures. Finally, a\nconjecture is proposed for pursuing a unified mathematical interpretation to\nlearning target selection.", "authors": ["Bao-Gang Hu"], "published": "2015-01-18T14:57:02+00:00", "updated": "2015-01-18T14:57:02+00:00", "categories": ["cs.IT", "cs.LG", "math.IT"], "pdf_url": "http://arxiv.org/pdf/1501.04309v1", "entry_id": "http://arxiv.org/abs/1501.04309v1"}, {"id": "1602.00198", "filepath": "papers/1602.00198.pdf", "title": "Discussion on Mechanical Learning and Learning Machine", "abstract": "Mechanical learning is a computing system that is based on a set of simple\nand fixed rules, and can learn from incoming data. A learning machine is a\nsystem that realizes mechanical learning. Importantly, we emphasis that it is\nbased on a set of simple and fixed rules, contrasting to often called machine\nlearning that is sophisticated software based on very complicated mathematical\ntheory, and often needs human intervene for software fine tune and manual\nadjustments. Here, we discuss some basic facts and principles of such system,\nand try to lay down a framework for further study. We propose 2 directions to\napproach mechanical learning, just like Church-Turing pair: one is trying to\nrealize a learning machine, another is trying to well describe the mechanical\nlearning.", "authors": ["Chuyu Xiong"], "published": "2016-01-31T04:05:50+00:00", "updated": "2016-01-31T04:05:50+00:00", "categories": ["cs.AI"], "pdf_url": "http://arxiv.org/pdf/1602.00198v1", "entry_id": "http://arxiv.org/abs/1602.00198v1"}, {"id": "2206.13446", "filepath": "papers/2206.13446.pdf", "title": "Pen and Paper Exercises in Machine Learning", "abstract": "This is a collection of (mostly) pen-and-paper exercises in machine learning.\nThe exercises are on the following topics: linear algebra, optimisation,\ndirected graphical models, undirected graphical models, expressive power of\ngraphical models, factor graphs and message passing, inference for hidden\nMarkov models, model-based learning (including ICA and unnormalised models),\nsampling and Monte-Carlo integration, and variational inference.", "authors": ["Michael U. Gutmann"], "published": "2022-06-27T16:53:18+00:00", "updated": "2022-06-27T16:53:18+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2206.13446v1", "entry_id": "http://arxiv.org/abs/2206.13446v1"}, {"id": "2105.03726", "filepath": "papers/2105.03726.pdf", "title": "Mental Models of Adversarial Machine Learning", "abstract": "Although machine learning is widely used in practice, little is known about\npractitioners' understanding of potential security challenges. In this work, we\nclose this substantial gap and contribute a qualitative study focusing on\ndevelopers' mental models of the machine learning pipeline and potentially\nvulnerable components. Similar studies have helped in other security fields to\ndiscover root causes or improve risk communication. Our study reveals two\n\\facets of practitioners' mental models of machine learning security. Firstly,\npractitioners often confuse machine learning security with threats and defences\nthat are not directly related to machine learning. Secondly, in contrast to\nmost academic research, our participants perceive security of machine learning\nas not solely related to individual models, but rather in the context of entire\nworkflows that consist of multiple components. Jointly with our additional\nfindings, these two facets provide a foundation to substantiate mental models\nfor machine learning security and have implications for the integration of\nadversarial machine learning into corporate workflows, \\new{decreasing\npractitioners' reported uncertainty}, and appropriate regulatory frameworks for\nmachine learning security.", "authors": ["Lukas Bieringer", "Kathrin Grosse", "Michael Backes", "Battista Biggio", "Katharina Krombholz"], "published": "2021-05-08T16:05:07+00:00", "updated": "2022-06-29T13:42:12+00:00", "categories": ["cs.CR", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2105.03726v4", "entry_id": "http://arxiv.org/abs/2105.03726v4"}, {"id": "1912.09630", "filepath": "papers/1912.09630.pdf", "title": "Practical Solutions for Machine Learning Safety in Autonomous Vehicles", "abstract": "Autonomous vehicles rely on machine learning to solve challenging tasks in\nperception and motion planning. However, automotive software safety standards\nhave not fully evolved to address the challenges of machine learning safety\nsuch as interpretability, verification, and performance limitations. In this\npaper, we review and organize practical machine learning safety techniques that\ncan complement engineering safety for machine learning based software in\nautonomous vehicles. Our organization maps safety strategies to\nstate-of-the-art machine learning techniques in order to enhance dependability\nand safety of machine learning algorithms. We also discuss security limitations\nand user experience aspects of machine learning components in autonomous\nvehicles.", "authors": ["Sina Mohseni", "Mandar Pitale", "Vasu Singh", "Zhangyang Wang"], "published": "2019-12-20T03:47:28+00:00", "updated": "2019-12-20T03:47:28+00:00", "categories": ["cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/1912.09630v1", "entry_id": "http://arxiv.org/abs/1912.09630v1"}, {"id": "2103.00742", "filepath": "papers/2103.00742.pdf", "title": "Automated Machine Learning on Graphs: A Survey", "abstract": "Machine learning on graphs has been extensively studied in both academic and\nindustry. However, as the literature on graph learning booms with a vast number\nof emerging methods and techniques, it becomes increasingly difficult to\nmanually design the optimal machine learning algorithm for different\ngraph-related tasks. To solve this critical challenge, automated machine\nlearning (AutoML) on graphs which combines the strength of graph machine\nlearning and AutoML together, is gaining attention from the research community.\nTherefore, we comprehensively survey AutoML on graphs in this paper, primarily\nfocusing on hyper-parameter optimization (HPO) and neural architecture search\n(NAS) for graph machine learning. We further overview libraries related to\nautomated graph machine learning and in-depth discuss AutoGL, the first\ndedicated open-source library for AutoML on graphs. In the end, we share our\ninsights on future research directions for automated graph machine learning.\nThis paper is the first systematic and comprehensive review of automated\nmachine learning on graphs to the best of our knowledge.", "authors": ["Ziwei Zhang", "Xin Wang", "Wenwu Zhu"], "published": "2021-03-01T04:20:33+00:00", "updated": "2021-12-20T05:03:30+00:00", "categories": ["cs.LG"], "pdf_url": "http://arxiv.org/pdf/2103.00742v4", "entry_id": "http://arxiv.org/abs/2103.00742v4"}, {"paper_id": "2204.07492", "filepath": "papers/2204.07492.pdf", "title": "A Machine Learning Tutorial for Operational Meteorology, Part I: Traditional Machine Learning", "abstract": "Recently, the use of machine learning in meteorology has increased greatly.\nWhile many machine learning methods are not new, university classes on machine\nlearning are largely unavailable to meteorology students and are not required\nto become a meteorologist. The lack of formal instruction has contributed to\nperception that machine learning methods are 'black boxes' and thus end-users\nare hesitant to apply the machine learning methods in their every day workflow.\nTo reduce the opaqueness of machine learning methods and lower hesitancy\ntowards machine learning in meteorology, this paper provides a survey of some\nof the most common machine learning methods. A familiar meteorological example\nis used to contextualize the machine learning methods while also discussing\nmachine learning topics using plain language. The following machine learning\nmethods are demonstrated: linear regression; logistic regression; decision\ntrees; random forest; gradient boosted decision trees; naive Bayes; and support\nvector machines. Beyond discussing the different methods, the paper also\ncontains discussions on the general machine learning process as well as best\npractices to enable readers to apply machine learning to their own datasets.\nFurthermore, all code (in the form of Jupyter notebooks and Google Colaboratory\nnotebooks) used to make the examples in the paper is provided in an effort to\ncatalyse the use of machine learning in meteorology.", "authors": ["Randy J. Chase", "David R. Harrison", "Amanda Burke", "Gary M. Lackmann", "Amy McGovern"], "year": "2022-04-15T14:48:04+00:00", "updated": "2022-06-07T14:48:09+00:00", "categories": ["physics.ao-ph", "cs.LG"], "url": "http://arxiv.org/pdf/2204.07492v2", "entry_id": "http://arxiv.org/abs/2204.07492v2"}, {"paper_id": "1807.06722", "filepath": "papers/1807.06722.pdf", "title": "Machine Learning Interpretability: A Science rather than a tool", "abstract": "The term \"interpretability\" is oftenly used by machine learning researchers\neach with their own intuitive understanding of it. There is no universal well\nagreed upon definition of interpretability in machine learning. As any type of\nscience discipline is mainly driven by the set of formulated questions rather\nthan by different tools in that discipline, e.g. astrophysics is the discipline\nthat learns the composition of stars, not as the discipline that use the\nspectroscopes. Similarly, we propose that machine learning interpretability\nshould be a discipline that answers specific questions related to\ninterpretability. These questions can be of statistical, causal and\ncounterfactual nature. Therefore, there is a need to look into the\ninterpretability problem of machine learning in the context of questions that\nneed to be addressed rather than different tools. We discuss about a\nhypothetical interpretability framework driven by a question based scientific\napproach rather than some specific machine learning model. Using a question\nbased notion of interpretability, we can step towards understanding the science\nof machine learning rather than its engineering. This notion will also help us\nunderstanding any specific problem more in depth rather than relying solely on\nmachine learning methods.", "authors": ["Abdul Karim", "Avinash Mishra", "MA Hakim Newton", "Abdul Sattar"], "year": "2018-07-18T00:50:18+00:00", "updated": "2018-07-25T07:23:45+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1807.06722v2", "entry_id": "http://arxiv.org/abs/1807.06722v2"}, {"paper_id": "2103.00742", "filepath": "papers/2103.00742.pdf", "title": "Automated Machine Learning on Graphs: A Survey", "abstract": "Machine learning on graphs has been extensively studied in both academic and\nindustry. However, as the literature on graph learning booms with a vast number\nof emerging methods and techniques, it becomes increasingly difficult to\nmanually design the optimal machine learning algorithm for different\ngraph-related tasks. To solve this critical challenge, automated machine\nlearning (AutoML) on graphs which combines the strength of graph machine\nlearning and AutoML together, is gaining attention from the research community.\nTherefore, we comprehensively survey AutoML on graphs in this paper, primarily\nfocusing on hyper-parameter optimization (HPO) and neural architecture search\n(NAS) for graph machine learning. We further overview libraries related to\nautomated graph machine learning and in-depth discuss AutoGL, the first\ndedicated open-source library for AutoML on graphs. In the end, we share our\ninsights on future research directions for automated graph machine learning.\nThis paper is the first systematic and comprehensive review of automated\nmachine learning on graphs to the best of our knowledge.", "authors": ["Ziwei Zhang", "Xin Wang", "Wenwu Zhu"], "year": "2021-03-01T04:20:33+00:00", "updated": "2021-12-20T05:03:30+00:00", "categories": ["cs.LG"], "url": "http://arxiv.org/pdf/2103.00742v4", "entry_id": "http://arxiv.org/abs/2103.00742v4"}, {"paper_id": "1707.03184", "filepath": "papers/1707.03184.pdf", "title": "A Survey on Resilient Machine Learning", "abstract": "Machine learning based system are increasingly being used for sensitive tasks\nsuch as security surveillance, guiding autonomous vehicle, taking investment\ndecisions, detecting and blocking network intrusion and malware etc. However,\nrecent research has shown that machine learning models are venerable to attacks\nby adversaries at all phases of machine learning (eg, training data collection,\ntraining, operation). All model classes of machine learning systems can be\nmisled by providing carefully crafted inputs making them wrongly classify\ninputs. Maliciously created input samples can affect the learning process of a\nML system by either slowing down the learning process, or affecting the\nperformance of the learned mode, or causing the system make error(s) only in\nattacker's planned scenario. Because of these developments, understanding\nsecurity of machine learning algorithms and systems is emerging as an important\nresearch area among computer security and machine learning researchers and\npractitioners. We present a survey of this emerging area in machine learning.", "authors": ["Atul Kumar", "Sameep Mehta"], "year": "2017-07-11T09:15:46+00:00", "updated": "2017-07-11T09:15:46+00:00", "categories": ["cs.AI", "cs.CR", "cs.LG"], "url": "http://arxiv.org/pdf/1707.03184v1", "entry_id": "http://arxiv.org/abs/1707.03184v1"}, {"paper_id": "2004.05366", "filepath": "papers/2004.05366.pdf", "title": "In-Machine-Learning Database: Reimagining Deep Learning with Old-School SQL", "abstract": "In-database machine learning has been very popular, almost being a cliche.\nHowever, can we do it the other way around? In this work, we say \"yes\" by\napplying plain old SQL to deep learning, in a sense implementing deep learning\nalgorithms with SQL. Most deep learning frameworks, as well as generic machine\nlearning ones, share a de facto standard of multidimensional array operations,\nunderneath fancier infrastructure such as automatic differentiation. As SQL\ntables can be regarded as generalisations of (multi-dimensional) arrays, we\nhave found a way to express common deep learning operations in SQL, encouraging\na different way of thinking and thus potentially novel models. In particular,\none of the latest trend in deep learning was the introduction of sparsity in\nthe name of graph convolutional networks, whereas we take sparsity almost for\ngranted in the database world. As both databases and machine learning involve\ntransformation of datasets, we hope this work can inspire further works\nutilizing the large body of existing wisdom, algorithms and technologies in the\ndatabase field to advance the state of the art in machine learning, rather than\nmerely integerating machine learning into databases.", "authors": ["Len Du"], "year": "2020-04-11T11:00:26+00:00", "updated": "2020-04-14T18:08:28+00:00", "categories": ["cs.LG", "cs.DB", "stat.ML"], "url": "http://arxiv.org/pdf/2004.05366v2", "entry_id": "http://arxiv.org/abs/2004.05366v2"}, {"paper_id": "2108.07915", "filepath": "papers/2108.07915.pdf", "title": "Data Pricing in Machine Learning Pipelines", "abstract": "Machine learning is disruptive. At the same time, machine learning can only\nsucceed by collaboration among many parties in multiple steps naturally as\npipelines in an eco-system, such as collecting data for possible machine\nlearning applications, collaboratively training models by multiple parties and\ndelivering machine learning services to end users. Data is critical and\npenetrating in the whole machine learning pipelines. As machine learning\npipelines involve many parties and, in order to be successful, have to form a\nconstructive and dynamic eco-system, marketplaces and data pricing are\nfundamental in connecting and facilitating those many parties. In this article,\nwe survey the principles and the latest research development of data pricing in\nmachine learning pipelines. We start with a brief review of data marketplaces\nand pricing desiderata. Then, we focus on pricing in three important steps in\nmachine learning pipelines. To understand pricing in the step of training data\ncollection, we review pricing raw data sets and data labels. We also\ninvestigate pricing in the step of collaborative training of machine learning\nmodels, and overview pricing machine learning models for end users in the step\nof machine learning deployment. We also discuss a series of possible future\ndirections.", "authors": ["Zicun Cong", "Xuan Luo", "Pei Jian", "Feida Zhu", "Yong Zhang"], "year": "2021-08-18T00:57:06+00:00", "updated": "2021-08-18T00:57:06+00:00", "categories": ["cs.LG"], "url": "http://arxiv.org/pdf/2108.07915v1", "entry_id": "http://arxiv.org/abs/2108.07915v1"}, {"paper_id": "2303.18087", "filepath": "papers/2303.18087.pdf", "title": "Evaluation Challenges for Geospatial ML", "abstract": "As geospatial machine learning models and maps derived from their predictions\nare increasingly used for downstream analyses in science and policy, it is\nimperative to evaluate their accuracy and applicability. Geospatial machine\nlearning has key distinctions from other learning paradigms, and as such, the\ncorrect way to measure performance of spatial machine learning outputs has been\na topic of debate. In this paper, I delineate unique challenges of model\nevaluation for geospatial machine learning with global or remotely sensed\ndatasets, culminating in concrete takeaways to improve evaluations of\ngeospatial model performance.", "authors": ["Esther Rolf"], "year": "2023-03-31T14:24:06+00:00", "updated": "2023-03-31T14:24:06+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/2303.18087v1", "entry_id": "http://arxiv.org/abs/2303.18087v1"}, {"paper_id": "2108.08712", "filepath": "papers/2108.08712.pdf", "title": "Teaching Uncertainty Quantification in Machine Learning through Use Cases", "abstract": "Uncertainty in machine learning is not generally taught as general knowledge\nin Machine Learning course curricula. In this paper we propose a short\ncurriculum for a course about uncertainty in machine learning, and complement\nthe course with a selection of use cases, aimed to trigger discussion and let\nstudents play with the concepts of uncertainty in a programming setting. Our\nuse cases cover the concept of output uncertainty, Bayesian neural networks and\nweight distributions, sources of uncertainty, and out of distribution\ndetection. We expect that this curriculum and set of use cases motivates the\ncommunity to adopt these important concepts into courses for safety in AI.", "authors": ["Matias Valdenegro-Toro"], "year": "2021-08-19T14:22:17+00:00", "updated": "2021-08-19T14:22:17+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/2108.08712v1", "entry_id": "http://arxiv.org/abs/2108.08712v1"}, {"paper_id": "2009.11087", "filepath": "papers/2009.11087.pdf", "title": "Probabilistic Machine Learning for Healthcare", "abstract": "Machine learning can be used to make sense of healthcare data. Probabilistic\nmachine learning models help provide a complete picture of observed data in\nhealthcare. In this review, we examine how probabilistic machine learning can\nadvance healthcare. We consider challenges in the predictive model building\npipeline where probabilistic models can be beneficial including calibration and\nmissing data. Beyond predictive models, we also investigate the utility of\nprobabilistic machine learning models in phenotyping, in generative models for\nclinical use cases, and in reinforcement learning.", "authors": ["Irene Y. Chen", "Shalmali Joshi", "Marzyeh Ghassemi", "Rajesh Ranganath"], "year": "2020-09-23T12:14:05+00:00", "updated": "2020-09-23T12:14:05+00:00", "categories": ["stat.ML", "cs.CY", "cs.LG"], "url": "http://arxiv.org/pdf/2009.11087v1", "entry_id": "http://arxiv.org/abs/2009.11087v1"}, {"paper_id": "2401.11351", "filepath": "papers/2401.11351.pdf", "title": "A comprehensive review of Quantum Machine Learning: from NISQ to Fault Tolerance", "abstract": "Quantum machine learning, which involves running machine learning algorithms\non quantum devices, has garnered significant attention in both academic and\nbusiness circles. In this paper, we offer a comprehensive and unbiased review\nof the various concepts that have emerged in the field of quantum machine\nlearning. This includes techniques used in Noisy Intermediate-Scale Quantum\n(NISQ) technologies and approaches for algorithms compatible with\nfault-tolerant quantum computing hardware. Our review covers fundamental\nconcepts, algorithms, and the statistical learning theory pertinent to quantum\nmachine learning.", "authors": ["Yunfei Wang", "Junyu Liu"], "year": "2024-01-21T00:19:16+00:00", "updated": "2024-03-31T00:32:13+00:00", "categories": ["quant-ph", "cs.AI", "cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/2401.11351v2", "entry_id": "http://arxiv.org/abs/2401.11351v2"}, {"paper_id": "2004.00993", "filepath": "papers/2004.00993.pdf", "title": "Augmented Q Imitation Learning (AQIL)", "abstract": "The study of unsupervised learning can be generally divided into two\ncategories: imitation learning and reinforcement learning. In imitation\nlearning the machine learns by mimicking the behavior of an expert system\nwhereas in reinforcement learning the machine learns via direct environment\nfeedback. Traditional deep reinforcement learning takes a significant time\nbefore the machine starts to converge to an optimal policy. This paper proposes\nAugmented Q-Imitation-Learning, a method by which deep reinforcement learning\nconvergence can be accelerated by applying Q-imitation-learning as the initial\ntraining process in traditional Deep Q-learning.", "authors": ["Xiao Lei Zhang", "Anish Agarwal"], "year": "2020-03-31T18:08:23+00:00", "updated": "2020-04-05T17:16:23+00:00", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/pdf/2004.00993v2", "entry_id": "http://arxiv.org/abs/2004.00993v2"}, {"paper_id": "1711.06552", "filepath": "papers/1711.06552.pdf", "title": "Introduction to intelligent computing unit 1", "abstract": "This brief note highlights some basic concepts required toward understanding\nthe evolution of machine learning and deep learning models. The note starts\nwith an overview of artificial intelligence and its relationship to biological\nneuron that ultimately led to the evolution of todays intelligent models.", "authors": ["Isa Inuwa-Dutse"], "year": "2017-11-15T16:52:48+00:00", "updated": "2017-11-15T16:52:48+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1711.06552v1", "entry_id": "http://arxiv.org/abs/1711.06552v1"}, {"paper_id": "1603.02185", "filepath": "papers/1603.02185.pdf", "title": "Distributed Multi-Task Learning with Shared Representation", "abstract": "We study the problem of distributed multi-task learning with shared\nrepresentation, where each machine aims to learn a separate, but related, task\nin an unknown shared low-dimensional subspaces, i.e. when the predictor matrix\nhas low rank. We consider a setting where each task is handled by a different\nmachine, with samples for the task available locally on the machine, and study\ncommunication-efficient methods for exploiting the shared structure.", "authors": ["Jialei Wang", "Mladen Kolar", "Nathan Srebro"], "year": "2016-03-07T18:11:54+00:00", "updated": "2016-03-07T18:11:54+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1603.02185v1", "entry_id": "http://arxiv.org/abs/1603.02185v1"}, {"paper_id": "2012.04105", "filepath": "papers/2012.04105.pdf", "title": "The Tribes of Machine Learning and the Realm of Computer Architecture", "abstract": "Machine learning techniques have influenced the field of computer\narchitecture like many other fields. This paper studies how the fundamental\nmachine learning techniques can be applied towards computer architecture\nproblems. We also provide a detailed survey of computer architecture research\nthat employs different machine learning methods. Finally, we present some\nfuture opportunities and the outstanding challenges that need to be overcome to\nexploit full potential of machine learning for computer architecture.", "authors": ["Ayaz Akram", "Jason Lowe-Power"], "year": "2020-12-07T23:10:51+00:00", "updated": "2020-12-07T23:10:51+00:00", "categories": ["cs.LG", "cs.AR"], "url": "http://arxiv.org/pdf/2012.04105v1", "entry_id": "http://arxiv.org/abs/2012.04105v1"}, {"paper_id": "1706.08001", "filepath": "papers/1706.08001.pdf", "title": "Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of learning relational order via reinforcement learning procedure?", "abstract": "In this article, we extend the conventional framework of\nconvolutional-Restricted-Boltzmann-Machine to learn highly abstract features\namong abitrary number of time related input maps by constructing a layer of\nmultiplicative units, which capture the relations among inputs. In many cases,\nmore than two maps are strongly related, so it is wise to make multiplicative\nunit learn relations among more input maps, in other words, to find the optimal\nrelational-order of each unit. In order to enable our machine to learn\nrelational order, we developed a reinforcement-learning method whose optimality\nis proven to train the network.", "authors": ["Zizhuang Wang"], "year": "2017-06-24T20:56:27+00:00", "updated": "2017-06-24T20:56:27+00:00", "categories": ["cs.AI", "cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1706.08001v1", "entry_id": "http://arxiv.org/abs/1706.08001v1"}, {"paper_id": "1811.04422", "filepath": "papers/1811.04422.pdf", "title": "An Optimal Control View of Adversarial Machine Learning", "abstract": "I describe an optimal control view of adversarial machine learning, where the\ndynamical system is the machine learner, the input are adversarial actions, and\nthe control costs are defined by the adversary's goals to do harm and be hard\nto detect. This view encompasses many types of adversarial machine learning,\nincluding test-item attacks, training-data poisoning, and adversarial reward\nshaping. The view encourages adversarial machine learning researcher to utilize\nadvances in control theory and reinforcement learning.", "authors": ["Xiaojin Zhu"], "year": "2018-11-11T14:28:34+00:00", "updated": "2018-11-11T14:28:34+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1811.04422v1", "entry_id": "http://arxiv.org/abs/1811.04422v1"}, {"paper_id": "2011.11819", "filepath": "papers/2011.11819.pdf", "title": "When Machine Learning Meets Privacy: A Survey and Outlook", "abstract": "The newly emerged machine learning (e.g. deep learning) methods have become a\nstrong driving force to revolutionize a wide range of industries, such as smart\nhealthcare, financial technology, and surveillance systems. Meanwhile, privacy\nhas emerged as a big concern in this machine learning-based artificial\nintelligence era. It is important to note that the problem of privacy\npreservation in the context of machine learning is quite different from that in\ntraditional data privacy protection, as machine learning can act as both friend\nand foe. Currently, the work on the preservation of privacy and machine\nlearning (ML) is still in an infancy stage, as most existing solutions only\nfocus on privacy problems during the machine learning process. Therefore, a\ncomprehensive study on the privacy preservation problems and machine learning\nis required. This paper surveys the state of the art in privacy issues and\nsolutions for machine learning. The survey covers three categories of\ninteractions between privacy and machine learning: (i) private machine\nlearning, (ii) machine learning aided privacy protection, and (iii) machine\nlearning-based privacy attack and corresponding protection schemes. The current\nresearch progress in each category is reviewed and the key challenges are\nidentified. Finally, based on our in-depth analysis of the area of privacy and\nmachine learning, we point out future research directions in this field.", "authors": ["Bo Liu", "Ming Ding", "Sina Shaham", "Wenny Rahayu", "Farhad Farokhi", "Zihuai Lin"], "year": "2020-11-24T00:52:49+00:00", "updated": "2020-11-24T00:52:49+00:00", "categories": ["cs.LG", "cs.AI", "cs.CR"], "url": "http://arxiv.org/pdf/2011.11819v1", "entry_id": "http://arxiv.org/abs/2011.11819v1"}, {"paper_id": "1909.01866", "filepath": "papers/1909.01866.pdf", "title": "Understanding Bias in Machine Learning", "abstract": "Bias is known to be an impediment to fair decisions in many domains such as\nhuman resources, the public sector, health care etc. Recently, hope has been\nexpressed that the use of machine learning methods for taking such decisions\nwould diminish or even resolve the problem. At the same time, machine learning\nexperts warn that machine learning models can be biased as well. In this\narticle, our goal is to explain the issue of bias in machine learning from a\ntechnical perspective and to illustrate the impact that biased data can have on\na machine learning model. To reach such a goal, we develop interactive plots to\nvisualizing the bias learned from synthetic data.", "authors": ["Jindong Gu", "Daniela Oelke"], "year": "2019-09-02T20:36:19+00:00", "updated": "2019-09-02T20:36:19+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1909.01866v1", "entry_id": "http://arxiv.org/abs/1909.01866v1"}, {"paper_id": "1909.09246", "filepath": "papers/1909.09246.pdf", "title": "Machine Learning for Clinical Predictive Analytics", "abstract": "In this chapter, we provide a brief overview of applying machine learning\ntechniques for clinical prediction tasks. We begin with a quick introduction to\nthe concepts of machine learning and outline some of the most common machine\nlearning algorithms. Next, we demonstrate how to apply the algorithms with\nappropriate toolkits to conduct machine learning experiments for clinical\nprediction tasks. The objectives of this chapter are to (1) understand the\nbasics of machine learning techniques and the reasons behind why they are\nuseful for solving clinical prediction problems, (2) understand the intuition\nbehind some machine learning models, including regression, decision trees, and\nsupport vector machines, and (3) understand how to apply these models to\nclinical prediction problems using publicly available datasets via case\nstudies.", "authors": ["Wei-Hung Weng"], "year": "2019-09-19T22:02:00+00:00", "updated": "2019-09-19T22:02:00+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1909.09246v1", "entry_id": "http://arxiv.org/abs/1909.09246v1"}, {"paper_id": "1911.00776", "filepath": "papers/1911.00776.pdf", "title": "Ten-year Survival Prediction for Breast Cancer Patients", "abstract": "This report assesses different machine learning approaches to 10-year\nsurvival prediction of breast cancer patients.", "authors": ["Changmao Li", "Han He", "Yunze Hao", "Caleb Ziems"], "year": "2019-11-02T19:53:32+00:00", "updated": "2019-11-02T19:53:32+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1911.00776v1", "entry_id": "http://arxiv.org/abs/1911.00776v1"}, {"paper_id": "2102.05639", "filepath": "papers/2102.05639.pdf", "title": "Energy-Harvesting Distributed Machine Learning", "abstract": "This paper provides a first study of utilizing energy harvesting for\nsustainable machine learning in distributed networks. We consider a distributed\nlearning setup in which a machine learning model is trained over a large number\nof devices that can harvest energy from the ambient environment, and develop a\npractical learning framework with theoretical convergence guarantees. We\ndemonstrate through numerical experiments that the proposed framework can\nsignificantly outperform energy-agnostic benchmarks. Our framework is scalable,\nrequires only local estimation of the energy statistics, and can be applied to\na wide range of distributed training settings, including machine learning in\nwireless networks, edge computing, and mobile internet of things.", "authors": ["Basak Guler", "Aylin Yener"], "year": "2021-02-10T18:53:51+00:00", "updated": "2021-02-10T18:53:51+00:00", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "url": "http://arxiv.org/pdf/2102.05639v1", "entry_id": "http://arxiv.org/abs/2102.05639v1"}, {"paper_id": "2003.05155", "filepath": "papers/2003.05155.pdf", "title": "Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology", "abstract": "Machine learning is an established and frequently used technique in industry\nand academia but a standard process model to improve success and efficiency of\nmachine learning applications is still missing. Project organizations and\nmachine learning practitioners have a need for guidance throughout the life\ncycle of a machine learning application to meet business expectations. We\ntherefore propose a process model for the development of machine learning\napplications, that covers six phases from defining the scope to maintaining the\ndeployed machine learning application. The first phase combines business and\ndata understanding as data availability oftentimes affects the feasibility of\nthe project. The sixth phase covers state-of-the-art approaches for monitoring\nand maintenance of a machine learning applications, as the risk of model\ndegradation in a changing environment is eminent. With each task of the\nprocess, we propose quality assurance methodology that is suitable to adress\nchallenges in machine learning development that we identify in form of risks.\nThe methodology is drawn from practical experience and scientific literature\nand has proven to be general and stable. The process model expands on CRISP-DM,\na data mining process model that enjoys strong industry support but lacks to\naddress machine learning specific tasks. Our work proposes an industry and\napplication neutral process model tailored for machine learning applications\nwith focus on technical tasks for quality assurance.", "authors": ["Stefan Studer", "Thanh Binh Bui", "Christian Drescher", "Alexander Hanuschkin", "Ludwig Winkler", "Steven Peters", "Klaus-Robert Mueller"], "year": "2020-03-11T08:25:49+00:00", "updated": "2021-02-24T14:33:24+00:00", "categories": ["cs.LG", "cs.SE", "stat.ML"], "url": "http://arxiv.org/pdf/2003.05155v2", "entry_id": "http://arxiv.org/abs/2003.05155v2"}, {"paper_id": "1906.06821", "filepath": "papers/1906.06821.pdf", "title": "A Survey of Optimization Methods from a Machine Learning Perspective", "abstract": "Machine learning develops rapidly, which has made many theoretical\nbreakthroughs and is widely applied in various fields. Optimization, as an\nimportant part of machine learning, has attracted much attention of\nresearchers. With the exponential growth of data amount and the increase of\nmodel complexity, optimization methods in machine learning face more and more\nchallenges. A lot of work on solving optimization problems or improving\noptimization methods in machine learning has been proposed successively. The\nsystematic retrospect and summary of the optimization methods from the\nperspective of machine learning are of great significance, which can offer\nguidance for both developments of optimization and machine learning research.\nIn this paper, we first describe the optimization problems in machine learning.\nThen, we introduce the principles and progresses of commonly used optimization\nmethods. Next, we summarize the applications and developments of optimization\nmethods in some popular machine learning fields. Finally, we explore and give\nsome challenges and open problems for the optimization in machine learning.", "authors": ["Shiliang Sun", "Zehui Cao", "Han Zhu", "Jing Zhao"], "year": "2019-06-17T02:54:51+00:00", "updated": "2019-10-23T08:26:31+00:00", "categories": ["cs.LG", "math.OC", "stat.ML"], "url": "http://arxiv.org/pdf/1906.06821v2", "entry_id": "http://arxiv.org/abs/1906.06821v2"}, {"paper_id": "1509.00913", "filepath": "papers/1509.00913.pdf", "title": "On-the-Fly Learning in a Perpetual Learning Machine", "abstract": "Despite the promise of brain-inspired machine learning, deep neural networks\n(DNN) have frustratingly failed to bridge the deceptively large gap between\nlearning and memory. Here, we introduce a Perpetual Learning Machine; a new\ntype of DNN that is capable of brain-like dynamic 'on the fly' learning because\nit exists in a self-supervised state of Perpetual Stochastic Gradient Descent.\nThus, we provide the means to unify learning and memory within a machine\nlearning framework. We also explore the elegant duality of abstraction and\nsynthesis: the Yin and Yang of deep learning.", "authors": ["Andrew J. R. Simpson"], "year": "2015-09-03T01:30:29+00:00", "updated": "2015-09-29T16:57:42+00:00", "categories": ["cs.LG", "68Txx"], "url": "http://arxiv.org/pdf/1509.00913v3", "entry_id": "http://arxiv.org/abs/1509.00913v3"}, {"paper_id": "1602.00198", "filepath": "papers/1602.00198.pdf", "title": "Discussion on Mechanical Learning and Learning Machine", "abstract": "Mechanical learning is a computing system that is based on a set of simple\nand fixed rules, and can learn from incoming data. A learning machine is a\nsystem that realizes mechanical learning. Importantly, we emphasis that it is\nbased on a set of simple and fixed rules, contrasting to often called machine\nlearning that is sophisticated software based on very complicated mathematical\ntheory, and often needs human intervene for software fine tune and manual\nadjustments. Here, we discuss some basic facts and principles of such system,\nand try to lay down a framework for further study. We propose 2 directions to\napproach mechanical learning, just like Church-Turing pair: one is trying to\nrealize a learning machine, another is trying to well describe the mechanical\nlearning.", "authors": ["Chuyu Xiong"], "year": "2016-01-31T04:05:50+00:00", "updated": "2016-01-31T04:05:50+00:00", "categories": ["cs.AI"], "url": "http://arxiv.org/pdf/1602.00198v1", "entry_id": "http://arxiv.org/abs/1602.00198v1"}, {"paper_id": "1802.03830", "filepath": "papers/1802.03830.pdf", "title": "Distributed Stochastic Multi-Task Learning with Graph Regularization", "abstract": "We propose methods for distributed graph-based multi-task learning that are\nbased on weighted averaging of messages from other machines. Uniform averaging\nor diminishing stepsize in these methods would yield consensus (single task)\nlearning. We show how simply skewing the averaging weights or controlling the\nstepsize allows learning different, but related, tasks on the different\nmachines.", "authors": ["Weiran Wang", "Jialei Wang", "Mladen Kolar", "Nathan Srebro"], "year": "2018-02-11T22:23:34+00:00", "updated": "2018-02-11T22:23:34+00:00", "categories": ["stat.ML", "cs.LG"], "url": "http://arxiv.org/pdf/1802.03830v1", "entry_id": "http://arxiv.org/abs/1802.03830v1"}, {"paper_id": "1909.09248", "filepath": "papers/1909.09248.pdf", "title": "Representation Learning for Electronic Health Records", "abstract": "Information in electronic health records (EHR), such as clinical narratives,\nexamination reports, lab measurements, demographics, and other patient\nencounter entries, can be transformed into appropriate data representations\nthat can be used for downstream clinical machine learning tasks using\nrepresentation learning. Learning better representations is critical to improve\nthe performance of downstream tasks. Due to the advances in machine learning,\nwe now can learn better and meaningful representations from EHR through\ndisentangling the underlying factors inside data and distilling large amounts\nof information and knowledge from heterogeneous EHR sources. In this chapter,\nwe first introduce the background of learning representations and reasons why\nwe need good EHR representations in machine learning for medicine and\nhealthcare in Section 1. Next, we explain the commonly-used machine learning\nand evaluation methods for representation learning using a deep learning\napproach in Section 2. Following that, we review recent related studies of\nlearning patient state representation from EHR for clinical machine learning\ntasks in Section 3. Finally, in Section 4 we discuss more techniques, studies,\nand challenges for learning natural language representations when free texts,\nsuch as clinical notes, examination reports, or biomedical literature are used.\nWe also discuss challenges and opportunities in these rapidly growing research\nfields.", "authors": ["Wei-Hung Weng", "Peter Szolovits"], "year": "2019-09-19T22:12:30+00:00", "updated": "2019-09-19T22:12:30+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1909.09248v1", "entry_id": "http://arxiv.org/abs/1909.09248v1"}, {"paper_id": "2007.01503", "filepath": "papers/2007.01503.pdf", "title": "Mathematical Perspective of Machine Learning", "abstract": "We take a closer look at some theoretical challenges of Machine Learning as a\nfunction approximation, gradient descent as the default optimization algorithm,\nlimitations of fixed length and width networks and a different approach to RNNs\nfrom a mathematical perspective.", "authors": ["Yarema Boryshchak"], "year": "2020-07-03T05:26:02+00:00", "updated": "2020-07-03T05:26:02+00:00", "categories": ["cs.LG", "stat.ML", "68T07"], "url": "http://arxiv.org/pdf/2007.01503v1", "entry_id": "http://arxiv.org/abs/2007.01503v1"}, {"paper_id": "2106.07032", "filepath": "papers/2106.07032.pdf", "title": "Category Theory in Machine Learning", "abstract": "Over the past two decades machine learning has permeated almost every realm\nof technology. At the same time, many researchers have begun using category\ntheory as a unifying language, facilitating communication between different\nscientific disciplines. It is therefore unsurprising that there is a burgeoning\ninterest in applying category theory to machine learning. We aim to document\nthe motivations, goals and common themes across these applications. We touch on\ngradient-based learning, probability, and equivariant learning.", "authors": ["Dan Shiebler", "Bruno Gavranovi\u0107", "Paul Wilson"], "year": "2021-06-13T15:58:13+00:00", "updated": "2021-06-13T15:58:13+00:00", "categories": ["cs.LG"], "url": "http://arxiv.org/pdf/2106.07032v1", "entry_id": "http://arxiv.org/abs/2106.07032v1"}, {"paper_id": "1607.02450", "filepath": "papers/1607.02450.pdf", "title": "Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in Social Good Applications", "abstract": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning\nin Social Good Applications, which was held on June 24, 2016 in New York.", "authors": ["Kush R. Varshney"], "year": "2016-07-08T16:55:31+00:00", "updated": "2016-08-28T15:23:47+00:00", "categories": ["stat.ML", "cs.CY", "cs.LG"], "url": "http://arxiv.org/pdf/1607.02450v2", "entry_id": "http://arxiv.org/abs/1607.02450v2"}, {"paper_id": "1501.04309", "filepath": "papers/1501.04309.pdf", "title": "Information Theory and its Relation to Machine Learning", "abstract": "In this position paper, I first describe a new perspective on machine\nlearning (ML) by four basic problems (or levels), namely, \"What to learn?\",\n\"How to learn?\", \"What to evaluate?\", and \"What to adjust?\". The paper stresses\nmore on the first level of \"What to learn?\", or \"Learning Target Selection\".\nTowards this primary problem within the four levels, I briefly review the\nexisting studies about the connection between information theoretical learning\n(ITL [1]) and machine learning. A theorem is given on the relation between the\nempirically-defined similarity measure and information measures. Finally, a\nconjecture is proposed for pursuing a unified mathematical interpretation to\nlearning target selection.", "authors": ["Bao-Gang Hu"], "year": "2015-01-18T14:57:02+00:00", "updated": "2015-01-18T14:57:02+00:00", "categories": ["cs.IT", "cs.LG", "math.IT"], "url": "http://arxiv.org/pdf/1501.04309v1", "entry_id": "http://arxiv.org/abs/1501.04309v1"}, {"paper_id": "2310.11470", "filepath": "papers/2310.11470.pdf", "title": "Classic machine learning methods", "abstract": "In this chapter, we present the main classic machine learning methods. A\nlarge part of the chapter is devoted to supervised learning techniques for\nclassification and regression, including nearest-neighbor methods, linear and\nlogistic regressions, support vector machines and tree-based algorithms. We\nalso describe the problem of overfitting as well as strategies to overcome it.\nWe finally provide a brief overview of unsupervised learning methods, namely\nfor clustering and dimensionality reduction.", "authors": ["Johann Faouzi", "Olivier Colliot"], "year": "2023-05-24T13:38:38+00:00", "updated": "2023-05-24T13:38:38+00:00", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/pdf/2310.11470v1", "entry_id": "http://arxiv.org/abs/2310.11470v1"}, {"paper_id": "2206.13446", "filepath": "papers/2206.13446.pdf", "title": "Pen and Paper Exercises in Machine Learning", "abstract": "This is a collection of (mostly) pen-and-paper exercises in machine learning.\nThe exercises are on the following topics: linear algebra, optimisation,\ndirected graphical models, undirected graphical models, expressive power of\ngraphical models, factor graphs and message passing, inference for hidden\nMarkov models, model-based learning (including ICA and unnormalised models),\nsampling and Monte-Carlo integration, and variational inference.", "authors": ["Michael U. Gutmann"], "year": "2022-06-27T16:53:18+00:00", "updated": "2022-06-27T16:53:18+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/2206.13446v1", "entry_id": "http://arxiv.org/abs/2206.13446v1"}, {"paper_id": "2305.15410", "filepath": "papers/2305.15410.pdf", "title": "Machine learning-assisted close-set X-ray diffraction phase identification of transition metals", "abstract": "Machine learning has been applied to the problem of X-ray diffraction phase\nprediction with promising results. In this paper, we describe a method for\nusing machine learning to predict crystal structure phases from X-ray\ndiffraction data of transition metals and their oxides. We evaluate the\nperformance of our method and compare the variety of its settings. Our results\ndemonstrate that the proposed machine learning framework achieves competitive\nperformance. This demonstrates the potential for machine learning to\nsignificantly impact the field of X-ray diffraction and crystal structure\ndetermination. Open-source implementation:\nhttps://github.com/maxnygma/NeuralXRD.", "authors": ["Maksim Zhdanov", "Andrey Zhdanov"], "year": "2023-04-28T09:29:10+00:00", "updated": "2023-04-28T09:29:10+00:00", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.LG"], "url": "http://arxiv.org/pdf/2305.15410v1", "entry_id": "http://arxiv.org/abs/2305.15410v1"}, {"paper_id": "1507.02188", "filepath": "papers/1507.02188.pdf", "title": "AutoCompete: A Framework for Machine Learning Competition", "abstract": "In this paper, we propose AutoCompete, a highly automated machine learning\nframework for tackling machine learning competitions. This framework has been\nlearned by us, validated and improved over a period of more than two years by\nparticipating in online machine learning competitions. It aims at minimizing\nhuman interference required to build a first useful predictive model and to\nassess the practical difficulty of a given machine learning challenge. The\nproposed system helps in identifying data types, choosing a machine learn- ing\nmodel, tuning hyper-parameters, avoiding over-fitting and optimization for a\nprovided evaluation metric. We also observe that the proposed system produces\nbetter (or comparable) results with less runtime as compared to other\napproaches.", "authors": ["Abhishek Thakur", "Artus Krohn-Grimberghe"], "year": "2015-07-08T15:07:39+00:00", "updated": "2015-07-08T15:07:39+00:00", "categories": ["stat.ML", "cs.LG"], "url": "http://arxiv.org/pdf/1507.02188v1", "entry_id": "http://arxiv.org/abs/1507.02188v1"}, {"paper_id": "2103.03122", "filepath": "papers/2103.03122.pdf", "title": "Machine Learning using Stata/Python", "abstract": "We present two related Stata modules, r_ml_stata and c_ml_stata, for fitting\npopular Machine Learning (ML) methods both in regression and classification\nsettings. Using the recent Stata/Python integration platform (sfi) of Stata 16,\nthese commands provide hyper-parameters' optimal tuning via K-fold\ncross-validation using greed search. More specifically, they make use of the\nPython Scikit-learn API to carry out both cross-validation and outcome/label\nprediction.", "authors": ["Giovanni Cerulli"], "year": "2021-03-03T10:31:44+00:00", "updated": "2021-03-03T10:31:44+00:00", "categories": ["stat.CO", "cs.LG", "cs.MS"], "url": "http://arxiv.org/pdf/2103.03122v1", "entry_id": "http://arxiv.org/abs/2103.03122v1"}, {"paper_id": "2206.07090", "filepath": "papers/2206.07090.pdf", "title": "Parallelization of Machine Learning Algorithms Respectively on Single Machine and Spark", "abstract": "With the rapid development of big data technologies, how to dig out useful\ninformation from massive data becomes an essential problem. However, using\nmachine learning algorithms to analyze large data may be time-consuming and\ninefficient on the traditional single machine. To solve these problems, this\npaper has made some research on the parallelization of several classic machine\nlearning algorithms respectively on the single machine and the big data\nplatform Spark. We compare the runtime and efficiency of traditional machine\nlearning algorithms with parallelized machine learning algorithms respectively\non the single machine and Spark platform. The research results have shown\nsignificant improvement in runtime and efficiency of parallelized machine\nlearning algorithms.", "authors": ["Jiajun Shen"], "year": "2022-05-08T03:47:30+00:00", "updated": "2023-04-13T03:30:03+00:00", "categories": ["cs.DC"], "url": "http://arxiv.org/pdf/2206.07090v2", "entry_id": "http://arxiv.org/abs/2206.07090v2"}, {"paper_id": "1212.2686", "filepath": "papers/1212.2686.pdf", "title": "Joint Training of Deep Boltzmann Machines", "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior\nmethods require an initial learning pass that trains the deep Boltzmann machine\ngreedily, one layer at a time, or do not perform well on classifi- cation\ntasks.", "authors": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "year": "2012-12-12T01:59:27+00:00", "updated": "2012-12-12T01:59:27+00:00", "categories": ["stat.ML", "cs.LG"], "url": "http://arxiv.org/pdf/1212.2686v1", "entry_id": "http://arxiv.org/abs/1212.2686v1"}, {"paper_id": "1907.03010", "filepath": "papers/1907.03010.pdf", "title": "Financial Time Series Data Processing for Machine Learning", "abstract": "This article studies the financial time series data processing for machine\nlearning. It introduces the most frequent scaling methods, then compares the\nresulting stationarity and preservation of useful information for trend\nforecasting. It proposes an empirical test based on the capability to learn\nsimple data relationship with simple models. It also speaks about the data\nsplit method specific to time series, avoiding unwanted overfitting and\nproposes various labelling for classification and regression.", "authors": ["Fabrice Daniel"], "year": "2019-07-03T15:10:23+00:00", "updated": "2019-07-03T15:10:23+00:00", "categories": ["q-fin.ST", "cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1907.03010v1", "entry_id": "http://arxiv.org/abs/1907.03010v1"}, {"paper_id": "2001.04942", "filepath": "papers/2001.04942.pdf", "title": "Private Machine Learning via Randomised Response", "abstract": "We introduce a general learning framework for private machine learning based\non randomised response. Our assumption is that all actors are potentially\nadversarial and as such we trust only to release a single noisy version of an\nindividual's datapoint. We discuss a general approach that forms a consistent\nway to estimate the true underlying machine learning model and demonstrate this\nin the case of logistic regression.", "authors": ["David Barber"], "year": "2020-01-14T17:56:16+00:00", "updated": "2020-02-24T19:13:28+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/2001.04942v2", "entry_id": "http://arxiv.org/abs/2001.04942v2"}, {"paper_id": "2312.03120", "filepath": "papers/2312.03120.pdf", "title": "The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning", "abstract": "With the advance of the powerful heterogeneous, parallel and distributed\ncomputing systems and ever increasing immense amount of data, machine learning\nhas become an indispensable part of cutting-edge technology, scientific\nresearch and consumer products. In this study, we present a review of modern\nmachine and deep learning. We provide a high-level overview for the latest\nadvanced machine learning algorithms, applications, and frameworks. Our\ndiscussion encompasses parallel distributed learning, deep learning as well as\nfederated learning. As a result, our work serves as an introductory text to the\nvast field of modern machine learning.", "authors": ["Omer Subasi", "Oceane Bel", "Joseph Manzano", "Kevin Barker"], "year": "2023-12-05T20:40:05+00:00", "updated": "2023-12-05T20:40:05+00:00", "categories": ["cs.LG", "cs.AI", "cs.DC"], "url": "http://arxiv.org/pdf/2312.03120v1", "entry_id": "http://arxiv.org/abs/2312.03120v1"}, {"paper_id": "1902.04622", "filepath": "papers/1902.04622.pdf", "title": "Learning Theory and Support Vector Machines - a primer", "abstract": "The main goal of statistical learning theory is to provide a fundamental\nframework for the problem of decision making and model construction based on\nsets of data. Here, we present a brief introduction to the fundamentals of\nstatistical learning theory, in particular the difference between empirical and\nstructural risk minimization, including one of its most prominent\nimplementations, i.e. the Support Vector Machine.", "authors": ["Michael Banf"], "year": "2019-02-12T20:28:09+00:00", "updated": "2019-02-12T20:28:09+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1902.04622v1", "entry_id": "http://arxiv.org/abs/1902.04622v1"}, {"paper_id": "2412.00464", "filepath": "papers/2412.00464.pdf", "title": "On the Conditions for Domain Stability for Machine Learning: a Mathematical Approach", "abstract": "This work proposes a mathematical approach that (re)defines a property of\nMachine Learning models named stability and determines sufficient conditions to\nvalidate it. Machine Learning models are represented as functions, and the\ncharacteristics in scope depend upon the domain of the function, what allows us\nto adopt topological and metric spaces theory as a basis. Finally, this work\nprovides some equivalences useful to prove and test stability in Machine\nLearning models. The results suggest that whenever stability is aligned with\nthe notion of function smoothness, then the stability of Machine Learning\nmodels primarily depends upon certain topological, measurable properties of the\nclassification sets within the ML model domain.", "authors": ["Gabriel Pedroza"], "year": "2024-11-30T12:57:07+00:00", "updated": "2024-11-30T12:57:07+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML", "F.4.1; I.2.0"], "url": "http://arxiv.org/pdf/2412.00464v1", "entry_id": "http://arxiv.org/abs/2412.00464v1"}, {"paper_id": "2502.01708", "filepath": "papers/2502.01708.pdf", "title": "Aspects of Artificial Intelligence: Transforming Machine Learning Systems Naturally", "abstract": "In this paper, we study the machine learning elements which we are interested\nin together as a machine learning system, consisting of a collection of machine\nlearning elements and a collection of relations between the elements. The\nrelations we concern are algebraic operations, binary relations, and binary\nrelations with composition that can be reasoned categorically. A machine\nlearning system transformation between two systems is a map between the\nsystems, which preserves the relations we concern. The system transformations\ngiven by quotient or clustering, representable functor, and Yoneda embedding\nare highlighted and discussed by machine learning examples. An adjunction\nbetween machine learning systems, a special machine learning system\ntransformation loop, provides the optimal way of solving problems. Machine\nlearning system transformations are linked and compared by their maps at\n2-cell, natural transformations. New insights and structures can be obtained\nfrom universal properties and algebraic structures given by monads, which are\ngenerated from adjunctions.", "authors": ["Xiuzhan Guo"], "year": "2025-02-03T14:45:02+00:00", "updated": "2025-02-03T14:45:02+00:00", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.DM"], "url": "http://arxiv.org/pdf/2502.01708v1", "entry_id": "http://arxiv.org/abs/2502.01708v1"}, {"paper_id": "2212.12303", "filepath": "papers/2212.12303.pdf", "title": "Introduction to Machine Learning for Physicians: A Survival Guide for Data Deluge", "abstract": "Many modern research fields increasingly rely on collecting and analysing\nmassive, often unstructured, and unwieldy datasets. Consequently, there is\ngrowing interest in machine learning and artificial intelligence applications\nthat can harness this `data deluge'. This broad nontechnical overview provides\na gentle introduction to machine learning with a specific focus on medical and\nbiological applications. We explain the common types of machine learning\nalgorithms and typical tasks that can be solved, illustrating the basics with\nconcrete examples from healthcare. Lastly, we provide an outlook on open\nchallenges, limitations, and potential impacts of machine-learning-powered\nmedicine.", "authors": ["Ri\u010dards Marcinkevi\u010ds", "Ece Ozkan", "Julia E. Vogt"], "year": "2022-12-23T13:08:59+00:00", "updated": "2022-12-23T13:08:59+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/2212.12303v1", "entry_id": "http://arxiv.org/abs/2212.12303v1"}, {"paper_id": "2409.03632", "filepath": "papers/2409.03632.pdf", "title": "Beyond Model Interpretability: Socio-Structural Explanations in Machine Learning", "abstract": "What is it to interpret the outputs of an opaque machine learning model. One\napproach is to develop interpretable machine learning techniques. These\ntechniques aim to show how machine learning models function by providing either\nmodel centric local or global explanations, which can be based on mechanistic\ninterpretations revealing the inner working mechanisms of models or\nnonmechanistic approximations showing input feature output data relationships.\nIn this paper, we draw on social philosophy to argue that interpreting machine\nlearning outputs in certain normatively salient domains could require appealing\nto a third type of explanation that we call sociostructural explanation. The\nrelevance of this explanation type is motivated by the fact that machine\nlearning models are not isolated entities but are embedded within and shaped by\nsocial structures. Sociostructural explanations aim to illustrate how social\nstructures contribute to and partially explain the outputs of machine learning\nmodels. We demonstrate the importance of sociostructural explanations by\nexamining a racially biased healthcare allocation algorithm. Our proposal\nhighlights the need for transparency beyond model interpretability,\nunderstanding the outputs of machine learning systems could require a broader\nanalysis that extends beyond the understanding of the machine learning model\nitself.", "authors": ["Andrew Smart", "Atoosa Kasirzadeh"], "year": "2024-09-05T15:47:04+00:00", "updated": "2024-09-05T15:47:04+00:00", "categories": ["cs.LG"], "url": "http://arxiv.org/pdf/2409.03632v1", "entry_id": "http://arxiv.org/abs/2409.03632v1"}, {"paper_id": "2412.18979", "filepath": "papers/2412.18979.pdf", "title": "Quantum memristors for neuromorphic quantum machine learning", "abstract": "Quantum machine learning may permit to realize more efficient machine\nlearning calculations with near-term quantum devices. Among the diverse quantum\nmachine learning paradigms which are currently being considered, quantum\nmemristors are promising as a way of combining, in the same quantum hardware, a\nunitary evolution with the nonlinearity provided by the measurement and\nfeedforward. Thus, an efficient way of deploying neuromorphic quantum computing\nfor quantum machine learning may be enabled.", "authors": ["Lucas Lamata"], "year": "2024-12-25T20:21:24+00:00", "updated": "2024-12-25T20:21:24+00:00", "categories": ["quant-ph", "cs.NE"], "url": "http://arxiv.org/pdf/2412.18979v1", "entry_id": "http://arxiv.org/abs/2412.18979v1"}, {"paper_id": "1707.09562", "filepath": "papers/1707.09562.pdf", "title": "MLBench: How Good Are Machine Learning Clouds for Binary Classification Tasks on Structured Data?", "abstract": "We conduct an empirical study of machine learning functionalities provided by\nmajor cloud service providers, which we call machine learning clouds. Machine\nlearning clouds hold the promise of hiding all the sophistication of running\nlarge-scale machine learning: Instead of specifying how to run a machine\nlearning task, users only specify what machine learning task to run and the\ncloud figures out the rest. Raising the level of abstraction, however, rarely\ncomes free - a performance penalty is possible. How good, then, are current\nmachine learning clouds on real-world machine learning workloads?\n  We study this question with a focus on binary classication problems. We\npresent mlbench, a novel benchmark constructed by harvesting datasets from\nKaggle competitions. We then compare the performance of the top winning code\navailable from Kaggle with that of running machine learning clouds from both\nAzure and Amazon on mlbench. Our comparative study reveals the strength and\nweakness of existing machine learning clouds and points out potential future\ndirections for improvement.", "authors": ["Yu Liu", "Hantian Zhang", "Luyuan Zeng", "Wentao Wu", "Ce Zhang"], "year": "2017-07-29T21:59:18+00:00", "updated": "2017-10-16T11:13:32+00:00", "categories": ["cs.DC", "cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1707.09562v3", "entry_id": "http://arxiv.org/abs/1707.09562v3"}, {"paper_id": "1812.01410", "filepath": "papers/1812.01410.pdf", "title": "Compressive Classification (Machine Learning without learning)", "abstract": "Compressive learning is a framework where (so far unsupervised) learning\ntasks use not the entire dataset but a compressed summary (sketch) of it. We\npropose a compressive learning classification method, and a novel sketch\nfunction for images.", "authors": ["Vincent Schellekens", "Laurent Jacques"], "year": "2018-12-04T13:50:11+00:00", "updated": "2018-12-04T13:50:11+00:00", "categories": ["cs.LG", "cs.CV", "stat.ML"], "url": "http://arxiv.org/pdf/1812.01410v1", "entry_id": "http://arxiv.org/abs/1812.01410v1"}, {"paper_id": "1907.08908", "filepath": "papers/1907.08908.pdf", "title": "Techniques for Automated Machine Learning", "abstract": "Automated machine learning (AutoML) aims to find optimal machine learning\nsolutions automatically given a machine learning problem. It could release the\nburden of data scientists from the multifarious manual tuning process and\nenable the access of domain experts to the off-the-shelf machine learning\nsolutions without extensive experience. In this paper, we review the current\ndevelopments of AutoML in terms of three categories, automated feature\nengineering (AutoFE), automated model and hyperparameter learning (AutoMHL),\nand automated deep learning (AutoDL). State-of-the-art techniques adopted in\nthe three categories are presented, including Bayesian optimization,\nreinforcement learning, evolutionary algorithm, and gradient-based approaches.\nWe summarize popular AutoML frameworks and conclude with current open\nchallenges of AutoML.", "authors": ["Yi-Wei Chen", "Qingquan Song", "Xia Hu"], "year": "2019-07-21T04:03:36+00:00", "updated": "2019-07-21T04:03:36+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML"], "url": "http://arxiv.org/pdf/1907.08908v1", "entry_id": "http://arxiv.org/abs/1907.08908v1"}, {"paper_id": "2312.14050", "filepath": "papers/2312.14050.pdf", "title": "Machine learning and domain decomposition methods -- a survey", "abstract": "Hybrid algorithms, which combine black-box machine learning methods with\nexperience from traditional numerical methods and domain expertise from diverse\napplication areas, are progressively gaining importance in scientific machine\nlearning and various industrial domains, especially in computational science\nand engineering. In the present survey, several promising avenues of research\nwill be examined which focus on the combination of machine learning (ML) and\ndomain decomposition methods (DDMs). The aim of this survey is to provide an\noverview of existing work within this field and to structure it into domain\ndecomposition for machine learning and machine learning-enhanced domain\ndecomposition, including: domain decomposition for classical machine learning,\ndomain decomposition to accelerate the training of physics-aware neural\nnetworks, machine learning to enhance the convergence properties or\ncomputational efficiency of DDMs, and machine learning as a discretization\nmethod in a DDM for the solution of PDEs. In each of these fields, we summarize\nexisting work and key advances within a common framework and, finally, disuss\nongoing challenges and opportunities for future research.", "authors": ["Axel Klawonn", "Martin Lanser", "Janine Weber"], "year": "2023-12-21T17:19:27+00:00", "updated": "2023-12-21T17:19:27+00:00", "categories": ["math.NA", "cs.LG", "cs.NA", "65F10, 65N22, 65N55, 68T05, 68T07"], "url": "http://arxiv.org/pdf/2312.14050v1", "entry_id": "http://arxiv.org/abs/2312.14050v1"}, {"paper_id": "2008.08080", "filepath": "papers/2008.08080.pdf", "title": "mlr3proba: An R Package for Machine Learning in Survival Analysis", "abstract": "As machine learning has become increasingly popular over the last few\ndecades, so too has the number of machine learning interfaces for implementing\nthese models. Whilst many R libraries exist for machine learning, very few\noffer extended support for survival analysis. This is problematic considering\nits importance in fields like medicine, bioinformatics, economics, engineering,\nand more. mlr3proba provides a comprehensive machine learning interface for\nsurvival analysis and connects with mlr3's general model tuning and\nbenchmarking facilities to provide a systematic infrastructure for survival\nmodeling and evaluation.", "authors": ["Raphael Sonabend", "Franz J. Kir\u00e1ly", "Andreas Bender", "Bernd Bischl", "Michel Lang"], "year": "2020-08-18T11:21:24+00:00", "updated": "2020-12-14T11:41:25+00:00", "categories": ["stat.CO", "cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/2008.08080v2", "entry_id": "http://arxiv.org/abs/2008.08080v2"}, {"paper_id": "2007.01977", "filepath": "papers/2007.01977.pdf", "title": "Lale: Consistent Automated Machine Learning", "abstract": "Automated machine learning makes it easier for data scientists to develop\npipelines by searching over possible choices for hyperparameters, algorithms,\nand even pipeline topologies. Unfortunately, the syntax for automated machine\nlearning tools is inconsistent with manual machine learning, with each other,\nand with error checks. Furthermore, few tools support advanced features such as\ntopology search or higher-order operators. This paper introduces Lale, a\nlibrary of high-level Python interfaces that simplifies and unifies automated\nmachine learning in a consistent way.", "authors": ["Guillaume Baudart", "Martin Hirzel", "Kiran Kate", "Parikshit Ram", "Avraham Shinnar"], "year": "2020-07-04T00:55:41+00:00", "updated": "2020-07-04T00:55:41+00:00", "categories": ["cs.LG", "cs.AI"], "url": "http://arxiv.org/pdf/2007.01977v1", "entry_id": "http://arxiv.org/abs/2007.01977v1"}, {"paper_id": "2209.02057", "filepath": "papers/2209.02057.pdf", "title": "Applying Machine Learning to Life Insurance: some knowledge sharing to master it", "abstract": "Machine Learning permeates many industries, which brings new source of\nbenefits for companies. However within the life insurance industry, Machine\nLearning is not widely used in practice as over the past years statistical\nmodels have shown their efficiency for risk assessment. Thus insurers may face\ndifficulties to assess the value of the artificial intelligence. Focusing on\nthe modification of the life insurance industry over time highlights the stake\nof using Machine Learning for insurers and benefits that it can bring by\nunleashing data value. This paper reviews traditional actuarial methodologies\nfor survival modeling and extends them with Machine Learning techniques. It\npoints out differences with regular machine learning models and emphasizes\nimportance of specific implementations to face censored data with machine\nlearning models family. In complement to this article, a Python library has\nbeen developed. Different open-source Machine Learning algorithms have been\nadjusted to adapt the specificities of life insurance data, namely censoring\nand truncation. Such models can be easily applied from this SCOR library to\naccurately model life insurance risks.", "authors": ["Antoine Chancel", "Laura Bradier", "Antoine Ly", "Razvan Ionescu", "Laurene Martin", "Marguerite Sauce"], "year": "2022-09-05T17:09:03+00:00", "updated": "2022-09-27T18:42:50+00:00", "categories": ["stat.ML", "cs.CY", "cs.LG", "stat.AP"], "url": "http://arxiv.org/pdf/2209.02057v3", "entry_id": "http://arxiv.org/abs/2209.02057v3"}, {"paper_id": "2407.19890", "filepath": "papers/2407.19890.pdf", "title": "Quantum Dynamics of Machine Learning", "abstract": "The quantum dynamic equation (QDE) of machine learning is obtained based on\nSchr\\\"odinger equation and potential energy equivalence relationship. Through\nWick rotation, the relationship between quantum dynamics and thermodynamics is\nalso established in this paper. This equation reformulates the iterative\nprocess of machine learning into a time-dependent partial differential equation\nwith a clear mathematical structure, offering a theoretical framework for\ninvestigating machine learning iterations through quantum and mathematical\ntheories. Within this framework, the fundamental iterative process, the\ndiffusion model, and the Softmax and Sigmoid functions are examined, validating\nthe proposed quantum dynamics equations. This approach not only presents a\nrigorous theoretical foundation for machine learning but also holds promise for\nsupporting the implementation of machine learning algorithms on quantum\ncomputers.", "authors": ["Peng Wang", "Maimaitiniyazi Maimaitiabudula"], "year": "2024-07-07T16:30:46+00:00", "updated": "2024-07-07T16:30:46+00:00", "categories": ["quant-ph", "cs.LG"], "url": "http://arxiv.org/pdf/2407.19890v1", "entry_id": "http://arxiv.org/abs/2407.19890v1"}, {"paper_id": "0904.3664", "filepath": "papers/0904.3664.pdf", "title": "Introduction to Machine Learning: Class Notes 67577", "abstract": "Introduction to Machine learning covering Statistical Inference (Bayes, EM,\nML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),\nand PAC learning (the Formal model, VC dimension, Double Sampling theorem).", "authors": ["Amnon Shashua"], "year": "2009-04-23T11:40:57+00:00", "updated": "2009-04-23T11:40:57+00:00", "categories": ["cs.LG"], "url": "http://arxiv.org/pdf/0904.3664v1", "entry_id": "http://arxiv.org/abs/0904.3664v1"}, {"paper_id": "2306.14624", "filepath": "papers/2306.14624.pdf", "title": "Insights From Insurance for Fair Machine Learning", "abstract": "We argue that insurance can act as an analogon for the social situatedness of\nmachine learning systems, hence allowing machine learning scholars to take\ninsights from the rich and interdisciplinary insurance literature. Tracing the\ninteraction of uncertainty, fairness and responsibility in insurance provides a\nfresh perspective on fairness in machine learning. We link insurance fairness\nconceptions to their machine learning relatives, and use this bridge to\nproblematize fairness as calibration. In this process, we bring to the\nforefront two themes that have been largely overlooked in the machine learning\nliterature: responsibility and aggregate-individual tensions.", "authors": ["Christian Fr\u00f6hlich", "Robert C. Williamson"], "year": "2023-06-26T11:56:00+00:00", "updated": "2024-01-23T09:43:23+00:00", "categories": ["cs.LG", "cs.CY"], "url": "http://arxiv.org/pdf/2306.14624v2", "entry_id": "http://arxiv.org/abs/2306.14624v2"}, {"paper_id": "2105.03726", "filepath": "papers/2105.03726.pdf", "title": "Mental Models of Adversarial Machine Learning", "abstract": "Although machine learning is widely used in practice, little is known about\npractitioners' understanding of potential security challenges. In this work, we\nclose this substantial gap and contribute a qualitative study focusing on\ndevelopers' mental models of the machine learning pipeline and potentially\nvulnerable components. Similar studies have helped in other security fields to\ndiscover root causes or improve risk communication. Our study reveals two\n\\facets of practitioners' mental models of machine learning security. Firstly,\npractitioners often confuse machine learning security with threats and defences\nthat are not directly related to machine learning. Secondly, in contrast to\nmost academic research, our participants perceive security of machine learning\nas not solely related to individual models, but rather in the context of entire\nworkflows that consist of multiple components. Jointly with our additional\nfindings, these two facets provide a foundation to substantiate mental models\nfor machine learning security and have implications for the integration of\nadversarial machine learning into corporate workflows, \\new{decreasing\npractitioners' reported uncertainty}, and appropriate regulatory frameworks for\nmachine learning security.", "authors": ["Lukas Bieringer", "Kathrin Grosse", "Michael Backes", "Battista Biggio", "Katharina Krombholz"], "year": "2021-05-08T16:05:07+00:00", "updated": "2022-06-29T13:42:12+00:00", "categories": ["cs.CR", "cs.AI"], "url": "http://arxiv.org/pdf/2105.03726v4", "entry_id": "http://arxiv.org/abs/2105.03726v4"}, {"paper_id": "2006.15680", "filepath": "papers/2006.15680.pdf", "title": "Modeling Generalization in Machine Learning: A Methodological and Computational Study", "abstract": "As machine learning becomes more and more available to the general public,\ntheoretical questions are turning into pressing practical issues. Possibly, one\nof the most relevant concerns is the assessment of our confidence in trusting\nmachine learning predictions. In many real-world cases, it is of utmost\nimportance to estimate the capabilities of a machine learning algorithm to\ngeneralize, i.e., to provide accurate predictions on unseen data, depending on\nthe characteristics of the target problem. In this work, we perform a\nmeta-analysis of 109 publicly-available classification data sets, modeling\nmachine learning generalization as a function of a variety of data set\ncharacteristics, ranging from number of samples to intrinsic dimensionality,\nfrom class-wise feature skewness to $F1$ evaluated on test samples falling\noutside the convex hull of the training set. Experimental results demonstrate\nthe relevance of using the concept of the convex hull of the training data in\nassessing machine learning generalization, by emphasizing the difference\nbetween interpolated and extrapolated predictions. Besides several predictable\ncorrelations, we observe unexpectedly weak associations between the\ngeneralization ability of machine learning models and all metrics related to\ndimensionality, thus challenging the common assumption that the \\textit{curse\nof dimensionality} might impair generalization in machine learning.", "authors": ["Pietro Barbiero", "Giovanni Squillero", "Alberto Tonda"], "year": "2020-06-28T19:06:16+00:00", "updated": "2020-06-28T19:06:16+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/2006.15680v1", "entry_id": "http://arxiv.org/abs/2006.15680v1"}, {"paper_id": "1911.08587", "filepath": "papers/1911.08587.pdf", "title": "Solving machine learning optimization problems using quantum computers", "abstract": "Classical optimization algorithms in machine learning often take a long time\nto compute when applied to a multi-dimensional problem and require a huge\namount of CPU and GPU resource. Quantum parallelism has a potential to speed up\nmachine learning algorithms. We describe a generic mathematical model to\nleverage quantum parallelism to speed-up machine learning algorithms. We also\napply quantum machine learning and quantum parallelism applied to a\n$3$-dimensional image that vary with time.", "authors": ["Venkat R. Dasari", "Mee Seong Im", "Lubjana Beshaj"], "year": "2019-11-17T17:36:41+00:00", "updated": "2019-11-17T17:36:41+00:00", "categories": ["quant-ph", "cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1911.08587v1", "entry_id": "http://arxiv.org/abs/1911.08587v1"}, {"paper_id": "1903.08801", "filepath": "papers/1903.08801.pdf", "title": "A Unified Analytical Framework for Trustable Machine Learning and Automation Running with Blockchain", "abstract": "Traditional machine learning algorithms use data from databases that are\nmutable, and therefore the data cannot be fully trusted. Also, the machine\nlearning process is difficult to automate. This paper proposes building a\ntrustable machine learning system by using blockchain technology, which can\nstore data in a permanent and immutable way. In addition, smart contracts are\nused to automate the machine learning process. This paper makes three\ncontributions. First, it establishes a link between machine learning technology\nand blockchain technology. Previously, machine learning and blockchain have\nbeen considered two independent technologies without an obvious link. Second,\nit proposes a unified analytical framework for trustable machine learning by\nusing blockchain technology. This unified framework solves both the\ntrustability and automation issues in machine learning. Third, it enables a\ncomputer to translate core machine learning implementation from a single thread\non a single machine to multiple threads on multiple machines running with\nblockchain by using a unified approach. The paper uses association rule mining\nas an example to demonstrate how trustable machine learning can be implemented\nwith blockchain, and it shows how this approach can be used to analyze opioid\nprescriptions to help combat the opioid crisis.", "authors": ["Tao Wang"], "year": "2019-03-21T02:17:08+00:00", "updated": "2019-03-21T02:17:08+00:00", "categories": ["cs.LG", "cs.CR"], "url": "http://arxiv.org/pdf/1903.08801v1", "entry_id": "http://arxiv.org/abs/1903.08801v1"}, {"paper_id": "1808.00033", "filepath": "papers/1808.00033.pdf", "title": "Techniques for Interpretable Machine Learning", "abstract": "Interpretable machine learning tackles the important problem that humans\ncannot understand the behaviors of complex machine learning models and how\nthese models arrive at a particular decision. Although many approaches have\nbeen proposed, a comprehensive understanding of the achievements and challenges\nis still lacking. We provide a survey covering existing techniques to increase\nthe interpretability of machine learning models. We also discuss crucial issues\nthat the community should consider in future work such as designing\nuser-friendly explanations and developing comprehensive evaluation metrics to\nfurther push forward the area of interpretable machine learning.", "authors": ["Mengnan Du", "Ninghao Liu", "Xia Hu"], "year": "2018-07-31T19:14:39+00:00", "updated": "2019-05-19T20:44:37+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML"], "url": "http://arxiv.org/pdf/1808.00033v3", "entry_id": "http://arxiv.org/abs/1808.00033v3"}, {"paper_id": "2003.10146", "filepath": "papers/2003.10146.pdf", "title": "Julia Language in Machine Learning: Algorithms, Applications, and Open Issues", "abstract": "Machine learning is driving development across many fields in science and\nengineering. A simple and efficient programming language could accelerate\napplications of machine learning in various fields. Currently, the programming\nlanguages most commonly used to develop machine learning algorithms include\nPython, MATLAB, and C/C ++. However, none of these languages well balance both\nefficiency and simplicity. The Julia language is a fast, easy-to-use, and\nopen-source programming language that was originally designed for\nhigh-performance computing, which can well balance the efficiency and\nsimplicity. This paper summarizes the related research work and developments in\nthe application of the Julia language in machine learning. It first surveys the\npopular machine learning algorithms that are developed in the Julia language.\nThen, it investigates applications of the machine learning algorithms\nimplemented with the Julia language. Finally, it discusses the open issues and\nthe potential future directions that arise in the use of the Julia language in\nmachine learning.", "authors": ["Kaifeng Gao", "Gang Mei", "Francesco Piccialli", "Salvatore Cuomo", "Jingzhi Tu", "Zenan Huo"], "year": "2020-03-23T09:31:02+00:00", "updated": "2020-05-17T10:52:22+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/2003.10146v2", "entry_id": "http://arxiv.org/abs/2003.10146v2"}, {"paper_id": "2007.07981", "filepath": "papers/2007.07981.pdf", "title": "Differential Replication in Machine Learning", "abstract": "When deployed in the wild, machine learning models are usually confronted\nwith data and requirements that constantly vary, either because of changes in\nthe generating distribution or because external constraints change the\nenvironment where the model operates. To survive in such an ecosystem, machine\nlearning models need to adapt to new conditions by evolving over time. The idea\nof model adaptability has been studied from different perspectives. In this\npaper, we propose a solution based on reusing the knowledge acquired by the\nalready deployed machine learning models and leveraging it to train future\ngenerations. This is the idea behind differential replication of machine\nlearning models.", "authors": ["Irene Unceta", "Jordi Nin", "Oriol Pujol"], "year": "2020-07-15T20:26:49+00:00", "updated": "2020-07-15T20:26:49+00:00", "categories": ["cs.LG", "stat.ML", "cs.LG, stat.ML"], "url": "http://arxiv.org/pdf/2007.07981v1", "entry_id": "http://arxiv.org/abs/2007.07981v1"}, {"paper_id": "1912.09630", "filepath": "papers/1912.09630.pdf", "title": "Practical Solutions for Machine Learning Safety in Autonomous Vehicles", "abstract": "Autonomous vehicles rely on machine learning to solve challenging tasks in\nperception and motion planning. However, automotive software safety standards\nhave not fully evolved to address the challenges of machine learning safety\nsuch as interpretability, verification, and performance limitations. In this\npaper, we review and organize practical machine learning safety techniques that\ncan complement engineering safety for machine learning based software in\nautonomous vehicles. Our organization maps safety strategies to\nstate-of-the-art machine learning techniques in order to enhance dependability\nand safety of machine learning algorithms. We also discuss security limitations\nand user experience aspects of machine learning components in autonomous\nvehicles.", "authors": ["Sina Mohseni", "Mandar Pitale", "Vasu Singh", "Zhangyang Wang"], "year": "2019-12-20T03:47:28+00:00", "updated": "2019-12-20T03:47:28+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1912.09630v1", "entry_id": "http://arxiv.org/abs/1912.09630v1"}, {"paper_id": "1803.10311", "filepath": "papers/1803.10311.pdf", "title": "How Developers Iterate on Machine Learning Workflows -- A Survey of the Applied Machine Learning Literature", "abstract": "Machine learning workflow development is anecdotally regarded to be an\niterative process of trial-and-error with humans-in-the-loop. However, we are\nnot aware of quantitative evidence corroborating this popular belief. A\nquantitative characterization of iteration can serve as a benchmark for machine\nlearning workflow development in practice, and can aid the development of\nhuman-in-the-loop machine learning systems. To this end, we conduct a\nsmall-scale survey of the applied machine learning literature from five\ndistinct application domains. We collect and distill statistics on the role of\niteration within machine learning workflow development, and report preliminary\ntrends and insights from our investigation, as a starting point towards this\nbenchmark. Based on our findings, we finally describe desiderata for effective\nand versatile human-in-the-loop machine learning systems that can cater to\nusers in diverse domains.", "authors": ["Doris Xin", "Litian Ma", "Shuchen Song", "Aditya Parameswaran"], "year": "2018-03-27T20:38:05+00:00", "updated": "2018-05-17T22:16:31+00:00", "categories": ["cs.LG", "cs.DB", "cs.HC", "stat.ML"], "url": "http://arxiv.org/pdf/1803.10311v2", "entry_id": "http://arxiv.org/abs/1803.10311v2"}, {"paper_id": "1909.03550", "filepath": "papers/1909.03550.pdf", "title": "Lecture Notes: Optimization for Machine Learning", "abstract": "Lecture notes on optimization for machine learning, derived from a course at\nPrinceton University and tutorials given in MLSS, Buenos Aires, as well as\nSimons Foundation, Berkeley.", "authors": ["Elad Hazan"], "year": "2019-09-08T21:49:42+00:00", "updated": "2019-09-08T21:49:42+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1909.03550v1", "entry_id": "http://arxiv.org/abs/1909.03550v1"}, {"paper_id": "1810.11383", "filepath": "papers/1810.11383.pdf", "title": "Some Requests for Machine Learning Research from the East African Tech Scene", "abstract": "Based on 46 in-depth interviews with scientists, engineers, and CEOs, this\ndocument presents a list of concrete machine research problems, progress on\nwhich would directly benefit tech ventures in East Africa.", "authors": ["Milan Cvitkovic"], "year": "2018-10-25T02:53:14+00:00", "updated": "2018-11-08T01:03:50+00:00", "categories": ["cs.LG", "cs.CY", "stat.ML"], "url": "http://arxiv.org/pdf/1810.11383v2", "entry_id": "http://arxiv.org/abs/1810.11383v2"}, {"paper_id": "1702.08608", "filepath": "papers/1702.08608.pdf", "title": "Towards A Rigorous Science of Interpretable Machine Learning", "abstract": "As machine learning systems become ubiquitous, there has been a surge of\ninterest in interpretable machine learning: systems that provide explanation\nfor their outputs. These explanations are often used to qualitatively assess\nother criteria such as safety or non-discrimination. However, despite the\ninterest in interpretability, there is very little consensus on what\ninterpretable machine learning is and how it should be measured. In this\nposition paper, we first define interpretability and describe when\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\nfor rigorous evaluation and expose open questions towards a more rigorous\nscience of interpretable machine learning.", "authors": ["Finale Doshi-Velez", "Been Kim"], "year": "2017-02-28T02:19:20+00:00", "updated": "2017-03-02T19:32:10+00:00", "categories": ["stat.ML", "cs.AI", "cs.LG"], "url": "http://arxiv.org/pdf/1702.08608v2", "entry_id": "http://arxiv.org/abs/1702.08608v2"}, {"paper_id": "1605.07805", "filepath": "papers/1605.07805.pdf", "title": "Learning Moore Machines from Input-Output Traces", "abstract": "The problem of learning automata from example traces (but no equivalence or\nmembership queries) is fundamental in automata learning theory and practice. In\nthis paper we study this problem for finite state machines with inputs and\noutputs, and in particular for Moore machines. We develop three algorithms for\nsolving this problem: (1) the PTAP algorithm, which transforms a set of\ninput-output traces into an incomplete Moore machine and then completes the\nmachine with self-loops; (2) the PRPNI algorithm, which uses the well-known\nRPNI algorithm for automata learning to learn a product of automata encoding a\nMoore machine; and (3) the MooreMI algorithm, which directly learns a Moore\nmachine using PTAP extended with state merging. We prove that MooreMI has the\nfundamental identification in the limit property. We also compare the\nalgorithms experimentally in terms of the size of the learned machine and\nseveral notions of accuracy, introduced in this paper. Finally, we compare with\nOSTIA, an algorithm that learns a more general class of transducers, and find\nthat OSTIA generally does not learn a Moore machine, even when fed with a\ncharacteristic sample.", "authors": ["Georgios Giantamidis", "Stavros Tripakis"], "year": "2016-05-25T10:11:03+00:00", "updated": "2016-09-02T09:27:40+00:00", "categories": ["cs.FL", "cs.LG"], "url": "http://arxiv.org/pdf/1605.07805v2", "entry_id": "http://arxiv.org/abs/1605.07805v2"}, {"paper_id": "2110.12773", "filepath": "papers/2110.12773.pdf", "title": "Scientific Machine Learning Benchmarks", "abstract": "The breakthrough in Deep Learning neural networks has transformed the use of\nAI and machine learning technologies for the analysis of very large\nexperimental datasets. These datasets are typically generated by large-scale\nexperimental facilities at national laboratories. In the context of science,\nscientific machine learning focuses on training machines to identify patterns,\ntrends, and anomalies to extract meaningful scientific insights from such\ndatasets. With a new generation of experimental facilities, the rate of data\ngeneration and the scale of data volumes will increasingly require the use of\nmore automated data analysis. At present, identifying the most appropriate\nmachine learning algorithm for the analysis of any given scientific dataset is\nstill a challenge for scientists. This is due to many different machine\nlearning frameworks, computer architectures, and machine learning models.\nHistorically, for modelling and simulation on HPC systems such problems have\nbeen addressed through benchmarking computer applications, algorithms, and\narchitectures. Extending such a benchmarking approach and identifying metrics\nfor the application of machine learning methods to scientific datasets is a new\nchallenge for both scientists and computer scientists. In this paper, we\ndescribe our approach to the development of scientific machine learning\nbenchmarks and review other approaches to benchmarking scientific machine\nlearning.", "authors": ["Jeyan Thiyagalingam", "Mallikarjun Shankar", "Geoffrey Fox", "Tony Hey"], "year": "2021-10-25T10:05:11+00:00", "updated": "2021-10-25T10:05:11+00:00", "categories": ["cs.LG", "physics.comp-ph", "I.2"], "url": "http://arxiv.org/pdf/2110.12773v1", "entry_id": "http://arxiv.org/abs/2110.12773v1"}, {"paper_id": "2407.05520", "filepath": "papers/2407.05520.pdf", "title": "A Theory of Machine Learning", "abstract": "We critically review three major theories of machine learning and provide a\nnew theory according to which machines learn a function when the machines\nsuccessfully compute it. We show that this theory challenges common assumptions\nin the statistical and the computational learning theories, for it implies that\nlearning true probabilities is equivalent neither to obtaining a correct\ncalculation of the true probabilities nor to obtaining an almost-sure\nconvergence to them. We also briefly discuss some case studies from natural\nlanguage processing and macroeconomics from the perspective of the new theory.", "authors": ["Jinsook Kim", "Jinho Kang"], "year": "2024-07-07T23:57:10+00:00", "updated": "2024-07-07T23:57:10+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/2407.05520v1", "entry_id": "http://arxiv.org/abs/2407.05520v1"}, {"paper_id": "2407.05526", "filepath": "papers/2407.05526.pdf", "title": "Can Machines Learn the True Probabilities?", "abstract": "When there exists uncertainty, AI machines are designed to make decisions so\nas to reach the best expected outcomes. Expectations are based on true facts\nabout the objective environment the machines interact with, and those facts can\nbe encoded into AI models in the form of true objective probability functions.\nAccordingly, AI models involve probabilistic machine learning in which the\nprobabilities should be objectively interpreted. We prove under some basic\nassumptions when machines can learn the true objective probabilities, if any,\nand when machines cannot learn them.", "authors": ["Jinsook Kim"], "year": "2024-07-08T00:19:43+00:00", "updated": "2024-07-08T00:19:43+00:00", "categories": ["cs.LG", "cs.AI", "stat.ML"], "url": "http://arxiv.org/pdf/2407.05526v1", "entry_id": "http://arxiv.org/abs/2407.05526v1"}, {"paper_id": "1705.07538", "filepath": "papers/1705.07538.pdf", "title": "Infrastructure for Usable Machine Learning: The Stanford DAWN Project", "abstract": "Despite incredible recent advances in machine learning, building machine\nlearning applications remains prohibitively time-consuming and expensive for\nall but the best-trained, best-funded engineering organizations. This expense\ncomes not from a need for new and improved statistical models but instead from\na lack of systems and tools for supporting end-to-end machine learning\napplication development, from data preparation and labeling to\nproductionization and monitoring. In this document, we outline opportunities\nfor infrastructure supporting usable, end-to-end machine learning applications\nin the context of the nascent DAWN (Data Analytics for What's Next) project at\nStanford.", "authors": ["Peter Bailis", "Kunle Olukotun", "Christopher Re", "Matei Zaharia"], "year": "2017-05-22T02:28:19+00:00", "updated": "2017-06-09T02:13:09+00:00", "categories": ["cs.LG", "cs.DB", "stat.ML"], "url": "http://arxiv.org/pdf/1705.07538v2", "entry_id": "http://arxiv.org/abs/1705.07538v2"}, {"paper_id": "2301.09753", "filepath": "papers/2301.09753.pdf", "title": "Towards Modular Machine Learning Solution Development: Benefits and Trade-offs", "abstract": "Machine learning technologies have demonstrated immense capabilities in\nvarious domains. They play a key role in the success of modern businesses.\nHowever, adoption of machine learning technologies has a lot of untouched\npotential. Cost of developing custom machine learning solutions that solve\nunique business problems is a major inhibitor to far-reaching adoption of\nmachine learning technologies. We recognize that the monolithic nature\nprevalent in today's machine learning applications stands in the way of\nefficient and cost effective customized machine learning solution development.\nIn this work we explore the benefits of modular machine learning solutions and\ndiscuss how modular machine learning solutions can overcome some of the major\nsolution engineering limitations of monolithic machine learning solutions. We\nanalyze the trade-offs between modular and monolithic machine learning\nsolutions through three deep learning problems; one text based and the two\nimage based. Our experimental results show that modular machine learning\nsolutions have a promising potential to reap the solution engineering\nadvantages of modularity while gaining performance and data advantages in a way\nthe monolithic machine learning solutions do not permit.", "authors": ["Samiyuru Menik", "Lakshmish Ramaswamy"], "year": "2023-01-23T22:54:34+00:00", "updated": "2023-01-23T22:54:34+00:00", "categories": ["cs.LG", "cs.SE"], "url": "http://arxiv.org/pdf/2301.09753v1", "entry_id": "http://arxiv.org/abs/2301.09753v1"}, {"paper_id": "2001.09608", "filepath": "papers/2001.09608.pdf", "title": "Some Insights into Lifelong Reinforcement Learning Systems", "abstract": "A lifelong reinforcement learning system is a learning system that has the\nability to learn through trail-and-error interaction with the environment over\nits lifetime. In this paper, I give some arguments to show that the traditional\nreinforcement learning paradigm fails to model this type of learning system.\nSome insights into lifelong reinforcement learning are provided, along with a\nsimplistic prototype lifelong reinforcement learning system.", "authors": ["Changjian Li"], "year": "2020-01-27T07:26:12+00:00", "updated": "2020-01-27T07:26:12+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/2001.09608v1", "entry_id": "http://arxiv.org/abs/2001.09608v1"}, {"paper_id": "2303.09491", "filepath": "papers/2303.09491.pdf", "title": "Challenges and Opportunities in Quantum Machine Learning", "abstract": "At the intersection of machine learning and quantum computing, Quantum\nMachine Learning (QML) has the potential of accelerating data analysis,\nespecially for quantum data, with applications for quantum materials,\nbiochemistry, and high-energy physics. Nevertheless, challenges remain\nregarding the trainability of QML models. Here we review current methods and\napplications for QML. We highlight differences between quantum and classical\nmachine learning, with a focus on quantum neural networks and quantum deep\nlearning. Finally, we discuss opportunities for quantum advantage with QML.", "authors": ["M. Cerezo", "Guillaume Verdon", "Hsin-Yuan Huang", "Lukasz Cincio", "Patrick J. Coles"], "year": "2023-03-16T17:10:39+00:00", "updated": "2023-03-16T17:10:39+00:00", "categories": ["quant-ph", "cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/2303.09491v1", "entry_id": "http://arxiv.org/abs/2303.09491v1"}, {"paper_id": "2103.11249", "filepath": "papers/2103.11249.pdf", "title": "SELM: Software Engineering of Machine Learning Models", "abstract": "One of the pillars of any machine learning model is its concepts. Using\nsoftware engineering, we can engineer these concepts and then develop and\nexpand them. In this article, we present a SELM framework for Software\nEngineering of machine Learning Models. We then evaluate this framework through\na case study. Using the SELM framework, we can improve a machine learning\nprocess efficiency and provide more accuracy in learning with less processing\nhardware resources and a smaller training dataset. This issue highlights the\nimportance of an interdisciplinary approach to machine learning. Therefore, in\nthis article, we have provided interdisciplinary teams' proposals for machine\nlearning.", "authors": ["Nafiseh Jafari", "Mohammad Reza Besharati", "Mohammad Izadi", "Maryam Hourali"], "year": "2021-03-20T21:43:24+00:00", "updated": "2021-03-20T21:43:24+00:00", "categories": ["cs.SE", "cs.AI"], "url": "http://arxiv.org/pdf/2103.11249v1", "entry_id": "http://arxiv.org/abs/2103.11249v1"}, {"paper_id": "2001.11489", "filepath": "papers/2001.11489.pdf", "title": "Machine Learning in Network Security Using KNIME Analytics", "abstract": "Machine learning has more and more effect on our every day's life. This field\nkeeps growing and expanding into new areas. Machine learning is based on the\nimplementation of artificial intelligence that gives systems the capability to\nautomatically learn and enhance from experiments without being explicitly\nprogrammed. Machine Learning algorithms apply mathematical equations to analyze\ndatasets and predict values based on the dataset. In the field of\ncybersecurity, machine learning algorithms can be utilized to train and analyze\nthe Intrusion Detection Systems (IDSs) on security-related datasets. In this\npaper, we tested different machine learning algorithms to analyze NSL-KDD\ndataset using KNIME analytics.", "authors": ["Munther Abualkibash"], "year": "2019-11-18T14:10:17+00:00", "updated": "2019-11-18T14:10:17+00:00", "categories": ["cs.CR"], "url": "http://arxiv.org/pdf/2001.11489v1", "entry_id": "http://arxiv.org/abs/2001.11489v1"}, {"paper_id": "1908.04710", "filepath": "papers/1908.04710.pdf", "title": "metric-learn: Metric Learning Algorithms in Python", "abstract": "metric-learn is an open source Python package implementing supervised and\nweakly-supervised distance metric learning algorithms. As part of\nscikit-learn-contrib, it provides a unified interface compatible with\nscikit-learn which allows to easily perform cross-validation, model selection,\nand pipelining with other machine learning estimators. metric-learn is\nthoroughly tested and available on PyPi under the MIT licence.", "authors": ["William de Vazelhes", "CJ Carey", "Yuan Tang", "Nathalie Vauquier", "Aur\u00e9lien Bellet"], "year": "2019-08-13T15:52:31+00:00", "updated": "2020-07-27T14:47:52+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1908.04710v3", "entry_id": "http://arxiv.org/abs/1908.04710v3"}, {"paper_id": "1510.00633", "filepath": "papers/1510.00633.pdf", "title": "Distributed Multitask Learning", "abstract": "We consider the problem of distributed multi-task learning, where each\nmachine learns a separate, but related, task. Specifically, each machine learns\na linear predictor in high-dimensional space,where all tasks share the same\nsmall support. We present a communication-efficient estimator based on the\ndebiased lasso and show that it is comparable with the optimal centralized\nmethod.", "authors": ["Jialei Wang", "Mladen Kolar", "Nathan Srebro"], "year": "2015-10-02T16:15:30+00:00", "updated": "2015-10-02T16:15:30+00:00", "categories": ["stat.ML", "cs.LG"], "url": "http://arxiv.org/pdf/1510.00633v1", "entry_id": "http://arxiv.org/abs/1510.00633v1"}, {"paper_id": "1910.02544", "filepath": "papers/1910.02544.pdf", "title": "Using Deep Learning and Machine Learning to Detect Epileptic Seizure with Electroencephalography (EEG) Data", "abstract": "The prediction of epileptic seizure has always been extremely challenging in\nmedical domain. However, as the development of computer technology, the\napplication of machine learning introduced new ideas for seizure forecasting.\nApplying machine learning model onto the predication of epileptic seizure could\nhelp us obtain a better result and there have been plenty of scientists who\nhave been doing such works so that there are sufficient medical data provided\nfor researchers to do training of machine learning models.", "authors": ["Haotian Liu", "Lin Xi", "Ying Zhao", "Zhixiang Li"], "year": "2019-10-06T22:53:28+00:00", "updated": "2019-10-06T22:53:28+00:00", "categories": ["cs.LG", "eess.SP", "stat.ML"], "url": "http://arxiv.org/pdf/1910.02544v1", "entry_id": "http://arxiv.org/abs/1910.02544v1"}, {"paper_id": "1908.00868", "filepath": "papers/1908.00868.pdf", "title": "Machine Learning as Ecology", "abstract": "Machine learning methods have had spectacular success on numerous problems.\nHere we show that a prominent class of learning algorithms - including Support\nVector Machines (SVMs) -- have a natural interpretation in terms of ecological\ndynamics. We use these ideas to design new online SVM algorithms that exploit\necological invasions, and benchmark performance using the MNIST dataset. Our\nwork provides a new ecological lens through which we can view statistical\nlearning and opens the possibility of designing ecosystems for machine\nlearning.\n  Supplemental code is found at https://github.com/owenhowell20/EcoSVM.", "authors": ["Owen Howell", "Cui Wenping", "Robert Marsland III", "Pankaj Mehta"], "year": "2019-08-02T14:08:17+00:00", "updated": "2019-08-23T13:52:08+00:00", "categories": ["cs.LG", "cond-mat.stat-mech", "stat.ML"], "url": "http://arxiv.org/pdf/1908.00868v2", "entry_id": "http://arxiv.org/abs/1908.00868v2"}, {"paper_id": "1612.04251", "filepath": "papers/1612.04251.pdf", "title": "TF.Learn: TensorFlow's High-level Module for Distributed Machine Learning", "abstract": "TF.Learn is a high-level Python module for distributed machine learning\ninside TensorFlow. It provides an easy-to-use Scikit-learn style interface to\nsimplify the process of creating, configuring, training, evaluating, and\nexperimenting a machine learning model. TF.Learn integrates a wide range of\nstate-of-art machine learning algorithms built on top of TensorFlow's low level\nAPIs for small to large-scale supervised and unsupervised problems. This module\nfocuses on bringing machine learning to non-specialists using a general-purpose\nhigh-level language as well as researchers who want to implement, benchmark,\nand compare their new methods in a structured environment. Emphasis is put on\nease of use, performance, documentation, and API consistency.", "authors": ["Yuan Tang"], "year": "2016-12-13T16:00:51+00:00", "updated": "2016-12-13T16:00:51+00:00", "categories": ["cs.DC", "cs.LG"], "url": "http://arxiv.org/pdf/1612.04251v1", "entry_id": "http://arxiv.org/abs/1612.04251v1"}, {"paper_id": "1607.01400", "filepath": "papers/1607.01400.pdf", "title": "An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality in Machine Learning", "abstract": "We propose a clustering-based iterative algorithm to solve certain\noptimization problems in machine learning, where we start the algorithm by\naggregating the original data, solving the problem on aggregated data, and then\nin subsequent steps gradually disaggregate the aggregated data. We apply the\nalgorithm to common machine learning problems such as the least absolute\ndeviation regression problem, support vector machines, and semi-supervised\nsupport vector machines. We derive model-specific data aggregation and\ndisaggregation procedures. We also show optimality, convergence, and the\noptimality gap of the approximated solution in each iteration. A computational\nstudy is provided.", "authors": ["Young Woong Park", "Diego Klabjan"], "year": "2016-07-05T20:04:57+00:00", "updated": "2016-07-05T20:04:57+00:00", "categories": ["stat.ML", "cs.LG"], "url": "http://arxiv.org/pdf/1607.01400v1", "entry_id": "http://arxiv.org/abs/1607.01400v1"}, {"paper_id": "1405.1304", "filepath": "papers/1405.1304.pdf", "title": "Application of Machine Learning Techniques in Aquaculture", "abstract": "In this paper we present applications of different machine learning\nalgorithms in aquaculture. Machine learning algorithms learn models from\nhistorical data. In aquaculture historical data are obtained from farm\npractices, yields, and environmental data sources. Associations between these\ndifferent variables can be obtained by applying machine learning algorithms to\nhistorical data. In this paper we present applications of different machine\nlearning algorithms in aquaculture applications.", "authors": ["Akhlaqur Rahman", "Sumaira Tasnim"], "year": "2014-05-03T14:26:42+00:00", "updated": "2014-05-03T14:26:42+00:00", "categories": ["cs.CE", "cs.LG"], "url": "http://arxiv.org/pdf/1405.1304v1", "entry_id": "http://arxiv.org/abs/1405.1304v1"}, {"paper_id": "2007.14206", "filepath": "papers/2007.14206.pdf", "title": "Machine Learning Potential Repository", "abstract": "This paper introduces a machine learning potential repository that includes\nPareto optimal machine learning potentials. It also shows the systematic\ndevelopment of accurate and fast machine learning potentials for a wide range\nof elemental systems. As a result, many Pareto optimal machine learning\npotentials are available in the repository from a website. Therefore, the\nrepository will help many scientists to perform accurate and fast atomistic\nsimulations.", "authors": ["Atsuto Seko"], "year": "2020-07-27T14:30:23+00:00", "updated": "2020-07-27T14:30:23+00:00", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci", "physics.chem-ph", "physics.data-an"], "url": "http://arxiv.org/pdf/2007.14206v1", "entry_id": "http://arxiv.org/abs/2007.14206v1"}, {"paper_id": "2202.10564", "filepath": "papers/2202.10564.pdf", "title": "Human-in-the-loop Machine Learning: A Macro-Micro Perspective", "abstract": "Though technical advance of artificial intelligence and machine learning has\nenabled many promising intelligent systems, many computing tasks are still not\nable to be fully accomplished by machine intelligence. Motivated by the\ncomplementary nature of human and machine intelligence, an emerging trend is to\ninvolve humans in the loop of machine learning and decision-making. In this\npaper, we provide a macro-micro review of human-in-the-loop machine learning.\nWe first describe major machine learning challenges which can be addressed by\nhuman intervention in the loop. Then we examine closely the latest research and\nfindings of introducing humans into each step of the lifecycle of machine\nlearning. Finally, we analyze current research gaps and point out future\nresearch directions.", "authors": ["Jiangtao Wang", "Bin Guo", "Liming Chen"], "year": "2022-02-21T22:45:59+00:00", "updated": "2022-02-21T22:45:59+00:00", "categories": ["cs.HC"], "url": "http://arxiv.org/pdf/2202.10564v1", "entry_id": "http://arxiv.org/abs/2202.10564v1"}, {"paper_id": "2002.12364", "filepath": "papers/2002.12364.pdf", "title": "Theoretical Models of Learning to Learn", "abstract": "A Machine can only learn if it is biased in some way. Typically the bias is\nsupplied by hand, for example through the choice of an appropriate set of\nfeatures. However, if the learning machine is embedded within an {\\em\nenvironment} of related tasks, then it can {\\em learn} its own bias by learning\nsufficiently many tasks from the environment. In this paper two models of bias\nlearning (or equivalently, learning to learn) are introduced and the main\ntheoretical results presented. The first model is a PAC-type model based on\nempirical process theory, while the second is a hierarchical Bayes model.", "authors": ["Jonathan Baxter"], "year": "2020-02-27T13:35:26+00:00", "updated": "2020-02-27T13:35:26+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/2002.12364v1", "entry_id": "http://arxiv.org/abs/2002.12364v1"}, {"paper_id": "2007.05479", "filepath": "papers/2007.05479.pdf", "title": "Impact of Legal Requirements on Explainability in Machine Learning", "abstract": "The requirements on explainability imposed by European laws and their\nimplications for machine learning (ML) models are not always clear. In that\nperspective, our research analyzes explanation obligations imposed for private\nand public decision-making, and how they can be implemented by machine learning\ntechniques.", "authors": ["Adrien Bibal", "Michael Lognoul", "Alexandre de Streel", "Beno\u00eet Fr\u00e9nay"], "year": "2020-07-10T16:57:18+00:00", "updated": "2020-07-10T16:57:18+00:00", "categories": ["cs.AI", "cs.CY", "cs.LG"], "url": "http://arxiv.org/pdf/2007.05479v1", "entry_id": "http://arxiv.org/abs/2007.05479v1"}, {"paper_id": "1810.03548", "filepath": "papers/1810.03548.pdf", "title": "Meta-Learning: A Survey", "abstract": "Meta-learning, or learning to learn, is the science of systematically\nobserving how different machine learning approaches perform on a wide range of\nlearning tasks, and then learning from this experience, or meta-data, to learn\nnew tasks much faster than otherwise possible. Not only does this dramatically\nspeed up and improve the design of machine learning pipelines or neural\narchitectures, it also allows us to replace hand-engineered algorithms with\nnovel approaches learned in a data-driven way. In this chapter, we provide an\noverview of the state of the art in this fascinating and continuously evolving\nfield.", "authors": ["Joaquin Vanschoren"], "year": "2018-10-08T16:07:11+00:00", "updated": "2018-10-08T16:07:11+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1810.03548v1", "entry_id": "http://arxiv.org/abs/1810.03548v1"}, {"paper_id": "1612.04858", "filepath": "papers/1612.04858.pdf", "title": "Bayesian Optimization for Machine Learning : A Practical Guidebook", "abstract": "The engineering of machine learning systems is still a nascent field; relying\non a seemingly daunting collection of quickly evolving tools and best\npractices. It is our hope that this guidebook will serve as a useful resource\nfor machine learning practitioners looking to take advantage of Bayesian\noptimization techniques. We outline four example machine learning problems that\ncan be solved using open source machine learning libraries, and highlight the\nbenefits of using Bayesian optimization in the context of these common machine\nlearning applications.", "authors": ["Ian Dewancker", "Michael McCourt", "Scott Clark"], "year": "2016-12-14T22:04:33+00:00", "updated": "2016-12-14T22:04:33+00:00", "categories": ["cs.LG"], "url": "http://arxiv.org/pdf/1612.04858v1", "entry_id": "http://arxiv.org/abs/1612.04858v1"}, {"paper_id": "1910.12387", "filepath": "papers/1910.12387.pdf", "title": "Components of Machine Learning: Binding Bits and FLOPS", "abstract": "Many machine learning problems and methods are combinations of three\ncomponents: data, hypothesis space and loss function. Different machine\nlearning methods are obtained as combinations of different choices for the\nrepresentation of data, hypothesis space and loss function. After reviewing the\nmathematical structure of these three components, we discuss intrinsic\ntrade-offs between statistical and computational properties of machine learning\nmethods.", "authors": ["Alexander Jung"], "year": "2019-10-25T17:33:33+00:00", "updated": "2019-10-30T07:59:02+00:00", "categories": ["cs.LG"], "url": "http://arxiv.org/pdf/1910.12387v2", "entry_id": "http://arxiv.org/abs/1910.12387v2"}, {"paper_id": "1911.06612", "filepath": "papers/1911.06612.pdf", "title": "Position Paper: Towards Transparent Machine Learning", "abstract": "Transparent machine learning is introduced as an alternative form of machine\nlearning, where both the model and the learning system are represented in\nsource code form. The goal of this project is to enable direct human\nunderstanding of machine learning models, giving us the ability to learn,\nverify, and refine them as programs. If solved, this technology could represent\na best-case scenario for the safety and security of AI systems going forward.", "authors": ["Dustin Juliano"], "year": "2019-11-12T10:49:55+00:00", "updated": "2019-11-12T10:49:55+00:00", "categories": ["cs.LG"], "url": "http://arxiv.org/pdf/1911.06612v1", "entry_id": "http://arxiv.org/abs/1911.06612v1"}, {"paper_id": "1611.03969", "filepath": "papers/1611.03969.pdf", "title": "An Introduction to MM Algorithms for Machine Learning and Statistical", "abstract": "MM (majorization--minimization) algorithms are an increasingly popular tool\nfor solving optimization problems in machine learning and statistical\nestimation. This article introduces the MM algorithm framework in general and\nvia three popular example applications: Gaussian mixture regressions,\nmultinomial logistic regressions, and support vector machines. Specific\nalgorithms for the three examples are derived and numerical demonstrations are\npresented. Theoretical and practical aspects of MM algorithm design are\ndiscussed.", "authors": ["Hien D. Nguyen"], "year": "2016-11-12T08:18:38+00:00", "updated": "2016-11-12T08:18:38+00:00", "categories": ["stat.CO", "cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1611.03969v1", "entry_id": "http://arxiv.org/abs/1611.03969v1"}, {"paper_id": "2104.05314", "filepath": "papers/2104.05314.pdf", "title": "Machine learning and deep learning", "abstract": "Today, intelligent systems that offer artificial intelligence capabilities\noften rely on machine learning. Machine learning describes the capacity of\nsystems to learn from problem-specific training data to automate the process of\nanalytical model building and solve associated tasks. Deep learning is a\nmachine learning concept based on artificial neural networks. For many\napplications, deep learning models outperform shallow machine learning models\nand traditional data analysis approaches. In this article, we summarize the\nfundamentals of machine learning and deep learning to generate a broader\nunderstanding of the methodical underpinning of current intelligent systems. In\nparticular, we provide a conceptual distinction between relevant terms and\nconcepts, explain the process of automated analytical model building through\nmachine learning and deep learning, and discuss the challenges that arise when\nimplementing such intelligent systems in the field of electronic markets and\nnetworked business. These naturally go beyond technological aspects and\nhighlight issues in human-machine interaction and artificial intelligence\nservitization.", "authors": ["Christian Janiesch", "Patrick Zschech", "Kai Heinrich"], "year": "2021-04-12T09:54:12+00:00", "updated": "2021-04-14T10:31:01+00:00", "categories": ["cs.AI"], "url": "http://arxiv.org/pdf/2104.05314v2", "entry_id": "http://arxiv.org/abs/2104.05314v2"}, {"paper_id": "1207.4676", "filepath": "papers/1207.4676.pdf", "title": "Proceedings of the 29th International Conference on Machine Learning (ICML-12)", "abstract": "This is an index to the papers that appear in the Proceedings of the 29th\nInternational Conference on Machine Learning (ICML-12). The conference was held\nin Edinburgh, Scotland, June 27th - July 3rd, 2012.", "authors": ["John Langford", "Joelle Pineau"], "year": "2012-07-19T14:08:22+00:00", "updated": "2012-09-16T11:24:54+00:00", "categories": ["cs.LG", "stat.ML"], "url": "http://arxiv.org/pdf/1207.4676v2", "entry_id": "http://arxiv.org/abs/1207.4676v2"}, {"paper_id": "2405.03720", "filepath": "papers/2405.03720.pdf", "title": "Spatial Transfer Learning with Simple MLP", "abstract": "First step to investigate the potential of transfer learning applied to the\nfield of spatial statistics", "authors": ["Hongjian Yang"], "year": "2024-05-05T20:39:15+00:00", "updated": "2024-05-05T20:39:15+00:00", "categories": ["cs.LG", "stat.ME", "stat.ML"], "url": "http://arxiv.org/pdf/2405.03720v1", "entry_id": "http://arxiv.org/abs/2405.03720v1"}, {"paper_id": "2201.06921", "filepath": "papers/2201.06921.pdf", "title": "Can Machine Learning be Moral?", "abstract": "The ethics of Machine Learning has become an unavoidable topic in the AI\nCommunity. The deployment of machine learning systems in multiple social\ncontexts has resulted in a closer ethical scrutiny of the design, development,\nand application of these systems. The AI/ML community has come to terms with\nthe imperative to think about the ethical implications of machine learning, not\nonly as a product but also as a practice (Birhane, 2021; Shen et al. 2021). The\ncritical question that is troubling many debates is what can constitute an\nethically accountable machine learning system. In this paper we explore\npossibilities for ethical evaluation of machine learning methodologies. We\nscrutinize techniques, methods and technical practices in machine learning from\na relational ethics perspective, taking into consideration how machine learning\nsystems are part of the world and how they relate to different forms of agency.\nTaking a page from Phil Agre (1997) we use the notion of a critical technical\npractice as a means of analysis of machine learning approaches. Our radical\nproposal is that supervised learning appears to be the only machine learning\nmethod that is ethically defensible.", "authors": ["Miguel Sicart", "Irina Shklovski", "Mirabelle Jones"], "year": "2021-12-13T07:20:50+00:00", "updated": "2021-12-13T07:20:50+00:00", "categories": ["cs.CY", "cs.HC"], "url": "http://arxiv.org/pdf/2201.06921v1", "entry_id": "http://arxiv.org/abs/2201.06921v1"}, {"paper_id": "1707.04849", "filepath": "papers/1707.04849.pdf", "title": "Minimax deviation strategies for machine learning and recognition with short learning samples", "abstract": "The article is devoted to the problem of small learning samples in machine\nlearning. The flaws of maximum likelihood learning and minimax learning are\nlooked into and the concept of minimax deviation learning is introduced that is\nfree of those flaws.", "authors": ["Michail Schlesinger", "Evgeniy Vodolazskiy"], "year": "2017-07-16T09:15:08+00:00", "updated": "2017-07-16T09:15:08+00:00", "categories": ["cs.LG"], "url": "http://arxiv.org/pdf/1707.04849v1", "entry_id": "http://arxiv.org/abs/1707.04849v1"}]